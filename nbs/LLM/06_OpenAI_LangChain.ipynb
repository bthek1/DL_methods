{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a1be17-148a-4006-88cc-dd7553c377fc",
   "metadata": {},
   "source": [
    "# OpenAI using LangChain\n",
    "\n",
    "> Using the **ChatGPT API with LangChain** is simple and powerful. LangChain provides high-level abstractions for interacting with ChatGPT (and other LLMs), adding things like memory, tools, and prompt templates — making it great for chatbots, agents, and RAG systems.\n",
    "\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c2546c-d7ab-480a-aea4-1bf4abfc49f8",
   "metadata": {},
   "source": [
    "## ✅ 1. Install Dependencies\n",
    "\n",
    "Install LangChain and OpenAI SDK:\n",
    "\n",
    "```bash\n",
    "pip install langchain openai\n",
    "```\n",
    "\n",
    "Or with Poetry:\n",
    "\n",
    "```bash\n",
    "poetry add langchain openai\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d631b5-67c4-4053-a893-b7bee58c3edf",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install -U langchain-openai langchain-core langchain-community\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48b30c-d7c8-48be-a7b0-d236f6a782a5",
   "metadata": {},
   "source": [
    "## ✅ 2. Setup Environment\n",
    "\n",
    "Set your OpenAI API key:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "Or load via `.env` and use `load_dotenv()`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd4146-df2d-4352-9a6b-a24e983b82ef",
   "metadata": {},
   "source": [
    "## ✅ 3. Basic Usage with ChatGPT (gpt-3.5 / gpt-4 / gpt-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea25ce-1e68-44e4-8b9a-f849ada06508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3362a-a3a8-4150-9d60-f1b9fd2af4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=os.getenv(\"openai_api_key\"),\n",
    ")\n",
    "\n",
    "response = chat([\n",
    "    HumanMessage(content=\"What's the capital of France?\")\n",
    "])\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1eb55-2d7e-4db0-a850-1168885fce67",
   "metadata": {},
   "source": [
    "## ✅ 4. Using Memory (ChatBot Style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db9c80-c046-4ee1-87ec-a81cbbcf24c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Ben! How can I assist you today?\n",
      "You just told me your name, Ben. If there's anything else you'd like to share or ask, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Chat model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"openai_api_key\")\n",
    ")\n",
    "\n",
    "# 2. Store for chat history\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 3. Wrap with memory\n",
    "chat = RunnableWithMessageHistory(\n",
    "    runnable=llm,\n",
    "    get_session_history=get_session_history,  # ✅ updated arg name\n",
    ")\n",
    "\n",
    "# 4. Simulate chat\n",
    "session_id = \"ben-session\"\n",
    "\n",
    "res1 = chat.invoke(\n",
    "    [HumanMessage(content=\"Hi, I'm Ben\")],\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(res1.content)\n",
    "\n",
    "res2 = chat.invoke(\n",
    "    [HumanMessage(content=\"What did I just tell you?\")],\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(res2.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ffd34-ea02-402d-b664-fecd938b2d20",
   "metadata": {},
   "source": [
    "## ✅ 5. Prompt Templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64dd53e-57a2-4a0c-97c0-8377f0569ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the ChatGPT API, you'll need to access the API provided by OpenAI. Here's a general guide on how to use it:\n",
      "\n",
      "1. **Set Up an OpenAI Account:**\n",
      "   - Go to the [OpenAI website](https://www.openai.com/) and sign up or log in if you already have an account.\n",
      "\n",
      "2. **Access API Keys:**\n",
      "   - Once logged in, navigate to the API section to obtain your API key. This key will be used to authenticate your requests.\n",
      "\n",
      "3. **Install Required Libraries:**\n",
      "   - Make sure you have the necessary libraries to make HTTP requests. If you're using Python, the `requests` library is commonly used.\n",
      "\n",
      "   ```bash\n",
      "   pip install requests\n",
      "   ```\n",
      "\n",
      "4. **Make API Requests:**\n",
      "   - Use the API key to make requests to the ChatGPT API endpoint. Here’s a basic example using Python:\n",
      "\n",
      "   ```python\n",
      "   import requests\n",
      "\n",
      "   # Define your API key and the endpoint URL\n",
      "   api_key = 'your-api-key-here'\n",
      "   url = 'https://api.openai.com/v1/chat/completions'\n",
      "\n",
      "   # Set up the headers for the request\n",
      "   headers = {\n",
      "       'Authorization': f'Bearer {api_key}',\n",
      "       'Content-Type': 'application/json'\n",
      "   }\n",
      "\n",
      "   # Define the parameters for the request\n",
      "   data = {\n",
      "       \"model\": \"gpt-3.5-turbo\",\n",
      "       \"messages\": [\n",
      "           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
      "           {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
      "       ],\n",
      "       \"max_tokens\": 150,\n",
      "       \"temperature\": 0.5\n",
      "   }\n",
      "\n",
      "   # Make the POST request to the API\n",
      "   response = requests.post(url, headers=headers, json=data)\n",
      "\n",
      "   # Parse the response\n",
      "   if response.status_code == 200:\n",
      "       response_data = response.json()\n",
      "       print(response_data['choices'][0]['message']['content'])\n",
      "   else:\n",
      "       print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
      "   ```\n",
      "\n",
      "5. **Handle the Response:**\n",
      "   - If the request is successful, the response will include the generated text along with additional information like the token usage.\n",
      "\n",
      "6. **Configure Parameters:**\n",
      "   - Adjust parameters such as `temperature`, `max_tokens`, and the conversation context in the `messages` array to achieve the desired behavior or output.\n",
      "\n",
      "7. **Security Considerations:**\n",
      "   - Keep your API key secure. Do not hardcode it in publicly accessible places. Consider using environment variables or secure vault services.\n",
      "\n",
      "8. **Read the Documentation:**\n",
      "   - OpenAI provides detailed [API documentation](https://platform.openai.com/docs/api-reference/introduction) that covers more advanced features and concepts you'll want to explore as you expand your implementation.\n",
      "\n",
      "With these steps, you should be able to start using the ChatGPT API to interact with the language model programmatically.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a senior software developer\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Chat model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=os.getenv(\"openai_api_key\")\n",
    ")\n",
    "\n",
    "# Output parser (converts from message to string)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain composition: Prompt → LLM → Parser\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Run it\n",
    "res = chain.invoke({\"question\": \"how to use chatGPT api?\"})\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051572ac-61a9-48cc-9bcb-3b49b6fa3eb6",
   "metadata": {},
   "source": [
    "## ✅ 6. Streaming Output from ChatGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900dc7d-92ad-45ce-b3cd-3b5e2a419ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a new type of computing that uses the principles of quantum mechanics, which is the science of very small things like atoms and particles, to process information. Traditional computers use bits, which can be either a 0 or a 1, to perform tasks and calculations. Quantum computers, on the other hand, use quantum bits or \"qubits.\"\n",
      "\n",
      "The unique features of qubits are:\n",
      "\n",
      "1. **Superposition**: Unlike bits, qubits can be both 0 and 1 at the same time. This allows quantum computers to process a vast amount of possibilities simultaneously.\n",
      "\n",
      "its can be linked together in such a way that the state of one qubit can depend on the state of another, no matter how far apart they are. This can lead to much faster processing speeds for certain tasks.\n",
      "\n",
      " the wrong ones.e**: Quantum algorithms can use interference to amplify the right answers and cancel out\n",
      "\n",
      ", they hold promise for complex problems in cryptography, optimization, and simulations of molecular structures. However, they are still in the experimental stage and are not yet practical for most applications."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "import os\n",
    "\n",
    "# Define a proper callback handler\n",
    "class PrintChunkHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(token, end=\"\")\n",
    "\n",
    "# Set up the streaming model with callback\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    streaming=True,\n",
    "    callbacks=[PrintChunkHandler()],\n",
    "    api_key=os.getenv(\"openai_api_key\")\n",
    ")\n",
    "\n",
    "# Invoke the chat model with a message\n",
    "res = chat.invoke([\n",
    "    HumanMessage(content=\"Explain quantum computing simply.\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c3394-dafd-4cd4-b1b4-fe4eecfce57a",
   "metadata": {},
   "source": [
    "## ✅ 7. Tool-Using Agent (with ChatGPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d365a-e596-40c3-8111-75b43f3fe4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: The result of \\(7 \\times (4 + 3)\\) is 49.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numexpr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import Tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define a safe calculator tool\n",
    "def safe_calculator(expression: str) -> str:\n",
    "    try:\n",
    "        return str(numexpr.evaluate(expression).item())\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=safe_calculator,\n",
    "        description=\"Useful for math operations like addition, subtraction, multiplication, and division.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the LLM\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"openai_api_key\")\n",
    ")\n",
    "\n",
    "# Create the LangGraph ReAct agent executor\n",
    "agent_executor = create_react_agent(\n",
    "    model=chat_model,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# Create a message input for the agent\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 7 * (4 + 3)?\"}\n",
    "]\n",
    "\n",
    "# Run the agent with a prompt\n",
    "response = agent_executor.invoke({\"messages\": messages})\n",
    "\n",
    "# Extract the final AI message from the returned state\n",
    "ai_messages = response.get(\"messages\", [])\n",
    "\n",
    "if ai_messages:\n",
    "    final_message = ai_messages[-1].content  # ✅ use .content for LangChain message objects\n",
    "    print(\"\\nFinal Answer:\", final_message)\n",
    "else:\n",
    "    print(\"No response from agent.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f895c64-4649-43ce-9824-11d5b5098a22",
   "metadata": {},
   "source": [
    "## ✅ 8. RAG with ChatGPT (Retrieval-Augmented Generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100e094-7ed9-48a4-acf2-bcd60bdf534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"Climate change refers to long-term shifts in temperatures and weather patterns.\"),\n",
    "    Document(page_content=\"Greenhouse gases are a major contributor to global warming.\"),\n",
    "]\n",
    "\n",
    "# Create and save FAISS index\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"openai_api_key\"))\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "db.save_local(\"my_index\")  # This creates my_index/index.faiss and index.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab98f1-afa7-4f92-84b8-fc22cd527753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document mentioned that climate change refers to long-term shifts in temperatures and weather patterns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Load vector store\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"openai_api_key\"))\n",
    "vectorstore = FAISS.load_local(\"my_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 2. Correct prompt with {context}\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant who answers questions based on the context.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{input}\")\n",
    "])\n",
    "\n",
    "# 3. LLM setup\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=os.getenv(\"openai_api_key\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 4. Chain setup\n",
    "question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=question_answer_chain)\n",
    "\n",
    "# 5. Query\n",
    "response = rag_chain.invoke({\"input\": \"What did the document say about climate change?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660768a-5a7c-44e3-bb4c-09a7eec4a64b",
   "metadata": {},
   "source": [
    "## ✅ Summary: Updated LangChain Patterns (2024+)\n",
    "\n",
    "| Feature            | Modern Usage with LangChain                   |\n",
    "|--------------------|-----------------------------------------------|\n",
    "| **Basic Chat**     | `ChatOpenAI(model=\"gpt-4o\")` with `invoke()` |\n",
    "| **Streaming**      | `ChatOpenAI(streaming=True)` + `callbacks`   |\n",
    "| **Memory**         | `RunnableWithMessageHistory` + `InMemoryChatMessageHistory` |\n",
    "| **Prompt Template**| `ChatPromptTemplate` + `LLMChain`             |\n",
    "| **Tools / Agents** | `create_react_agent()` + `Tool[]` (LangGraph) |\n",
    "| **RAG**            | `create_retrieval_chain()` + `FAISS` / `Chroma` |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this in Markdown table format, visual diagram, or as a quick reference card!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093fa8e-f14e-4164-9cf3-e1c50b1a2a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
