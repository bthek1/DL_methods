{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32a72b4-daaf-43eb-a851-605e9b4f9a8b",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "> Ollama is a lightweight, extensible framework designed for building and running large language models (LLMs) on local machines. It provides a command-line interface (CLI) that facilitates model management, customization, and interaction. Here's a comprehensive guide to using Ollama, including essential commands and examples.\n",
    "\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d25416-0ed7-4f85-bf3c-bdbc07a44dc0",
   "metadata": {},
   "source": [
    "## **1. Installation**\n",
    "\n",
    "- **For Linux:**\n",
    "\n",
    "  Open your terminal and execute:\n",
    "\n",
    "  ```bash\n",
    "  curl -fsSL https://ollama.com/install.sh | sh\n",
    "  ```\n",
    "\n",
    "  This command downloads and installs Ollama on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aea2e0-1da6-4bd6-8a3a-22af6a9d8138",
   "metadata": {},
   "source": [
    "## **2. System Requirements**\n",
    "\n",
    "- **Operating System:** macOS or Linux\n",
    "- **Memory (RAM):** Minimum 8GB; 16GB or more recommended\n",
    "- **Storage:** At least 10GB of free space\n",
    "- **Processor:** Modern CPU from the last 5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fb26e-0997-41f9-8c81-2121ccd768c6",
   "metadata": {},
   "source": [
    "## **3. Basic CLI Commands**\n",
    "\n",
    "- **Start the Ollama Server:**\n",
    "\n",
    "  To run Ollama without the desktop application:\n",
    "\n",
    "  ```bash\n",
    "  ollama serve\n",
    "  ```\n",
    "\n",
    "- **Download a Model:**\n",
    "\n",
    "  To download a specific model:\n",
    "\n",
    "  ```bash\n",
    "  ollama pull <model-name>\n",
    "  ```\n",
    "\n",
    "  Replace `<model-name>` with the desired model's name, e.g., `llama3.2`.\n",
    "\n",
    "- **List Downloaded Models:**\n",
    "\n",
    "  To view all models available on your system:\n",
    "\n",
    "  ```bash\n",
    "  ollama list\n",
    "  ```\n",
    "\n",
    "- **Run a Model:**\n",
    "\n",
    "  To start a model and enter an interactive session:\n",
    "\n",
    "  ```bash\n",
    "  ollama run <model-name>\n",
    "  ```\n",
    "\n",
    "  For example:\n",
    "\n",
    "  ```bash\n",
    "  ollama run llama3.2\n",
    "  ```\n",
    "\n",
    "- **Stop a Running Model:**\n",
    "\n",
    "  To stop a specific model:\n",
    "\n",
    "  ```bash\n",
    "  ollama stop <model-name>\n",
    "  ```\n",
    "\n",
    "- **Remove a Model:**\n",
    "\n",
    "  To delete a model from your system:\n",
    "\n",
    "  ```bash\n",
    "  ollama rm <model-name>\n",
    "  ```\n",
    "\n",
    "- **Display Model Information:**\n",
    "\n",
    "  To view details about a specific model:\n",
    "\n",
    "  ```bash\n",
    "  ollama show <model-name>\n",
    "  ```\n",
    "\n",
    "- **List Running Models:**\n",
    "\n",
    "  To see which models are currently active:\n",
    "\n",
    "  ```bash\n",
    "  ollama ps\n",
    "  ```\n",
    "\n",
    "- **Access Help:**\n",
    "\n",
    "  For a list of available commands and their descriptions:\n",
    "\n",
    "  ```bash\n",
    "  ollama help\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8b786-4f0e-4d8e-a1cc-d749ce96b14b",
   "metadata": {},
   "source": [
    "## **4. Model Customization**\n",
    "\n",
    "Ollama allows users to customize models using a `Modelfile`. This file specifies the base model and any modifications, such as system prompts or parameters.\n",
    "\n",
    "- **Create a `Modelfile`:**\n",
    "\n",
    "  Create a file named `Modelfile` with the following content:\n",
    "\n",
    "  ```\n",
    "  FROM llama3.2\n",
    "\n",
    "  SYSTEM \"You are an AI assistant specializing in environmental science. Answer all questions with a focus on sustainability.\"\n",
    "\n",
    "  PARAMETER temperature 0.7\n",
    "  ```\n",
    "\n",
    "- **Build the Custom Model:**\n",
    "\n",
    "  Use the `ollama create` command to build the model:\n",
    "\n",
    "  ```bash\n",
    "  ollama create my_custom_model -f ./Modelfile\n",
    "  ```\n",
    "\n",
    "- **Run the Custom Model:**\n",
    "\n",
    "  Start the customized model:\n",
    "\n",
    "  ```bash\n",
    "  ollama run my_custom_model\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbfea8-0142-49f5-b09a-2ec3dde0f486",
   "metadata": {},
   "source": [
    "## **5. Using Ollama with Files**\n",
    "\n",
    "- **Summarize Text from a File:**\n",
    "\n",
    "  To summarize the content of `input.txt`:\n",
    "\n",
    "  ```bash\n",
    "  ollama run llama3.2 \"Summarize the content of this file.\" < input.txt\n",
    "  ```\n",
    "\n",
    "- **Save Model Responses to a File:**\n",
    "\n",
    "  To save the model's response to `output.txt`:\n",
    "\n",
    "  ```bash\n",
    "  ollama run llama3.2 \"Explain the concept of quantum computing.\" > output.txt\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb3949-a71a-4a6c-bd41-f60665a55272",
   "metadata": {},
   "source": [
    "## **6. Common Use Cases**\n",
    "\n",
    "- **Text Generation:**\n",
    "\n",
    "  - *Content Creation:*\n",
    "\n",
    "    Generate an article on a specific topic:\n",
    "\n",
    "    ```bash\n",
    "    ollama run llama3.2 \"Write a short article on the benefits of renewable energy.\" > article.txt\n",
    "    ```\n",
    "\n",
    "  - *Question Answering:*\n",
    "\n",
    "    Answer specific queries:\n",
    "\n",
    "    ```bash\n",
    "    ollama run llama3.2 \"What are the latest advancements in artificial intelligence?\"\n",
    "    ```\n",
    "\n",
    "- **Data Analysis:**\n",
    "\n",
    "  - *Sentiment Analysis:*\n",
    "\n",
    "    Analyze the sentiment of a given text:\n",
    "\n",
    "    ```bash\n",
    "    ollama run llama3.2 \"Determine the sentiment of this review: 'The product exceeded my expectations.'\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea66d3-aa27-44f6-9870-e1556e16ba79",
   "metadata": {},
   "source": [
    "## **7. Python Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73633df6-5396-4b58-b190-289fe2a22a8d",
   "metadata": {},
   "source": [
    "### Ollama Python Library\n",
    "\n",
    "The Ollama Python library provides the easiest way to integrate Python 3.8+ projects with [Ollama](https://github.com/ollama/ollama)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2164a91-b651-4937-be63-db837e2ef2a4",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "- [Ollama](https://ollama.com/download) should be installed and running\n",
    "- Pull a model to use with the library: `ollama pull <model>` e.g. `ollama pull llama3.2`\n",
    "  - See [Ollama.com](https://ollama.com/search) for more information on the models available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621fda2-7304-4da2-8b09-1047d8a062ad",
   "metadata": {},
   "source": [
    "### Install\n",
    "\n",
    "```sh\n",
    "pip install ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e4941-305e-405b-aabc-db9969f7f439",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "```python\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)\n",
    "```\n",
    "\n",
    "See [_types.py](ollama/_types.py) for more information on the response types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de768f-e0ac-454e-a9e7-884b84fd7cc0",
   "metadata": {},
   "source": [
    "### Streaming responses\n",
    "\n",
    "Response streaming can be enabled by setting `stream=True`.\n",
    "\n",
    "```python\n",
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffbac4-7d00-4154-9efd-143488c852c6",
   "metadata": {},
   "source": [
    "### Custom client\n",
    "A custom client can be created by instantiating `Client` or `AsyncClient` from `ollama`.\n",
    "\n",
    "All extra keyword arguments are passed into the [`httpx.Client`](https://www.python-httpx.org/api/#client).\n",
    "\n",
    "```python\n",
    "from ollama import Client\n",
    "client = Client(\n",
    "  host='http://localhost:11434',\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "response = client.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c0a0c-0266-444e-befa-e1254554d4f2",
   "metadata": {},
   "source": [
    "### Async client\n",
    "\n",
    "The `AsyncClient` class is used to make asynchronous requests. It can be configured with the same fields as the `Client` class.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "  response = await AsyncClient().chat(model='llama3.2', messages=[message])\n",
    "\n",
    "asyncio.run(chat())\n",
    "```\n",
    "\n",
    "Setting `stream=True` modifies functions to return a Python asynchronous generator:\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "  async for part in await AsyncClient().chat(model='llama3.2', messages=[message], stream=True):\n",
    "    print(part['message']['content'], end='', flush=True)\n",
    "\n",
    "asyncio.run(chat())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae65c53-4924-49e1-a05f-5ebce1cb821b",
   "metadata": {},
   "source": [
    "### API\n",
    "\n",
    "The Ollama Python library's API is designed around the [Ollama REST API](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "\n",
    "#### Chat\n",
    "\n",
    "```python\n",
    "ollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n",
    "```\n",
    "\n",
    "#### Generate\n",
    "\n",
    "```python\n",
    "ollama.generate(model='llama3.2', prompt='Why is the sky blue?')\n",
    "```\n",
    "\n",
    "#### List\n",
    "\n",
    "```python\n",
    "ollama.list()\n",
    "```\n",
    "\n",
    "#### Show\n",
    "\n",
    "```python\n",
    "ollama.show('llama3.2')\n",
    "```\n",
    "\n",
    "#### Create\n",
    "\n",
    "```python\n",
    "ollama.create(model='example', from_='llama3.2', system=\"You are Mario from Super Mario Bros.\")\n",
    "```\n",
    "\n",
    "#### Copy\n",
    "\n",
    "```python\n",
    "ollama.copy('llama3.2', 'user/llama3.2')\n",
    "```\n",
    "\n",
    "#### Delete\n",
    "\n",
    "```python\n",
    "ollama.delete('llama3.2')\n",
    "```\n",
    "\n",
    "#### Pull\n",
    "\n",
    "```python\n",
    "ollama.pull('llama3.2')\n",
    "```\n",
    "\n",
    "#### Push\n",
    "\n",
    "```python\n",
    "ollama.push('user/llama3.2')\n",
    "```\n",
    "\n",
    "#### Embed\n",
    "\n",
    "```python\n",
    "ollama.embed(model='llama3.2', input='The sky is blue because of rayleigh scattering')\n",
    "```\n",
    "\n",
    "#### Embed (batch)\n",
    "\n",
    "```python\n",
    "ollama.embed(model='llama3.2', input=['The sky is blue because of rayleigh scattering', 'Grass is green because of chlorophyll'])\n",
    "```\n",
    "\n",
    "#### Ps\n",
    "\n",
    "```python\n",
    "ollama.ps()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7e7a5-e62c-4280-82fd-ad0ffbc46696",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "Errors are raised if requests return an error status or if an error is detected while streaming.\n",
    "\n",
    "```python\n",
    "model = 'does-not-yet-exist'\n",
    "\n",
    "try:\n",
    "  ollama.chat(model)\n",
    "except ollama.ResponseError as e:\n",
    "  print('Error:', e.error)\n",
    "  if e.status_code == 404:\n",
    "    ollama.pull(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d976d5e-3db2-4ca9-ad19-06818f032f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe153a0-959d-4845-b001-7944c3ef9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.generate(model='deepseek-r1:7b', prompt='write a haiku about time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69273a3d-a5bd-413e-82bf-85ea86c47c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, so I need to write a haiku about time. Hmm, okay. Let me think... A haiku is a traditional Japanese poem with three lines. The structure is typically 5 syllables in the first line, 7 in the second, and 5 again in the third. So, something like that.\n",
       "\n",
       "Time is such a big topic, but I need to fit it into this structure. Maybe start by thinking about how time feels or looks. Time can be seen as moving forward, something we don't always stop to appreciate. It's also something that doesn't wait for us; sometimes it zips by quickly, other times it seems to drag on.\n",
       "\n",
       "Okay, so maybe the first line could describe something related to time in a poetic way. Maybe using imagery like flowing or passing. Let me think of some words: \"sneaks,\" \"glides,\" \"whispers.\" Those have syllables that add up. \"Sneaks silently\" is two syllables... Hmm, need five.\n",
       "\n",
       "Wait, maybe \"Time slips through the cracks of night\"? That's 6 syllables, too long. Or perhaps something else. Maybe \"Time whispers secrets near the sea,\" but that's also more than five. Hmm.\n",
       "\n",
       "Alternatively, focusing on the passage of time: \"Day turns to night,\" that's two lines already. But I need three lines with the right syllable counts.\n",
       "\n",
       "Let me try again. First line: 5 syllables. Maybe \"Time glides soft as morning breaks.\" That's 6 syllables. Close but not quite. How about \"Time slips by in endless days\"? 7 syllables, which is too long for the first line.\n",
       "\n",
       "Perhaps using a metaphor. Like comparing time to something else. Maybe \"Clocks tick softly,\" but that's only two lines again.\n",
       "\n",
       "Wait, maybe combining elements like seasons or natural cycles since they relate to time. \"Autumn leaves drift down,\" that's 5 syllables: Aut (1), leaf (2), drift (3), down (4). Wait, no— Autumn is one word, then leaves, so \"Autumn leaves drift down the hill.\" That's more than five. Hmm.\n",
       "\n",
       "Alternatively, focusing on time passing without us noticing: \"Time marches on silently,\" that's 6 syllables again. Maybe \"Time glides by unseen.\"\n",
       "\n",
       "But I need to fit into the haiku structure with three lines of 5-7-5.\n",
       "\n",
       "Let me try this approach: first line about something happening during a season, second line about time passing, third line about the result or feeling.\n",
       "\n",
       "\"Autumn's breeze carries leaves,\" (5 syllables)  \n",
       "Time ticks on unseen,  \n",
       "Seasons shift, no pause for thoughts.  \n",
       "\n",
       "Wait, let's check the syllables:  \n",
       "First line: Autumn (1), breeze (2), carries (3), leaves (4). Wait, that's four? Maybe \"Autumn's breeze carries leaves,\" which is 5 syllables if I count each word as one, but in reality, it's more. Hmm.\n",
       "\n",
       "Alternatively: \"Time ticks silently on.\" That’s five syllables for the first line.  \n",
       "Second line could be something about seasons or change: \"Spring arrives unseen.\" Seven syllables? Spring (1), arrives (2), unseen (3). Wait, that's only three. Maybe add a bit more: \"Spring arrives slowly each year,\" which is six. Hmm.\n",
       "\n",
       "Third line needs to have five again. Maybe something about reflection or waiting: \"We wait for dawn to come.\" That’s six syllables. Maybe adjust it to \"We watch the sun rise,\" which is seven. Not quite.\n",
       "\n",
       "Alternatively, \"Echoes of time linger,\" but that's five. Let me piece this together:\n",
       "\n",
       "Time ticks silently on.  \n",
       "Spring arrives slowly each year.  \n",
       "Echos of time linger in our way.  \n",
       "\n",
       "Wait, first line: 5 syllables? Time (1), ticks (2), silently (3), on (4). That’s four. Hmm.\n",
       "\n",
       "Perhaps \"Sneaks through the quiet night,\" which is five syllables. Then second line about seasons or change— maybe \"Spring approaches unseen.\" Seven syllables: Spring, approaches, unseen.\n",
       "\n",
       "Third line about the lingering effects: \"Echoes of time's passing unfold.\" That's nine syllables. Too long.\n",
       "\n",
       "I'm a bit stuck here. Maybe I should look for simpler words with clear syllable counts. Let me try again:\n",
       "\n",
       "First line: 5 syllables.  \n",
       "\"Time glides soft as morning breaks,\" – that's six, maybe too much. Alternatively, \"Silent ticks pass by night.\" Five syllables.\n",
       "\n",
       "Second line:7  \n",
       "\"Spring comes slowly each year.\" Seven?\n",
       "\n",
       "Third line: five  \n",
       "\"Echoes of time linger here.\"\n",
       "\n",
       "Putting it together:\n",
       "\n",
       "Silent ticks pass by night.  \n",
       "Spring comes slowly each year.  \n",
       "Echoes of time linger here.\n",
       "\n",
       "That works! Let me check the syllables:\n",
       "\n",
       "First line: Silent (1), ticks (2), pass (3), by (4), night. Wait, that's five words but some have two letters, so syllables might add up to more than five? Hmm, maybe I'm overcomplicating it.\n",
       "\n",
       "Alternatively, \"Time slips through the cracks of night.\" That’s six syllables for first line, which is too much. Maybe adjust: \"Time glides by in quiet ease,\" but that's seven.\n",
       "\n",
       "I think I'll go with my initial attempt and see how it sounds, even if the syllables aren't perfect on first try.\n",
       "</think>\n",
       "\n",
       "Silent ticks pass by night.  \n",
       "Spring comes slowly each year.  \n",
       "Echoes of time linger here."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf88d62-13fe-4d35-9f8d-22f31c22e1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
