{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a454a34f-a6e1-40c5-8592-34231a78db5e",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "> LangChain is a **framework** designed to simplify the development of applications that use **Large Language Models (LLMs)**. It provides tools to integrate LLMs with **external data sources**, **memory**, **retrieval-augmented generation (RAG)**, **agents**, and **chains of operations**.\n",
    "\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81000e7-3f44-431a-9288-e840811c1e43",
   "metadata": {},
   "source": [
    "## **1. What is LangChain?**\n",
    "LangChain enables **developers** to build powerful applications that leverage **LLMs** like OpenAI's GPT, Cohere, Mistral, DeepSeek, and more.\n",
    "\n",
    "### **Why Use LangChain?**\n",
    "- **Simplifies LLM integration** with APIs, documents, and databases.\n",
    "- **Provides modular components** for easy prototyping and scaling.\n",
    "- **Supports memory and state management** for multi-turn conversations.\n",
    "- **Built-in retrieval and knowledge augmentation** for reducing hallucinations.\n",
    "- **Enables reasoning and action** using **LLM Agents**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f59f7c-444a-457f-8a84-4647bfc91071",
   "metadata": {},
   "source": [
    "## **2. Core Components of LangChain**\n",
    "LangChain consists of **six primary components**:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **LLMs** | Interface for calling language models (GPT-4, LLaMA, Claude, etc.) |\n",
    "| **Chains** | Sequences of operations that process inputs and outputs |\n",
    "| **Agents** | AI decision-making system that dynamically selects tools |\n",
    "| **Memory** | Stores conversation history and user state |\n",
    "| **Retrieval** | Enables RAG with vector databases |\n",
    "| **Tools & Utilities** | Functions like Google Search, APIs, or code execution |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186bf11-2f08-4a34-8929-1d643b347012",
   "metadata": {},
   "source": [
    "## **3. Setting Up LangChain**\n",
    "### **Install LangChain and Dependencies**\n",
    "```bash\n",
    "pip install langchain openai chromadb faiss-cpu tiktoken\n",
    "```\n",
    "\n",
    "### **Set Up API Keys (Example: OpenAI)**\n",
    "```python\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ead8c5-08a1-4a82-a8ab-4fe6e72cff3e",
   "metadata": {},
   "source": [
    "## **4. Using LLMs in LangChain**\n",
    "### **Basic Example with OpenAI's GPT-4**\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7)\n",
    "response = llm.predict(\"What is LangChain?\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### **Using LLaMA or Other Local Models**\n",
    "```python\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "response = llm(\"Tell me about AI applications.\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1061f-f974-49c1-b141-967ba5e21f36",
   "metadata": {},
   "source": [
    "## **5. Chains: Creating Pipelines**\n",
    "A **Chain** is a sequence of operations (e.g., input → LLM → output).\n",
    "\n",
    "### **Basic LLM Chain**\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What are the top 3 applications of {technology}?\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "response = chain.run(technology=\"Machine Learning\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### **Sequential Chains (Multiple Steps)**\n",
    "```python\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Explain {concept}\"))\n",
    "chain2 = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Summarize in one line: {text}\"))\n",
    "\n",
    "pipeline = SimpleSequentialChain(chains=[chain1, chain2])\n",
    "response = pipeline.run(\"Quantum Computing\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a79a3-c003-4f6c-8680-b775461a27f9",
   "metadata": {},
   "source": [
    "## **6. Agents: Dynamic AI with Tool Use**\n",
    "Agents allow an LLM to **choose tools dynamically** rather than following a fixed flow.\n",
    "\n",
    "### **Basic Agent with OpenAI Functions**\n",
    "```python\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def search_tool(query):\n",
    "    return f\"Searching for {query} on the web...\"\n",
    "\n",
    "tools = [Tool(name=\"WebSearch\", func=search_tool, description=\"Searches the web\")]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=ChatOpenAI(model=\"gpt-4\"),\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent.run(\"What is the latest AI research?\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### **Self-Reflecting AI Agent**\n",
    "```python\n",
    "from langchain.agents import OpenAIFunctionsAgent\n",
    "from langchain.agents.agent import AgentExecutor\n",
    "\n",
    "agent = OpenAIFunctionsAgent.from_llm(llm=ChatOpenAI(model=\"gpt-4\"))\n",
    "executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "response = executor.invoke(\"Find the latest AI papers.\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2bbcf-3e21-45db-8e55-29d65f969b8c",
   "metadata": {},
   "source": [
    "## **7. Memory and State Management**\n",
    "By default, LLMs do not remember past interactions. **Memory** in LangChain enables persistence.\n",
    "\n",
    "### **Adding Memory to Conversations**\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(conversation.run(\"Hello, who are you?\"))\n",
    "print(conversation.run(\"Can you summarize our conversation?\"))\n",
    "```\n",
    "\n",
    "### **Different Memory Types**\n",
    "| Memory Type | Use Case |\n",
    "|-------------|----------|\n",
    "| **ConversationBufferMemory** | Stores complete conversation history |\n",
    "| **ConversationSummaryMemory** | Summarizes past interactions |\n",
    "| **VectorStoreRetrieverMemory** | Uses embeddings to store long-term memory |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9444fdb-1e38-4e0d-af13-940333338eb1",
   "metadata": {},
   "source": [
    "## **8. Retrieval-Augmented Generation (RAG)**\n",
    "RAG combines **vector search** with **LLMs** to **reduce hallucinations**.\n",
    "\n",
    "### **Loading a Document and Storing Embeddings**\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Load and split documents\n",
    "loader = TextLoader(\"docs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Store as vector embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "```\n",
    "\n",
    "### **Retrieving and Passing to LLM**\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "retrieved_docs = retriever.get_relevant_documents(\"What is AI?\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5d08c-278b-4e0b-a43d-1b8acf57f06f",
   "metadata": {},
   "source": [
    "## **9. LangChain Tools and Integrations**\n",
    "LangChain connects with **APIs, databases, and search engines**.\n",
    "\n",
    "### **Google Search API**\n",
    "```python\n",
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "tool = Tool(name=\"Google Search\", func=search.run)\n",
    "\n",
    "result = tool.run(\"Latest AI news\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### **Code Execution**\n",
    "```python\n",
    "from langchain.tools import PythonREPLTool\n",
    "\n",
    "python_tool = PythonREPLTool()\n",
    "print(python_tool.run(\"import math; math.sqrt(16)\"))\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046adb3-8e8f-47e4-bdbb-ac8c43b7abab",
   "metadata": {},
   "source": [
    "## **10. Scaling and Optimization**\n",
    "### **Optimizing Token Usage**\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(len(encoder.encode(\"This is a test message.\")))  # Token count\n",
    "```\n",
    "\n",
    "### **Using Local LLMs Instead of API Calls**\n",
    "For privacy or cost efficiency, use models like LLaMA or Mistral:\n",
    "```python\n",
    "from langchain.llms import Ollama\n",
    "local_llm = Ollama(model=\"mistral\")\n",
    "```\n",
    "\n",
    "### **Batch Processing for Efficiency**\n",
    "```python\n",
    "responses = llm.batch([\"Explain AI\", \"Define RAG\", \"What is LangChain?\"])\n",
    "print(responses)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155eeca-5025-4ee8-9e05-779ff809694e",
   "metadata": {},
   "source": [
    "## **11. Example Architectures and Use Cases**\n",
    "| Use Case | LangChain Components Used |\n",
    "|----------|---------------------------|\n",
    "| **AI-Powered Chatbots** | Memory, Chains, LLMs |\n",
    "| **Retrieval-Augmented QA Systems** | Retrieval, Vector Stores, LLMs |\n",
    "| **AI Agents for Research** | Agents, Tools, API Integrations |\n",
    "| **Automated Code Generation** | LLMs, Python Execution Tools |\n",
    "| **Personalized Assistants** | Memory, State Tracking |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ea32c-f42a-45ec-83dd-29ed1cf5314a",
   "metadata": {},
   "source": [
    "## **Final Thoughts**\n",
    "- **LangChain simplifies LLM development** by abstracting complex workflows.\n",
    "- **It integrates retrieval, memory, and agents** for dynamic applications.\n",
    "- **It supports local and cloud-based LLMs**, ensuring flexibility.\n",
    "- **Optimizing context size, retrieval methods, and caching** improves efficiency.\n",
    "\n",
    "LangChain is the foundation for **building advanced, production-ready AI applications** with modular, scalable components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0980c62-0849-4002-b2f4-eaf6c49df01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
