{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9082fa1c-4df9-4ac5-9a3e-bad105077079",
   "metadata": {},
   "source": [
    "# Dataset and Dataloaders\n",
    "\n",
    "> Datasets and Dataloaders\n",
    "\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2bd0f-944a-4b55-b108-176be84d2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        #data loading\n",
    "        xy = np.loadtxt('Data/wine.csv', delimiter=\",\", dtype=np.float32, skiprows = 1)\n",
    "        self.xy = xy\n",
    "        self.x = torch.from_numpy(xy[:,1:])\n",
    "        self.y = torch.from_numpy(xy[:,[0]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bba668-efea-4b4e-bed3-8f2904ecb09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3802636-0a71-489e-85b8-1ae1c465221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
       "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
       "         1.0650e+03]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_data = dataset[0]\n",
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26f3ba-4ece-4d46-9247-ec97ba6b4a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
       "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
       "         1.0650e+03]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = dataset[0]\n",
    "features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea78b7c-6b84-4ea4-aa18-2190e9e27f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size = 4, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eae700-f657-4d90-884b-1f79b9754634",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d6229-c858-49d8-9ef0-023eabaeb093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1.4020e+01, 1.6800e+00, 2.2100e+00, 1.6000e+01, 9.6000e+01, 2.6500e+00,\n",
       "          2.3300e+00, 2.6000e-01, 1.9800e+00, 4.7000e+00, 1.0400e+00, 3.5900e+00,\n",
       "          1.0350e+03],\n",
       "         [1.2600e+01, 2.4600e+00, 2.2000e+00, 1.8500e+01, 9.4000e+01, 1.6200e+00,\n",
       "          6.6000e-01, 6.3000e-01, 9.4000e-01, 7.1000e+00, 7.3000e-01, 1.5800e+00,\n",
       "          6.9500e+02],\n",
       "         [1.2790e+01, 2.6700e+00, 2.4800e+00, 2.2000e+01, 1.1200e+02, 1.4800e+00,\n",
       "          1.3600e+00, 2.4000e-01, 1.2600e+00, 1.0800e+01, 4.8000e-01, 1.4700e+00,\n",
       "          4.8000e+02],\n",
       "         [1.2080e+01, 1.8300e+00, 2.3200e+00, 1.8500e+01, 8.1000e+01, 1.6000e+00,\n",
       "          1.5000e+00, 5.2000e-01, 1.6400e+00, 2.4000e+00, 1.0800e+00, 2.2700e+00,\n",
       "          4.8000e+02]]),\n",
       " tensor([[1.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [2.]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5c8db-ed55-4280-8422-e6fe3c40e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857b2b2-223c-40d3-966f-4594d81ab141",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(dataiter)\n",
    "features, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0917e-0a20-4856-af20-8342827dab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.4200e+01, 1.7600e+00, 2.4500e+00, 1.5200e+01, 1.1200e+02, 3.2700e+00,\n",
       "          3.3900e+00, 3.4000e-01, 1.9700e+00, 6.7500e+00, 1.0500e+00, 2.8500e+00,\n",
       "          1.4500e+03],\n",
       "         [1.2170e+01, 1.4500e+00, 2.5300e+00, 1.9000e+01, 1.0400e+02, 1.8900e+00,\n",
       "          1.7500e+00, 4.5000e-01, 1.0300e+00, 2.9500e+00, 1.4500e+00, 2.2300e+00,\n",
       "          3.5500e+02],\n",
       "         [1.3400e+01, 3.9100e+00, 2.4800e+00, 2.3000e+01, 1.0200e+02, 1.8000e+00,\n",
       "          7.5000e-01, 4.3000e-01, 1.4100e+00, 7.3000e+00, 7.0000e-01, 1.5600e+00,\n",
       "          7.5000e+02],\n",
       "         [1.4390e+01, 1.8700e+00, 2.4500e+00, 1.4600e+01, 9.6000e+01, 2.5000e+00,\n",
       "          2.5200e+00, 3.0000e-01, 1.9800e+00, 5.2500e+00, 1.0200e+00, 3.5800e+00,\n",
       "          1.2900e+03]]),\n",
       " tensor([[1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd078e1-69ba-448a-aa9c-fdb4ec2b0d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 45)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "\n",
    "total_samples, n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968e12a-3fde-4c1c-8b86-a07389ac8b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2, step 5/45, inputs:tensor([13.1100,  1.0100,  1.7000, 15.0000, 78.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 10/45, inputs:tensor([12.0800,  1.8300,  2.3200, 18.5000, 81.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 15/45, inputs:tensor([ 12.4700,   1.5200,   2.2000,  19.0000, 162.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 20/45, inputs:tensor([ 13.6800,   1.8300,   2.3600,  17.2000, 104.0000]) labels:tensor([1.])\n",
      "epoch 1/2, step 25/45, inputs:tensor([12.3700,  0.9400,  1.3600, 10.6000, 88.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 30/45, inputs:tensor([12.8200,  3.3700,  2.3000, 19.5000, 88.0000]) labels:tensor([3.])\n",
      "epoch 1/2, step 35/45, inputs:tensor([ 13.5800,   2.5800,   2.6900,  24.5000, 105.0000]) labels:tensor([3.])\n",
      "epoch 1/2, step 40/45, inputs:tensor([ 13.9400,   1.7300,   2.2700,  17.4000, 108.0000]) labels:tensor([1.])\n",
      "epoch 1/2, step 45/45, inputs:tensor([ 13.6400,   3.1000,   2.5600,  15.2000, 116.0000]) labels:tensor([1.])\n",
      "epoch 2/2, step 5/45, inputs:tensor([13.1100,  1.0100,  1.7000, 15.0000, 78.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 10/45, inputs:tensor([12.8700,  4.6100,  2.4800, 21.5000, 86.0000]) labels:tensor([3.])\n",
      "epoch 2/2, step 15/45, inputs:tensor([ 13.8200,   1.7500,   2.4200,  14.0000, 111.0000]) labels:tensor([1.])\n",
      "epoch 2/2, step 20/45, inputs:tensor([12.1600,  1.6100,  2.3100, 22.8000, 90.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 25/45, inputs:tensor([12.7200,  1.8100,  2.2000, 18.8000, 86.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 30/45, inputs:tensor([11.8400,  0.8900,  2.5800, 18.0000, 94.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 35/45, inputs:tensor([ 12.9900,   1.6700,   2.6000,  30.0000, 139.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 40/45, inputs:tensor([ 13.3400,   0.9400,   2.3600,  17.0000, 110.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 45/45, inputs:tensor([12.3400,  2.4500,  2.4600, 21.0000, 98.0000]) labels:tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f'epoch {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs:{inputs[0][:5]} labels:{labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fbd76-149c-47a0-ae09-5dc6984fa592",
   "metadata": {},
   "source": [
    "## Dataset Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cb483-f61b-4f03-ba30-a3e61d99b980",
   "metadata": {},
   "source": [
    "### Types of Transform:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6462ac-20cc-4c48-949c-0ae185523285",
   "metadata": {},
   "source": [
    "#### On Images:\n",
    "> CenterCrop, Grayscale, Pad, RandomAffine RandomCrop, RandomHorizontalFlip, RandomRotation Resize, Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0b957-245f-4485-9133-d0c81d09873f",
   "metadata": {},
   "source": [
    "#### On Tensors:\n",
    "> LinearTransformation, Normalize, RandomErasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca6cff-fceb-41a8-968d-4fa52e3ea47a",
   "metadata": {},
   "source": [
    "#### Conversion:\n",
    "> ToPILImage: from tensor or ndarray\n",
    "\n",
    "> ToTensor: from numpy.ndarray or PIL Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d54148-e487-44f4-ab02-b0728c4b795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        #data loading\n",
    "        xy = np.loadtxt('Data/wine.csv', delimiter=\",\", dtype=np.float32, skiprows = 1)\n",
    "        self.xy = xy\n",
    "        self.x = xy[:,1:]\n",
    "        self.y = xy[:,[0]]\n",
    "        self.n_samples = xy.shape[0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b50d6-cc33-4a2e-91ef-15b0342dbd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor():\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets  = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "\n",
    "class MulTransform:\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, target = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7684fa-38e0-4c38-ab16-f6a3a88d5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbb8a1-c0c1-4d63-b8c4-c185cd95eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineDataset(transform = composed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a002bf-e09e-4767-b648-f48bc7b0f9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n",
       "         6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n",
       "         2.1300e+03]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_data = dataset[0]\n",
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69b74c-9ba6-4dd3-aa1d-9d4043d0739b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
       "         1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
       "         4.2600e+03]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = dataset[0]\n",
    "features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea6df1-3771-4fb7-8adc-0b1e43c0b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size = 4, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4db31-b159-4add-bb23-6435effaa880",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617db1f8-4c4d-4d35-817e-a53ad8c7f2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2.4660e+01, 1.9800e+00, 3.9000e+00, 2.9600e+01, 2.7200e+02, 3.8000e+00,\n",
       "          3.7000e+00, 7.0000e-01, 5.5200e+00, 6.8000e+00, 2.1200e+00, 4.6200e+00,\n",
       "          1.5000e+03],\n",
       "         [2.7000e+01, 6.2400e+00, 5.2400e+00, 4.8000e+01, 2.4600e+02, 2.8000e+00,\n",
       "          3.1400e+00, 4.4000e-01, 2.5000e+00, 1.7200e+01, 1.1800e+00, 2.6000e+00,\n",
       "          1.0000e+03],\n",
       "         [2.8200e+01, 4.0400e+00, 4.8000e+00, 3.7600e+01, 2.0600e+02, 5.5000e+00,\n",
       "          5.8400e+00, 6.4000e-01, 4.7600e+00, 1.2400e+01, 2.1400e+00, 5.5000e+00,\n",
       "          2.1200e+03],\n",
       "         [2.4900e+01, 6.0600e+00, 5.2800e+00, 5.4000e+01, 1.9400e+02, 3.8000e+00,\n",
       "          1.1600e+00, 1.2600e+00, 2.2800e+00, 1.5000e+01, 1.3400e+00, 3.4600e+00,\n",
       "          1.7600e+03]]),\n",
       " tensor([[2.],\n",
       "         [3.],\n",
       "         [1.],\n",
       "         [3.]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c924653-d73b-415b-a432-7c57ebe1ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8172c-c4f4-47aa-a3f7-1ef26681df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(dataiter)\n",
    "features, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772d011-d1b5-4271-bdf4-9183ce186aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.4440e+01, 2.5800e+00, 3.8800e+00, 3.8000e+01, 1.8400e+02, 4.7200e+00,\n",
       "          4.0800e+00, 7.8000e-01, 4.1600e+00, 5.4000e+00, 1.7200e+00, 6.0400e+00,\n",
       "          6.2400e+02],\n",
       "         [2.3920e+01, 2.1800e+00, 4.6000e+00, 4.2000e+01, 2.0200e+02, 6.7600e+00,\n",
       "          4.2800e+00, 2.6000e-01, 3.3000e+00, 6.4200e+00, 1.9800e+00, 6.2600e+00,\n",
       "          1.7720e+03],\n",
       "         [2.4500e+01, 3.4600e+00, 4.2400e+00, 3.8000e+01, 1.6000e+02, 3.3000e+00,\n",
       "          4.0600e+00, 7.4000e-01, 3.2600e+00, 6.8000e+00, 2.0000e+00, 6.3400e+00,\n",
       "          1.0200e+03],\n",
       "         [2.6820e+01, 7.6800e+00, 4.2400e+00, 3.7600e+01, 1.8000e+02, 4.9000e+00,\n",
       "          5.3600e+00, 5.4000e-01, 2.9600e+00, 8.5600e+00, 1.8200e+00, 6.0000e+00,\n",
       "          2.0700e+03]]),\n",
       " tensor([[2.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fed319-c2e6-45b8-8f78-c179107a6faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 45)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "\n",
    "total_samples, n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec600b34-7bdb-4714-9a9e-d861e542882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2, step 5/45, inputs:tensor([ 23.9200,   2.1800,   4.6000,  42.0000, 202.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 10/45, inputs:tensor([ 24.5800,   3.2200,   4.4200,  40.8000, 206.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 15/45, inputs:tensor([ 28.2000,   4.0400,   4.8000,  37.6000, 206.0000]) labels:tensor([1.])\n",
      "epoch 1/2, step 20/45, inputs:tensor([ 27.3800,   6.5200,   5.0800,  40.0000, 214.0000]) labels:tensor([3.])\n",
      "epoch 1/2, step 25/45, inputs:tensor([ 28.2000,   4.3200,   4.6000,  36.0000, 210.0000]) labels:tensor([1.])\n",
      "epoch 1/2, step 30/45, inputs:tensor([ 23.3000,   3.3400,   5.2400,  52.0000, 176.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 35/45, inputs:tensor([ 26.1000,  11.6000,   4.2600,  43.0000, 172.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 40/45, inputs:tensor([ 24.6600,   2.2000,   4.5600,  32.0000, 202.0000]) labels:tensor([2.])\n",
      "epoch 1/2, step 45/45, inputs:tensor([ 27.1200,   3.4200,   4.6200,  32.4000, 234.0000]) labels:tensor([1.])\n",
      "epoch 2/2, step 5/45, inputs:tensor([ 25.0600,  11.0200,   5.2800,  50.0000, 192.0000]) labels:tensor([3.])\n",
      "epoch 2/2, step 10/45, inputs:tensor([ 24.6600,   2.2000,   4.5600,  32.0000, 202.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 15/45, inputs:tensor([ 25.8600,   5.6200,   5.4000,  42.0000, 192.0000]) labels:tensor([3.])\n",
      "epoch 2/2, step 20/45, inputs:tensor([ 23.6200,   4.2400,   5.4800,  43.0000, 268.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 25/45, inputs:tensor([ 25.0200,   3.4600,   3.9600,  41.0000, 170.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 30/45, inputs:tensor([ 28.4400,   7.9800,   5.0200,  26.4000, 256.0000]) labels:tensor([1.])\n",
      "epoch 2/2, step 35/45, inputs:tensor([ 26.3200,   4.7200,   5.3400,  37.2000, 202.0000]) labels:tensor([1.])\n",
      "epoch 2/2, step 40/45, inputs:tensor([ 24.1600,   3.6600,   4.6400,  37.0000, 162.0000]) labels:tensor([2.])\n",
      "epoch 2/2, step 45/45, inputs:tensor([ 23.5800,   4.2600,   5.5600,  57.0000, 184.0000]) labels:tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f'epoch {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs:{inputs[0][:5]} labels:{labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4097d-4a31-4468-8a58-f036e844a7e4",
   "metadata": {},
   "source": [
    "## MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a8ff8-012d-4a13-89e4-43fffa7ddd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch \n",
    "from PIL import Image\n",
    "from torch import nn, save, load\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision \n",
    "\n",
    "# Get data \n",
    "train = torchvision.datasets.MNIST(root=\"data\", download=True, train=True, transform=torchvision.transforms.ToTensor())\n",
    "dataset = DataLoader(train, 32)\n",
    "#1,28,28 - classes 0-9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
