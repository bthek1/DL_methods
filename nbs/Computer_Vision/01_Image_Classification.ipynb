{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61675874-efaf-407f-a1cf-3ff41210a846",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "\n",
    "> The task of assigning a label to an entire image, indicating what object or scene is present in the image.\n",
    "\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fdd09-f04c-46fa-bb6e-8f6b1079fa38",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e8f31-2085-4314-a409-47d2bb45f8a1",
   "metadata": {},
   "source": [
    "- CIFAR-10 and CIFAR-100\n",
    "  - CIFAR-10: 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n",
    "  - CIFAR-100: Similar to CIFAR-10 but with 100 classes containing 600 images each.\n",
    "  - Use Case: Benchmarking small-scale image classification models.\n",
    "\n",
    "- ImageNet\n",
    "  - Description: Over 14 million images across 1,000 classes.\n",
    "  - Use Case: Large-scale image classification, used for pre-training models.\n",
    "\n",
    "- MNIST and Fashion-MNIST\n",
    "  - MNIST: 70,000 grayscale images of handwritten digits (0-9).\n",
    "  - Fashion-MNIST: 70,000 grayscale images of fashion items in 10 categories.\n",
    "  - Use Case: Benchmarking simple image classification models.\n",
    "\n",
    "- COCO (Common Objects in Context)\n",
    "  - Description: 330,000 images with objects segmented into 80 categories.\n",
    "  - Use Case: Object detection, segmentation, and image classification.\n",
    "\n",
    "- SVHN (Street View House Numbers)\n",
    "  - Description: Over 600,000 32x32 color images of house numbers from Google Street View.\n",
    "  - Use Case: Real-world digit classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e78a98-d33d-4906-a4a1-673f717bf617",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8235700-a6e8-49a3-a94d-d5cd1f3fab76",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3273c0-4a11-4fd6-8677-6bc27d67c526",
   "metadata": {},
   "source": [
    "- LeNet: One of the earliest CNN architectures, used for digit recognition.\n",
    "- AlexNet: Won the 2012 ImageNet competition, significantly deep with ReLU activations.\n",
    "- VGGNet: Known for using very small (3x3) convolution filters, 16-19 weight layers.\n",
    "- GoogLeNet (Inception): Uses a network-in-network architecture, significantly reducing parameters.\n",
    "- ResNet (Residual Networks): Introduces residual connections to train very deep networks.\n",
    "- DenseNet: Each layer receives input from all previous layers, promoting feature reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dccb8d-cdbf-453a-8082-61f2e2dd665f",
   "metadata": {},
   "source": [
    "### Vision Transformers (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e16e00-ea24-4b22-96b6-b9a4fe20617b",
   "metadata": {},
   "source": [
    "- Description: Adapted the Transformer architecture to image classification tasks.\n",
    "- Use Case: Competes with CNNs on image classification benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04ee79-e275-41c2-90a2-a0e92e4ee2a2",
   "metadata": {},
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2b766-b57a-4eed-9bd7-714c1d8c6c65",
   "metadata": {},
   "source": [
    "- Description: Uses a compound scaling method to uniformly scale width, depth, and resolution.\n",
    "- Use Case: Balances accuracy and efficiency, outperforming many existing models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f604a63b-7bcd-4963-bdd5-690854cc0981",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6ba0f-8e56-4f73-803c-09f55fdd58f5",
   "metadata": {},
   "source": [
    "1. Learning Rate\n",
    "  - Description: Controls the step size at each iteration while moving towards a minimum of the loss function.\n",
    "  - Tuning: Start with a moderate value (e.g., 0.001), use learning rate schedules (e.g., step decay, exponential decay).\n",
    "\n",
    "2. Batch Size\n",
    "  - Description: Number of samples processed before the model is updated.\n",
    "  - Tuning: Common values range from 32 to 256. Larger batches require more memory but can leverage better parallelism.\n",
    "\n",
    "4. Number of Epochs\n",
    "  - Description: Number of complete passes through the training dataset.\n",
    "  - Tuning: Monitor validation loss to avoid overfitting, typically between 10 to 100 epochs.\n",
    "\n",
    "5. Optimizer\n",
    "  - Popular Choices: SGD, Adam, RMSprop.\n",
    "  - Tuning: Adam is a good default choice; try SGD with momentum for potentially better convergence.\n",
    "\n",
    "6. Regularization Parameters\n",
    "  - Weight Decay (L2 Regularization): Penalizes large weights, reducing overfitting.\n",
    "  - Dropout Rate: Randomly sets a fraction of input units to 0 at each update during training, preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a4dc4-8216-487f-8783-dfd94c17f2c2",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3959fc9-f092-4158-adce-e7773ab4ad60",
   "metadata": {},
   "source": [
    "1. Cross-Entropy Loss\n",
    "  - Description: Measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "  - Use Case: Standard for multi-class classification problems.\n",
    "\n",
    "2. Mean Squared Error (MSE)\n",
    "  - Description: Measures the average of the squares of the errors between predicted and actual values.\n",
    "  - Use Case: More common in regression, but sometimes used in classification problems with continuous labels.\n",
    "\n",
    "4. Categorical Hinge Loss\n",
    "  - Description: Measures the performance for \"one-versus-all\" classification tasks.\n",
    "  - Use Case: Useful in scenarios with class imbalance.\n",
    "\n",
    "5. Focal Loss\n",
    "  - Description: Modifies cross-entropy loss to address class imbalance by down-weighting the loss assigned to well-classified examples.\n",
    "  - Use Case: Effective in highly imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8978ad6-bf0a-48b6-8ac1-6fa3095f560e",
   "metadata": {},
   "source": [
    "### Other Important Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627f75c-117f-4049-94b9-4c76c0f4cc19",
   "metadata": {},
   "source": [
    "- Data Augmentation\n",
    "  - Description: Techniques to artificially increase the size of a dataset by creating modified versions of images.\n",
    "  - Methods: Rotation, flipping, scaling, cropping, color jittering.\n",
    "  - Use Case: Helps improve model generalization.\n",
    "\n",
    "- Transfer Learning\n",
    "  - Description: Using a pre-trained model on a new, but related task.\n",
    "  - Approach: Fine-tuning the pre-trained model on the new dataset.\n",
    "  - Use Case: Effective when the new dataset is small.\n",
    "\n",
    "- Evaluation Metrics\n",
    "  - Accuracy: Proportion of correctly predicted instances.\n",
    "  - Precision, Recall, F1-Score: Useful for class imbalance scenarios.\n",
    "  - Confusion Matrix: Provides insight into the performance of the classification model on each class.\n",
    "\n",
    "- Model Interpretability\n",
    "  - Grad-CAM (Gradient-weighted Class Activation Mapping): Visualizes which parts of the image are important for the modelâ€™s predictions.\n",
    "  - SHAP (SHapley Additive exPlanations): Explains the output of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77122a38-a47b-41cc-801e-1af9d3c86f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
