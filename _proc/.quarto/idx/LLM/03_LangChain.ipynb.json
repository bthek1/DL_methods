{"title":"LangChain","markdown":{"yaml":{"description":"LangChain is a **framework** designed to simplify the development of applications that use **Large Language Models (LLMs)**. It provides tools to integrate LLMs with **external data sources**, **memory**, **retrieval-augmented generation (RAG)**, **agents**, and **chains of operations**.","output-file":"langchain.html","skip_exec":true,"skip_showdoc":true,"title":"LangChain"},"headingText":"**1. What is LangChain?**","containsRefs":false,"markdown":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\nLangChain enables **developers** to build powerful applications that leverage **LLMs** like OpenAI's GPT, Cohere, Mistral, DeepSeek, and more.\n\n### **Why Use LangChain?**\n- **Simplifies LLM integration** with APIs, documents, and databases.\n- **Provides modular components** for easy prototyping and scaling.\n- **Supports memory and state management** for multi-turn conversations.\n- **Built-in retrieval and knowledge augmentation** for reducing hallucinations.\n- **Enables reasoning and action** using **LLM Agents**.\n\n---\n\n## **2. Core Components of LangChain**\nLangChain consists of **six primary components**:\n\n| Component | Description |\n|-----------|-------------|\n| **LLMs** | Interface for calling language models (GPT-4, LLaMA, Claude, etc.) |\n| **Chains** | Sequences of operations that process inputs and outputs |\n| **Agents** | AI decision-making system that dynamically selects tools |\n| **Memory** | Stores conversation history and user state |\n| **Retrieval** | Enables RAG with vector databases |\n| **Tools & Utilities** | Functions like Google Search, APIs, or code execution |\n\n---\n\n## **3. Setting Up LangChain**\n### **Install LangChain and Dependencies**\n```bash\npip install langchain openai chromadb faiss-cpu tiktoken\n```\n\n### **Set Up API Keys (Example: OpenAI)**\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n```\n\n---\n\n## **4. Using LLMs in LangChain**\n### **Basic Example with OpenAI's GPT-4**\n```python\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7)\nresponse = llm.predict(\"What is LangChain?\")\nprint(response)\n```\n\n### **Using LLaMA or Other Local Models**\n```python\nfrom langchain.llms import Ollama\n\nllm = Ollama(model=\"llama2\")\nresponse = llm(\"Tell me about AI applications.\")\nprint(response)\n```\n\n---\n\n## **5. Chains: Creating Pipelines**\nA **Chain** is a sequence of operations (e.g., input → LLM → output).\n\n### **Basic LLM Chain**\n```python\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"What are the top 3 applications of {technology}?\")\nchain = LLMChain(llm=llm, prompt=prompt)\n\nresponse = chain.run(technology=\"Machine Learning\")\nprint(response)\n```\n\n### **Sequential Chains (Multiple Steps)**\n```python\nfrom langchain.chains import SimpleSequentialChain\n\nchain1 = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Explain {concept}\"))\nchain2 = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Summarize in one line: {text}\"))\n\npipeline = SimpleSequentialChain(chains=[chain1, chain2])\nresponse = pipeline.run(\"Quantum Computing\")\nprint(response)\n```\n\n---\n\n## **6. Agents: Dynamic AI with Tool Use**\nAgents allow an LLM to **choose tools dynamically** rather than following a fixed flow.\n\n### **Basic Agent with OpenAI Functions**\n```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.tools import Tool\nfrom langchain.chat_models import ChatOpenAI\n\ndef search_tool(query):\n    return f\"Searching for {query} on the web...\"\n\ntools = [Tool(name=\"WebSearch\", func=search_tool, description=\"Searches the web\")]\n\nagent = initialize_agent(\n    tools=tools,\n    llm=ChatOpenAI(model=\"gpt-4\"),\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\nresponse = agent.run(\"What is the latest AI research?\")\nprint(response)\n```\n\n### **Self-Reflecting AI Agent**\n```python\nfrom langchain.agents import OpenAIFunctionsAgent\nfrom langchain.agents.agent import AgentExecutor\n\nagent = OpenAIFunctionsAgent.from_llm(llm=ChatOpenAI(model=\"gpt-4\"))\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresponse = executor.invoke(\"Find the latest AI papers.\")\nprint(response)\n```\n\n---\n\n## **7. Memory and State Management**\nBy default, LLMs do not remember past interactions. **Memory** in LangChain enables persistence.\n\n### **Adding Memory to Conversations**\n```python\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(llm=llm, memory=memory)\n\nprint(conversation.run(\"Hello, who are you?\"))\nprint(conversation.run(\"Can you summarize our conversation?\"))\n```\n\n### **Different Memory Types**\n| Memory Type | Use Case |\n|-------------|----------|\n| **ConversationBufferMemory** | Stores complete conversation history |\n| **ConversationSummaryMemory** | Summarizes past interactions |\n| **VectorStoreRetrieverMemory** | Uses embeddings to store long-term memory |\n\n---\n\n## **8. Retrieval-Augmented Generation (RAG)**\nRAG combines **vector search** with **LLMs** to **reduce hallucinations**.\n\n### **Loading a Document and Storing Embeddings**\n```python\nfrom langchain.document_loaders import TextLoader\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Load and split documents\nloader = TextLoader(\"docs.txt\")\ndocuments = loader.load()\n\n# Store as vector embeddings\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(documents, embeddings)\n```\n\n### **Retrieving and Passing to LLM**\n```python\nretriever = vectorstore.as_retriever()\nretrieved_docs = retriever.get_relevant_documents(\"What is AI?\")\n```\n\n---\n\n## **9. LangChain Tools and Integrations**\nLangChain connects with **APIs, databases, and search engines**.\n\n### **Google Search API**\n```python\nfrom langchain.tools import Tool\nfrom langchain.utilities import GoogleSearchAPIWrapper\n\nsearch = GoogleSearchAPIWrapper()\ntool = Tool(name=\"Google Search\", func=search.run)\n\nresult = tool.run(\"Latest AI news\")\nprint(result)\n```\n\n### **Code Execution**\n```python\nfrom langchain.tools import PythonREPLTool\n\npython_tool = PythonREPLTool()\nprint(python_tool.run(\"import math; math.sqrt(16)\"))\n```\n\n---\n\n## **10. Scaling and Optimization**\n### **Optimizing Token Usage**\n```python\nimport tiktoken\n\nencoder = tiktoken.get_encoding(\"cl100k_base\")\nprint(len(encoder.encode(\"This is a test message.\")))  # Token count\n```\n\n### **Using Local LLMs Instead of API Calls**\nFor privacy or cost efficiency, use models like LLaMA or Mistral:\n```python\nfrom langchain.llms import Ollama\nlocal_llm = Ollama(model=\"mistral\")\n```\n\n### **Batch Processing for Efficiency**\n```python\nresponses = llm.batch([\"Explain AI\", \"Define RAG\", \"What is LangChain?\"])\nprint(responses)\n```\n\n---\n\n## **11. Example Architectures and Use Cases**\n| Use Case | LangChain Components Used |\n|----------|---------------------------|\n| **AI-Powered Chatbots** | Memory, Chains, LLMs |\n| **Retrieval-Augmented QA Systems** | Retrieval, Vector Stores, LLMs |\n| **AI Agents for Research** | Agents, Tools, API Integrations |\n| **Automated Code Generation** | LLMs, Python Execution Tools |\n| **Personalized Assistants** | Memory, State Tracking |\n\n---\n\n## **Final Thoughts**\n- **LangChain simplifies LLM development** by abstracting complex workflows.\n- **It integrates retrieval, memory, and agents** for dynamic applications.\n- **It supports local and cloud-based LLMs**, ensuring flexibility.\n- **Optimizing context size, retrieval methods, and caching** improves efficiency.\n\nLangChain is the foundation for **building advanced, production-ready AI applications** with modular, scalable components.\n","srcMarkdownNoYaml":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n## **1. What is LangChain?**\nLangChain enables **developers** to build powerful applications that leverage **LLMs** like OpenAI's GPT, Cohere, Mistral, DeepSeek, and more.\n\n### **Why Use LangChain?**\n- **Simplifies LLM integration** with APIs, documents, and databases.\n- **Provides modular components** for easy prototyping and scaling.\n- **Supports memory and state management** for multi-turn conversations.\n- **Built-in retrieval and knowledge augmentation** for reducing hallucinations.\n- **Enables reasoning and action** using **LLM Agents**.\n\n---\n\n## **2. Core Components of LangChain**\nLangChain consists of **six primary components**:\n\n| Component | Description |\n|-----------|-------------|\n| **LLMs** | Interface for calling language models (GPT-4, LLaMA, Claude, etc.) |\n| **Chains** | Sequences of operations that process inputs and outputs |\n| **Agents** | AI decision-making system that dynamically selects tools |\n| **Memory** | Stores conversation history and user state |\n| **Retrieval** | Enables RAG with vector databases |\n| **Tools & Utilities** | Functions like Google Search, APIs, or code execution |\n\n---\n\n## **3. Setting Up LangChain**\n### **Install LangChain and Dependencies**\n```bash\npip install langchain openai chromadb faiss-cpu tiktoken\n```\n\n### **Set Up API Keys (Example: OpenAI)**\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n```\n\n---\n\n## **4. Using LLMs in LangChain**\n### **Basic Example with OpenAI's GPT-4**\n```python\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7)\nresponse = llm.predict(\"What is LangChain?\")\nprint(response)\n```\n\n### **Using LLaMA or Other Local Models**\n```python\nfrom langchain.llms import Ollama\n\nllm = Ollama(model=\"llama2\")\nresponse = llm(\"Tell me about AI applications.\")\nprint(response)\n```\n\n---\n\n## **5. Chains: Creating Pipelines**\nA **Chain** is a sequence of operations (e.g., input → LLM → output).\n\n### **Basic LLM Chain**\n```python\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"What are the top 3 applications of {technology}?\")\nchain = LLMChain(llm=llm, prompt=prompt)\n\nresponse = chain.run(technology=\"Machine Learning\")\nprint(response)\n```\n\n### **Sequential Chains (Multiple Steps)**\n```python\nfrom langchain.chains import SimpleSequentialChain\n\nchain1 = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Explain {concept}\"))\nchain2 = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Summarize in one line: {text}\"))\n\npipeline = SimpleSequentialChain(chains=[chain1, chain2])\nresponse = pipeline.run(\"Quantum Computing\")\nprint(response)\n```\n\n---\n\n## **6. Agents: Dynamic AI with Tool Use**\nAgents allow an LLM to **choose tools dynamically** rather than following a fixed flow.\n\n### **Basic Agent with OpenAI Functions**\n```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.tools import Tool\nfrom langchain.chat_models import ChatOpenAI\n\ndef search_tool(query):\n    return f\"Searching for {query} on the web...\"\n\ntools = [Tool(name=\"WebSearch\", func=search_tool, description=\"Searches the web\")]\n\nagent = initialize_agent(\n    tools=tools,\n    llm=ChatOpenAI(model=\"gpt-4\"),\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\nresponse = agent.run(\"What is the latest AI research?\")\nprint(response)\n```\n\n### **Self-Reflecting AI Agent**\n```python\nfrom langchain.agents import OpenAIFunctionsAgent\nfrom langchain.agents.agent import AgentExecutor\n\nagent = OpenAIFunctionsAgent.from_llm(llm=ChatOpenAI(model=\"gpt-4\"))\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresponse = executor.invoke(\"Find the latest AI papers.\")\nprint(response)\n```\n\n---\n\n## **7. Memory and State Management**\nBy default, LLMs do not remember past interactions. **Memory** in LangChain enables persistence.\n\n### **Adding Memory to Conversations**\n```python\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(llm=llm, memory=memory)\n\nprint(conversation.run(\"Hello, who are you?\"))\nprint(conversation.run(\"Can you summarize our conversation?\"))\n```\n\n### **Different Memory Types**\n| Memory Type | Use Case |\n|-------------|----------|\n| **ConversationBufferMemory** | Stores complete conversation history |\n| **ConversationSummaryMemory** | Summarizes past interactions |\n| **VectorStoreRetrieverMemory** | Uses embeddings to store long-term memory |\n\n---\n\n## **8. Retrieval-Augmented Generation (RAG)**\nRAG combines **vector search** with **LLMs** to **reduce hallucinations**.\n\n### **Loading a Document and Storing Embeddings**\n```python\nfrom langchain.document_loaders import TextLoader\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Load and split documents\nloader = TextLoader(\"docs.txt\")\ndocuments = loader.load()\n\n# Store as vector embeddings\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(documents, embeddings)\n```\n\n### **Retrieving and Passing to LLM**\n```python\nretriever = vectorstore.as_retriever()\nretrieved_docs = retriever.get_relevant_documents(\"What is AI?\")\n```\n\n---\n\n## **9. LangChain Tools and Integrations**\nLangChain connects with **APIs, databases, and search engines**.\n\n### **Google Search API**\n```python\nfrom langchain.tools import Tool\nfrom langchain.utilities import GoogleSearchAPIWrapper\n\nsearch = GoogleSearchAPIWrapper()\ntool = Tool(name=\"Google Search\", func=search.run)\n\nresult = tool.run(\"Latest AI news\")\nprint(result)\n```\n\n### **Code Execution**\n```python\nfrom langchain.tools import PythonREPLTool\n\npython_tool = PythonREPLTool()\nprint(python_tool.run(\"import math; math.sqrt(16)\"))\n```\n\n---\n\n## **10. Scaling and Optimization**\n### **Optimizing Token Usage**\n```python\nimport tiktoken\n\nencoder = tiktoken.get_encoding(\"cl100k_base\")\nprint(len(encoder.encode(\"This is a test message.\")))  # Token count\n```\n\n### **Using Local LLMs Instead of API Calls**\nFor privacy or cost efficiency, use models like LLaMA or Mistral:\n```python\nfrom langchain.llms import Ollama\nlocal_llm = Ollama(model=\"mistral\")\n```\n\n### **Batch Processing for Efficiency**\n```python\nresponses = llm.batch([\"Explain AI\", \"Define RAG\", \"What is LangChain?\"])\nprint(responses)\n```\n\n---\n\n## **11. Example Architectures and Use Cases**\n| Use Case | LangChain Components Used |\n|----------|---------------------------|\n| **AI-Powered Chatbots** | Memory, Chains, LLMs |\n| **Retrieval-Augmented QA Systems** | Retrieval, Vector Stores, LLMs |\n| **AI Agents for Research** | Agents, Tools, API Integrations |\n| **Automated Code Generation** | LLMs, Python Execution Tools |\n| **Personalized Assistants** | Memory, State Tracking |\n\n---\n\n## **Final Thoughts**\n- **LangChain simplifies LLM development** by abstracting complex workflows.\n- **It integrates retrieval, memory, and agents** for dynamic applications.\n- **It supports local and cloud-based LLMs**, ensuring flexibility.\n- **Optimizing context size, retrieval methods, and caching** improves efficiency.\n\nLangChain is the foundation for **building advanced, production-ready AI applications** with modular, scalable components.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"langchain.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.27","comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"author":"Benedict Thekkel","theme":{"light":"flatly","dark":"darkly"},"description":"LangChain is a **framework** designed to simplify the development of applications that use **Large Language Models (LLMs)**. It provides tools to integrate LLMs with **external data sources**, **memory**, **retrieval-augmented generation (RAG)**, **agents**, and **chains of operations**.","skip_exec":true,"skip_showdoc":true,"title":"LangChain"},"extensions":{"book":{"multiFile":true}}},"gfm":{"identifier":{"display-name":"Github (GFM)","target-format":"gfm","base-format":"gfm"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":false,"output-ext":"md","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"variant":"+autolink_bare_uris+emoji+footnotes+gfm_auto_identifiers+pipe_tables+strikeout+task_lists+tex_math_dollars"},"pandoc":{"standalone":true,"default-image-extension":"png","to":"commonmark","output-file":"langchain.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"description":"LangChain is a **framework** designed to simplify the development of applications that use **Large Language Models (LLMs)**. It provides tools to integrate LLMs with **external data sources**, **memory**, **retrieval-augmented generation (RAG)**, **agents**, and **chains of operations**.","skip_exec":true,"skip_showdoc":true,"title":"LangChain"}}},"projectFormats":["html","gfm"]}