{"title":"Semantic Segmentation","markdown":{"yaml":{"description":"Assigns a class label to each pixel, but does not distinguish between instances of the same class.","output-file":"semantic_segmentation.html","skip_exec":true,"skip_showdoc":true,"title":"Semantic Segmentation"},"headingText":"1. **Popular Datasets for Semantic Segmentation**","containsRefs":false,"markdown":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n\nA wide variety of datasets are available for training and evaluating semantic segmentation models. Each dataset has unique characteristics depending on the application domain, such as autonomous driving, general object segmentation, or medical imaging.\n\n### a. **Autonomous Driving Datasets**\n1. **Cityscapes**\n   - **Description**: Large-scale dataset for urban scene understanding. Includes street-level imagery from various German cities.\n   - **Classes**: 30 classes (e.g., road, person, car, building).\n   - **Size**: 5,000 fine-annotated images (2975 train, 500 validation, 1525 test), 20,000 coarse annotations.\n   - **Resolution**: 2048×1024 pixels.\n   - **Link**: [Cityscapes Dataset](https://www.cityscapes-dataset.com/)\n\n2. **Mapillary Vistas**\n   - **Description**: A diverse dataset collected from various countries, designed for street-level segmentation tasks.\n   - **Classes**: 66 object categories.\n   - **Size**: 25,000 annotated high-resolution images.\n   - **Resolution**: Varies, high resolution.\n   - **Link**: [Mapillary Vistas](https://www.mapillary.com/dataset/vistas)\n\n3. **CamVid**\n   - **Description**: One of the earlier datasets for autonomous driving, with video sequences.\n   - **Classes**: 32 classes.\n   - **Size**: 701 labeled frames.\n   - **Resolution**: 960×720 pixels.\n   - **Link**: [CamVid Dataset](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)\n\n### b. **General Object Segmentation Datasets**\n1. **PASCAL VOC 2012**\n   - **Description**: Widely used dataset for image classification, detection, and segmentation tasks.\n   - **Classes**: 21 object categories.\n   - **Size**: 1,464 images (train), 1,449 (val), 1,456 (test).\n   - **Resolution**: Varies.\n   - **Link**: [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)\n\n2. **MS COCO (Common Objects in Context)**\n   - **Description**: Large-scale object detection, segmentation, and captioning dataset.\n   - **Classes**: 80 object categories.\n   - **Size**: 123,287 images with pixel-wise annotations.\n   - **Resolution**: Varies.\n   - **Link**: [MS COCO](https://cocodataset.org/)\n\n### c. **Medical Imaging Datasets**\n1. **Lung Nodule Analysis (LUNA)**\n   - **Description**: A dataset for detecting and segmenting lung nodules in CT scans.\n   - **Classes**: Lung nodules.\n   - **Size**: 888 CT scans.\n   - **Resolution**: Voxel-wise 3D data.\n   - **Link**: [LUNA Dataset](https://luna16.grand-challenge.org/)\n\n2. **BraTS (Brain Tumor Segmentation)**\n   - **Description**: A dataset for segmenting brain tumors using MRI scans.\n   - **Classes**: Tumor, normal tissue.\n   - **Size**: 500+ 3D MRI images.\n   - **Resolution**: Voxel-wise 3D data.\n   - **Link**: [BraTS Dataset](https://www.med.upenn.edu/sbia/brats2018.html)\n\n## 2. **Popular Models for Semantic Segmentation**\n\n### a. **Fully Convolutional Networks (FCN)**\n- **Architecture**: The first major deep learning-based approach for semantic segmentation. FCNs use convolutional layers for both feature extraction and pixel-wise classification.\n- **Key Idea**: Replace fully connected layers with convolutional layers for dense pixel prediction.\n- **References**: Long et al., 2015, [FCN Paper](https://arxiv.org/abs/1411.4038)\n\n### b. **U-Net**\n- **Architecture**: Symmetrical \"U\"-shaped network with encoder-decoder architecture.\n- **Key Idea**: Encoder extracts features, and the decoder upsamples to full resolution using skip connections to recover spatial information.\n- **Applications**: Extremely popular in medical imaging.\n- **References**: Ronneberger et al., 2015, [U-Net Paper](https://arxiv.org/abs/1505.04597)\n\n### c. **DeepLab (v1, v2, v3, v3+)**\n- **Architecture**: Encoder-decoder architecture with Atrous (dilated) convolutions to capture multi-scale context.\n- **Key Idea**: Atrous convolutions and ASPP (Atrous Spatial Pyramid Pooling) enable capturing objects at multiple scales.\n- **DeepLab v3+**: Combines spatial pyramid pooling with a decoder module for better object boundary detection.\n- **References**: Chen et al., 2017, [DeepLab Paper](https://arxiv.org/abs/1706.05587)\n\n### d. **SegNet**\n- **Architecture**: Encoder-decoder architecture that recovers resolution using max-pooling indices from the encoder.\n- **Key Idea**: Efficient upsampling using indices from max-pooling in the encoder, which reduces computational cost.\n- **References**: Badrinarayanan et al., 2015, [SegNet Paper](https://arxiv.org/abs/1511.00561)\n\n### e. **PSPNet (Pyramid Scene Parsing Network)**\n- **Architecture**: Uses a pyramid pooling module to capture multi-scale global context.\n- **Key Idea**: Captures global scene-level context using pyramid pooling before the final pixel-wise prediction.\n- **References**: Zhao et al., 2017, [PSPNet Paper](https://arxiv.org/abs/1612.01105)\n\n### f. **HRNet (High-Resolution Network)**\n- **Architecture**: Maintains high-resolution feature maps throughout the network while using multi-scale fusion.\n- **Key Idea**: Avoids the downsampling-heavy nature of many segmentation networks, preserving spatial detail.\n- **References**: Wang et al., 2020, [HRNet Paper](https://arxiv.org/abs/1908.07919)\n\n## 3. **Important Hyperparameters**\n\nTuning hyperparameters is crucial to improving the performance of segmentation models. Below are the most important ones:\n\n### a. **Learning Rate**\n- **Description**: Controls the step size during optimization.\n- **Typical Range**: 0.0001 – 0.01 (with learning rate schedules like cosine annealing, or warm restarts).\n\n### b. **Batch Size**\n- **Description**: The number of samples processed before the model is updated.\n- **Typical Range**: 2 – 16 (for large images due to memory constraints).\n\n### c. **Number of Filters / Feature Maps**\n- **Description**: Number of filters in convolutional layers, which controls model capacity.\n- **Typical Range**: 32 – 512 per layer, depending on model depth and complexity.\n\n### d. **Optimizer**\n- **Popular Choices**: \n  - Adam (adaptive learning rate).\n  - SGD with momentum (common in large-scale datasets).\n\n### e. **Weight Decay / L2 Regularization**\n- **Description**: Helps prevent overfitting by penalizing large weights.\n- **Typical Range**: 0.0001 – 0.001.\n\n## 4. **Popular Loss Functions**\n\nIn semantic segmentation, the loss function plays a critical role in training the network by minimizing pixel-wise errors. Common loss functions include:\n\n### a. **Cross-Entropy Loss**\n- **Description**: The most common loss for multi-class pixel-wise classification tasks.\n- **Formula**:  $L = - \\sum_{i} y_i \\log(\\hat{y}_i)$\n  where $ y_i $ is the true label and $ \\hat{y}_i $ is the predicted probability.\n\n### b. **Dice Loss**\n- **Description**: Measures the overlap between predicted and ground truth segmentation.\n- **Formula**: $L = 1 - \\frac{2 |Y \\cap \\hat{Y}|}{|Y| + |\\hat{Y}|}$\n  where $ Y $ is the ground truth mask, and $ \\hat{Y} $ is the predicted mask.\n\n### c. **Intersection over Union (IoU) Loss**\n- **Description**: Measures the overlap between the predicted and ground truth areas.\n- **Formula**: $\\text{IoU} = \\frac{|Y \\cap \\hat{Y}|}{|Y \\cup \\hat{Y}|}$\n  where $ |Y| $ is the area of ground truth and $ |\\hat{Y}| $ is the area of prediction.\n\n### d. **Tversky Loss**\n- **Description**: A generalization of Dice Loss, controlling false positives and false negatives.\n- **Formula**:\n  $\n  L = 1 - \\frac{|Y \\cap \\hat{Y}|}{|Y \\cap \\hat{Y}| + \\alpha |Y \\setminus \\hat{Y}| + \\beta |\\hat{Y} \\setminus Y|}\n  $\n\n## 5. **Other Important Topics**\n\n### a. **Data Augmentation**\n- **Techniques**: Random crop, horizontal/vertical flipping, color jittering, and elastic deformation.\n- **Purpose**: Prevent overfitting and improve generalization.\n\n### b. **Post-Processing Techniques**\n- **CRF (Conditional Random Field)**: Often used as a post-processing step to refine segmentation boundaries by enforcing spatial consistency.\n\n### c. **Evaluation Metrics**\n- **Mean Intersection over Union (mIoU)**: The most widely used evaluation metric for segmentation tasks.\n- **Pixel Accuracy**: The ratio of correctly predicted pixels to total pixels.\n\n## 6. **References**\n\n1. Long, J., Shelhamer, E., & Darrell, T. (2015). \"Fully Convolutional Networks for Semantic Segmentation.\" CVPR. [Link](https://arxiv.org/abs/1411.4038)\n2. Ronneberger, O., Fischer, P., & Brox, T. (2015). \"U-Net: Convolutional Networks for Biomedical Image Segmentation.\" MICCAI. [Link](https://arxiv.org/abs/1505.04597)\n3. Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2017). \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.\" PAMI. [Link](https://arxiv.org/abs/1606.00915)\n4. Zhao, H., Shi, J., Qi, X., Wang, X., & Jia, J. (2017). \"Pyramid Scene Parsing Network.\" CVPR. [Link](https://arxiv.org/abs/1612.01105)\n5. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., & Liu, W. (2020). \"Deep High-Resolution Representation Learning for Visual Recognition.\" PAMI. [Link](https://arxiv.org/abs/1908.07919)\n","srcMarkdownNoYaml":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n## 1. **Popular Datasets for Semantic Segmentation**\n\nA wide variety of datasets are available for training and evaluating semantic segmentation models. Each dataset has unique characteristics depending on the application domain, such as autonomous driving, general object segmentation, or medical imaging.\n\n### a. **Autonomous Driving Datasets**\n1. **Cityscapes**\n   - **Description**: Large-scale dataset for urban scene understanding. Includes street-level imagery from various German cities.\n   - **Classes**: 30 classes (e.g., road, person, car, building).\n   - **Size**: 5,000 fine-annotated images (2975 train, 500 validation, 1525 test), 20,000 coarse annotations.\n   - **Resolution**: 2048×1024 pixels.\n   - **Link**: [Cityscapes Dataset](https://www.cityscapes-dataset.com/)\n\n2. **Mapillary Vistas**\n   - **Description**: A diverse dataset collected from various countries, designed for street-level segmentation tasks.\n   - **Classes**: 66 object categories.\n   - **Size**: 25,000 annotated high-resolution images.\n   - **Resolution**: Varies, high resolution.\n   - **Link**: [Mapillary Vistas](https://www.mapillary.com/dataset/vistas)\n\n3. **CamVid**\n   - **Description**: One of the earlier datasets for autonomous driving, with video sequences.\n   - **Classes**: 32 classes.\n   - **Size**: 701 labeled frames.\n   - **Resolution**: 960×720 pixels.\n   - **Link**: [CamVid Dataset](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)\n\n### b. **General Object Segmentation Datasets**\n1. **PASCAL VOC 2012**\n   - **Description**: Widely used dataset for image classification, detection, and segmentation tasks.\n   - **Classes**: 21 object categories.\n   - **Size**: 1,464 images (train), 1,449 (val), 1,456 (test).\n   - **Resolution**: Varies.\n   - **Link**: [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)\n\n2. **MS COCO (Common Objects in Context)**\n   - **Description**: Large-scale object detection, segmentation, and captioning dataset.\n   - **Classes**: 80 object categories.\n   - **Size**: 123,287 images with pixel-wise annotations.\n   - **Resolution**: Varies.\n   - **Link**: [MS COCO](https://cocodataset.org/)\n\n### c. **Medical Imaging Datasets**\n1. **Lung Nodule Analysis (LUNA)**\n   - **Description**: A dataset for detecting and segmenting lung nodules in CT scans.\n   - **Classes**: Lung nodules.\n   - **Size**: 888 CT scans.\n   - **Resolution**: Voxel-wise 3D data.\n   - **Link**: [LUNA Dataset](https://luna16.grand-challenge.org/)\n\n2. **BraTS (Brain Tumor Segmentation)**\n   - **Description**: A dataset for segmenting brain tumors using MRI scans.\n   - **Classes**: Tumor, normal tissue.\n   - **Size**: 500+ 3D MRI images.\n   - **Resolution**: Voxel-wise 3D data.\n   - **Link**: [BraTS Dataset](https://www.med.upenn.edu/sbia/brats2018.html)\n\n## 2. **Popular Models for Semantic Segmentation**\n\n### a. **Fully Convolutional Networks (FCN)**\n- **Architecture**: The first major deep learning-based approach for semantic segmentation. FCNs use convolutional layers for both feature extraction and pixel-wise classification.\n- **Key Idea**: Replace fully connected layers with convolutional layers for dense pixel prediction.\n- **References**: Long et al., 2015, [FCN Paper](https://arxiv.org/abs/1411.4038)\n\n### b. **U-Net**\n- **Architecture**: Symmetrical \"U\"-shaped network with encoder-decoder architecture.\n- **Key Idea**: Encoder extracts features, and the decoder upsamples to full resolution using skip connections to recover spatial information.\n- **Applications**: Extremely popular in medical imaging.\n- **References**: Ronneberger et al., 2015, [U-Net Paper](https://arxiv.org/abs/1505.04597)\n\n### c. **DeepLab (v1, v2, v3, v3+)**\n- **Architecture**: Encoder-decoder architecture with Atrous (dilated) convolutions to capture multi-scale context.\n- **Key Idea**: Atrous convolutions and ASPP (Atrous Spatial Pyramid Pooling) enable capturing objects at multiple scales.\n- **DeepLab v3+**: Combines spatial pyramid pooling with a decoder module for better object boundary detection.\n- **References**: Chen et al., 2017, [DeepLab Paper](https://arxiv.org/abs/1706.05587)\n\n### d. **SegNet**\n- **Architecture**: Encoder-decoder architecture that recovers resolution using max-pooling indices from the encoder.\n- **Key Idea**: Efficient upsampling using indices from max-pooling in the encoder, which reduces computational cost.\n- **References**: Badrinarayanan et al., 2015, [SegNet Paper](https://arxiv.org/abs/1511.00561)\n\n### e. **PSPNet (Pyramid Scene Parsing Network)**\n- **Architecture**: Uses a pyramid pooling module to capture multi-scale global context.\n- **Key Idea**: Captures global scene-level context using pyramid pooling before the final pixel-wise prediction.\n- **References**: Zhao et al., 2017, [PSPNet Paper](https://arxiv.org/abs/1612.01105)\n\n### f. **HRNet (High-Resolution Network)**\n- **Architecture**: Maintains high-resolution feature maps throughout the network while using multi-scale fusion.\n- **Key Idea**: Avoids the downsampling-heavy nature of many segmentation networks, preserving spatial detail.\n- **References**: Wang et al., 2020, [HRNet Paper](https://arxiv.org/abs/1908.07919)\n\n## 3. **Important Hyperparameters**\n\nTuning hyperparameters is crucial to improving the performance of segmentation models. Below are the most important ones:\n\n### a. **Learning Rate**\n- **Description**: Controls the step size during optimization.\n- **Typical Range**: 0.0001 – 0.01 (with learning rate schedules like cosine annealing, or warm restarts).\n\n### b. **Batch Size**\n- **Description**: The number of samples processed before the model is updated.\n- **Typical Range**: 2 – 16 (for large images due to memory constraints).\n\n### c. **Number of Filters / Feature Maps**\n- **Description**: Number of filters in convolutional layers, which controls model capacity.\n- **Typical Range**: 32 – 512 per layer, depending on model depth and complexity.\n\n### d. **Optimizer**\n- **Popular Choices**: \n  - Adam (adaptive learning rate).\n  - SGD with momentum (common in large-scale datasets).\n\n### e. **Weight Decay / L2 Regularization**\n- **Description**: Helps prevent overfitting by penalizing large weights.\n- **Typical Range**: 0.0001 – 0.001.\n\n## 4. **Popular Loss Functions**\n\nIn semantic segmentation, the loss function plays a critical role in training the network by minimizing pixel-wise errors. Common loss functions include:\n\n### a. **Cross-Entropy Loss**\n- **Description**: The most common loss for multi-class pixel-wise classification tasks.\n- **Formula**:  $L = - \\sum_{i} y_i \\log(\\hat{y}_i)$\n  where $ y_i $ is the true label and $ \\hat{y}_i $ is the predicted probability.\n\n### b. **Dice Loss**\n- **Description**: Measures the overlap between predicted and ground truth segmentation.\n- **Formula**: $L = 1 - \\frac{2 |Y \\cap \\hat{Y}|}{|Y| + |\\hat{Y}|}$\n  where $ Y $ is the ground truth mask, and $ \\hat{Y} $ is the predicted mask.\n\n### c. **Intersection over Union (IoU) Loss**\n- **Description**: Measures the overlap between the predicted and ground truth areas.\n- **Formula**: $\\text{IoU} = \\frac{|Y \\cap \\hat{Y}|}{|Y \\cup \\hat{Y}|}$\n  where $ |Y| $ is the area of ground truth and $ |\\hat{Y}| $ is the area of prediction.\n\n### d. **Tversky Loss**\n- **Description**: A generalization of Dice Loss, controlling false positives and false negatives.\n- **Formula**:\n  $\n  L = 1 - \\frac{|Y \\cap \\hat{Y}|}{|Y \\cap \\hat{Y}| + \\alpha |Y \\setminus \\hat{Y}| + \\beta |\\hat{Y} \\setminus Y|}\n  $\n\n## 5. **Other Important Topics**\n\n### a. **Data Augmentation**\n- **Techniques**: Random crop, horizontal/vertical flipping, color jittering, and elastic deformation.\n- **Purpose**: Prevent overfitting and improve generalization.\n\n### b. **Post-Processing Techniques**\n- **CRF (Conditional Random Field)**: Often used as a post-processing step to refine segmentation boundaries by enforcing spatial consistency.\n\n### c. **Evaluation Metrics**\n- **Mean Intersection over Union (mIoU)**: The most widely used evaluation metric for segmentation tasks.\n- **Pixel Accuracy**: The ratio of correctly predicted pixels to total pixels.\n\n## 6. **References**\n\n1. Long, J., Shelhamer, E., & Darrell, T. (2015). \"Fully Convolutional Networks for Semantic Segmentation.\" CVPR. [Link](https://arxiv.org/abs/1411.4038)\n2. Ronneberger, O., Fischer, P., & Brox, T. (2015). \"U-Net: Convolutional Networks for Biomedical Image Segmentation.\" MICCAI. [Link](https://arxiv.org/abs/1505.04597)\n3. Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2017). \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.\" PAMI. [Link](https://arxiv.org/abs/1606.00915)\n4. Zhao, H., Shi, J., Qi, X., Wang, X., & Jia, J. (2017). \"Pyramid Scene Parsing Network.\" CVPR. [Link](https://arxiv.org/abs/1612.01105)\n5. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., & Liu, W. (2020). \"Deep High-Resolution Representation Learning for Visual Recognition.\" PAMI. [Link](https://arxiv.org/abs/1908.07919)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"semantic_segmentation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.27","comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"author":"Benedict Thekkel","theme":{"light":"flatly","dark":"darkly"},"description":"Assigns a class label to each pixel, but does not distinguish between instances of the same class.","skip_exec":true,"skip_showdoc":true,"title":"Semantic Segmentation"},"extensions":{"book":{"multiFile":true}}},"gfm":{"identifier":{"display-name":"Github (GFM)","target-format":"gfm","base-format":"gfm"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":false,"output-ext":"md","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"variant":"+autolink_bare_uris+emoji+footnotes+gfm_auto_identifiers+pipe_tables+strikeout+task_lists+tex_math_dollars"},"pandoc":{"standalone":true,"default-image-extension":"png","to":"commonmark","output-file":"semantic_segmentation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"description":"Assigns a class label to each pixel, but does not distinguish between instances of the same class.","skip_exec":true,"skip_showdoc":true,"title":"Semantic Segmentation"}}},"projectFormats":["html","gfm"]}