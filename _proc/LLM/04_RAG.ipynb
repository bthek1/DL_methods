{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: RAG\n",
    "output-file: rag.html\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "title: RAG\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb31dc8-82b2-40c3-a67e-b0b3b83dd8a0",
   "metadata": {},
   "source": [
    "## **1. What is Retrieval-Augmented Generation (RAG)?**\n",
    "Retrieval-Augmented Generation (**RAG**) is a method that enhances **Large Language Models (LLMs)** by integrating **external knowledge retrieval** before generating responses. Unlike standard LLMs that rely solely on pre-trained knowledge, RAG **retrieves relevant documents** from a knowledge base and **injects them into the prompt** before inference.\n",
    "\n",
    "### **Key Features of RAG:**\n",
    "- **Improves factual accuracy** by pulling real-time or domain-specific knowledge.\n",
    "- **Reduces hallucinations** by grounding the model in reliable sources.\n",
    "- **Enhances performance on domain-specific tasks** such as finance, healthcare, and legal analysis.\n",
    "- **Eliminates the need for full fine-tuning** by dynamically incorporating external information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12f95b-9514-4a20-b399-417423b4f021",
   "metadata": {},
   "source": [
    "## **2. How RAG Works**\n",
    "### **Step-by-Step Process**\n",
    "1. **User Query**: The user inputs a question or request.\n",
    "2. **Retrieval**: A search engine or vector database retrieves the most relevant documents from a knowledge source.\n",
    "3. **Augmentation**: The retrieved documents are inserted into the model’s prompt.\n",
    "4. **Generation**: The LLM processes both the query and retrieved information to generate a response.\n",
    "\n",
    "### **Architecture Overview**\n",
    "Below is a high-level architecture of RAG:\n",
    "\n",
    "```\n",
    "User Query → Embedding Model → Vector Database → Top-K Document Retrieval → Prompt Augmentation → LLM → Response\n",
    "```\n",
    "\n",
    "### **Diagram: Basic RAG Workflow**\n",
    "```\n",
    "+-------------+        +----------------+        +--------------------+        +-------------+\n",
    "| User Query  | ---->  |  Document Index| ---->  |  LLM Augmentation  | ---->  |  AI Response |\n",
    "+-------------+        +----------------+        +--------------------+        +-------------+\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a9559-e16d-42e9-b69f-efb1d4660ce9",
   "metadata": {},
   "source": [
    "## **3. Comparing RAG with Traditional LLM Methods**\n",
    "| Feature               | Standard LLMs | Fine-Tuned LLMs | RAG |\n",
    "|----------------------|--------------|----------------|-----|\n",
    "| **External Knowledge** | ❌ No | ✅ Limited | ✅ Yes |\n",
    "| **Memory Efficiency** | ✅ Yes | ❌ No (New Weights) | ✅ Yes |\n",
    "| **Real-Time Updates** | ❌ No | ❌ No | ✅ Yes |\n",
    "| **Accuracy Improvement** | ❌ Limited | ✅ Yes | ✅ Yes |\n",
    "| **Scalability** | ✅ High | ❌ Costly | ✅ High |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1452af6-3f95-4ba5-b3f1-c266b0b71ad4",
   "metadata": {},
   "source": [
    "## **4. Implementing RAG in Python**\n",
    "A basic **RAG system** consists of:\n",
    "- **LLM (e.g., OpenAI GPT, Mistral, LLaMA)**\n",
    "- **Vector database (e.g., FAISS, Pinecone, ChromaDB)**\n",
    "- **Embedding model (e.g., SentenceTransformers, OpenAI embeddings)**\n",
    "\n",
    "### **Step 1: Install Dependencies**\n",
    "```bash\n",
    "pip install langchain faiss-cpu openai sentence-transformers\n",
    "```\n",
    "\n",
    "### **Step 2: Load a Knowledge Base**\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents\n",
    "loader = TextLoader(\"data.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "### **Step 3: Generate Embeddings and Store in FAISS**\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert documents to embeddings\n",
    "doc_texts = [doc.page_content for doc in docs]\n",
    "doc_embeddings = embedding_model.encode(doc_texts)\n",
    "\n",
    "# Store embeddings in FAISS\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(doc_embeddings))\n",
    "```\n",
    "\n",
    "### **Step 4: Retrieve Relevant Documents**\n",
    "```python\n",
    "def retrieve_documents(query, k=3):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), k)\n",
    "    retrieved_texts = [doc_texts[i] for i in indices[0]]\n",
    "    return \"\\n\".join(retrieved_texts)\n",
    "\n",
    "query = \"Explain neural networks.\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "print(retrieved_docs)\n",
    "```\n",
    "\n",
    "### **Step 5: Pass Retrieved Data to LLM**\n",
    "```python\n",
    "import openai\n",
    "\n",
    "def generate_rag_response(query):\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    \n",
    "    prompt = f\"Use the following retrieved documents to answer:\\n\\n{retrieved_docs}\\n\\nUser: {query}\\nAssistant:\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example query\n",
    "response = generate_rag_response(\"What is deep learning?\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c71c0-e050-4163-9278-c4b3fe7dc5ef",
   "metadata": {},
   "source": [
    "## **5. Evaluating RAG Performance**\n",
    "To measure the effectiveness of RAG, compare:\n",
    "- **Retrieval Accuracy**: How relevant are the retrieved documents?\n",
    "- **Response Quality**: Does the LLM provide accurate answers based on the retrieval?\n",
    "- **Latency**: Is retrieval slowing down response generation?\n",
    "\n",
    "### **Performance Metrics**\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Recall@K** | Percentage of correct documents retrieved |\n",
    "| **BLEU Score** | Measures text similarity to ground truth |\n",
    "| **Response Latency** | Measures time taken to retrieve + generate response |\n",
    "\n",
    "### **Graph: Accuracy Improvement Using RAG**\n",
    "This graph shows accuracy improvement with RAG compared to a standalone LLM.\n",
    "\n",
    "```\n",
    "Accuracy (%)\n",
    "│\n",
    "│   90 ──  RAG-based LLM\n",
    "│\n",
    "│   75 ──  Fine-tuned LLM\n",
    "│\n",
    "│   60 ──  Standard LLM\n",
    "│\n",
    "└───────────────────\n",
    "        LLM Type\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653540a-24f3-4dbe-9aa0-5defd321e5e1",
   "metadata": {},
   "source": [
    "## **6. Scaling RAG for Large Applications**\n",
    "For production-scale RAG systems:\n",
    "1. **Use Distributed Vector Databases** (Pinecone, Weaviate) instead of FAISS.\n",
    "2. **Pre-filter Documents** to improve retrieval speed.\n",
    "3. **Optimize Context Window** to avoid overloading the LLM.\n",
    "4. **Use Hybrid Search** (BM25 + Embeddings) for better recall.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f1723-f17c-4636-91b5-a2d831ae9f5b",
   "metadata": {},
   "source": [
    "## **7. Challenges and Considerations**\n",
    "| Challenge | Solution |\n",
    "|-----------|------------|\n",
    "| **Latency in Retrieval** | Optimize vector search, use caching |\n",
    "| **Memory Consumption** | Use compressed embeddings, distributed storage |\n",
    "| **Data Drift** | Regularly update knowledge base |\n",
    "| **Hallucination Despite RAG** | Filter retrieved documents to ensure factual consistency |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e362a6-f864-4334-8a2d-a50ce2ba66f5",
   "metadata": {},
   "source": [
    "## **8. Advanced RAG Techniques**\n",
    "### **Multi-Stage RAG**\n",
    "Instead of a single retrieval step, **multi-stage RAG** refines retrieval by applying **re-ranking algorithms**.\n",
    "\n",
    "### **Graph-Based RAG**\n",
    "Instead of keyword matching, **knowledge graphs** can retrieve structured facts more accurately.\n",
    "\n",
    "### **Agent-Based RAG**\n",
    "Combines RAG with **autonomous AI agents** that perform multiple reasoning steps before generating a response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17feda72-a82e-46ae-a6af-b9f6dfa00121",
   "metadata": {},
   "source": [
    "## **9. Real-World Use Cases**\n",
    "| Industry | Application |\n",
    "|----------|------------|\n",
    "| **Healthcare** | Medical chatbots retrieving up-to-date research papers |\n",
    "| **Legal** | AI-powered legal document search and Q&A |\n",
    "| **Finance** | Market analysis by retrieving real-time reports |\n",
    "| **Customer Support** | AI assistants providing support from internal documentation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454e19f-fc41-4127-b72a-9aaf792b6a97",
   "metadata": {},
   "source": [
    "## **10. Summary**\n",
    "- **RAG enhances LLMs by integrating real-time information retrieval**.\n",
    "- **It prevents hallucinations and improves factual accuracy**.\n",
    "- **Using vector databases like FAISS or Pinecone enables fast retrieval**.\n",
    "- **Hybrid search and multi-stage retrieval can further optimize results**.\n",
    "- **Scaling RAG requires optimizing retrieval efficiency and memory usage**.\n",
    "\n",
    "By leveraging RAG, **LLMs can become more accurate, reliable, and adaptable without expensive fine-tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e65288-0632-4b18-98ea-0d5ce1d2d16e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
