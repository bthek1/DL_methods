{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Ollama is a lightweight, extensible framework designed for building and\n",
    "  running large language models (LLMs) on local machines. It provides a command-line\n",
    "  interface (CLI) that facilitates model management, customization, and interaction.\n",
    "  Here's a comprehensive guide to using Ollama, including essential commands and examples.\n",
    "output-file: ollama.html\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "title: Ollama\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d25416-0ed7-4f85-bf3c-bdbc07a44dc0",
   "metadata": {},
   "source": [
    "## **1. Installation**\n",
    "\n",
    "- **For Linux:**\n",
    "\n",
    "  Open your terminal and execute:\n",
    "\n",
    "  ```bash\n",
    "  curl -fsSL https://ollama.com/install.sh | sh\n",
    "  ```\n",
    "\n",
    "  This command downloads and installs Ollama on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aea2e0-1da6-4bd6-8a3a-22af6a9d8138",
   "metadata": {},
   "source": [
    "## **2. System Requirements**\n",
    "\n",
    "- **Operating System:** macOS or Linux\n",
    "- **Memory (RAM):** Minimum 8GB; 16GB or more recommended\n",
    "- **Storage:** At least 10GB of free space\n",
    "- **Processor:** Modern CPU from the last 5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fb26e-0997-41f9-8c81-2121ccd768c6",
   "metadata": {},
   "source": [
    "## **3. Basic CLI Commands**\n",
    "\n",
    "- **Start the Ollama Server:**\n",
    "\n",
    "  To run Ollama without the desktop application:\n",
    "\n",
    "  ```bash\n",
    "  ollama serve\n",
    "  ```\n",
    "\n",
    "- **Download a Model:**\n",
    "\n",
    "  To download a specific model:\n",
    "\n",
    "  ```bash\n",
    "  ollama pull <model-name>\n",
    "  ```\n",
    "\n",
    "  Replace `<model-name>` with the desired model's name, e.g., `llama3.2`.\n",
    "\n",
    "- **List Downloaded Models:**\n",
    "\n",
    "  To view all models available on your system:\n",
    "\n",
    "  ```bash\n",
    "  ollama list\n",
    "  ```\n",
    "\n",
    "- **Run a Model:**\n",
    "\n",
    "  To start a model and enter an interactive session:\n",
    "\n",
    "  ```bash\n",
    "  ollama run <model-name>\n",
    "  ```\n",
    "\n",
    "  For example:\n",
    "\n",
    "  ```bash\n",
    "  ollama run llama3.2\n",
    "  ```\n",
    "\n",
    "- **Stop a Running Model:**\n",
    "\n",
    "  To stop a specific model:\n",
    "\n",
    "  ```bash\n",
    "  ollama stop <model-name>\n",
    "  ```\n",
    "\n",
    "- **Remove a Model:**\n",
    "\n",
    "  To delete a model from your system:\n",
    "\n",
    "  ```bash\n",
    "  ollama rm <model-name>\n",
    "  ```\n",
    "\n",
    "- **Display Model Information:**\n",
    "\n",
    "  To view details about a specific model:\n",
    "\n",
    "  ```bash\n",
    "  ollama show <model-name>\n",
    "  ```\n",
    "\n",
    "- **List Running Models:**\n",
    "\n",
    "  To see which models are currently active:\n",
    "\n",
    "  ```bash\n",
    "  ollama ps\n",
    "  ```\n",
    "\n",
    "- **Access Help:**\n",
    "\n",
    "  For a list of available commands and their descriptions:\n",
    "\n",
    "  ```bash\n",
    "  ollama help\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8b786-4f0e-4d8e-a1cc-d749ce96b14b",
   "metadata": {},
   "source": [
    "## **4. Model Customization**\n",
    "\n",
    "Ollama allows users to customize models using a `Modelfile`. This file specifies the base model and any modifications, such as system prompts or parameters.\n",
    "\n",
    "- **Create a `Modelfile`:**\n",
    "\n",
    "  Create a file named `Modelfile` with the following content:\n",
    "\n",
    "  ```\n",
    "  FROM llama3.2\n",
    "\n",
    "  SYSTEM \"You are an AI assistant specializing in environmental science. Answer all questions with a focus on sustainability.\"\n",
    "\n",
    "  PARAMETER temperature 0.7\n",
    "  ```\n",
    "\n",
    "- **Build the Custom Model:**\n",
    "\n",
    "  Use the `ollama create` command to build the model:\n",
    "\n",
    "  ```bash\n",
    "  ollama create my_custom_model -f ./Modelfile\n",
    "  ```\n",
    "\n",
    "- **Run the Custom Model:**\n",
    "\n",
    "  Start the customized model:\n",
    "\n",
    "  ```bash\n",
    "  ollama run my_custom_model\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbfea8-0142-49f5-b09a-2ec3dde0f486",
   "metadata": {},
   "source": [
    "## **5. Using Ollama with Files**\n",
    "\n",
    "- **Summarize Text from a File:**\n",
    "\n",
    "  To summarize the content of `input.txt`:\n",
    "\n",
    "  ```bash\n",
    "  ollama run llama3.2 \"Summarize the content of this file.\" < input.txt\n",
    "  ```\n",
    "\n",
    "- **Save Model Responses to a File:**\n",
    "\n",
    "  To save the model's response to `output.txt`:\n",
    "\n",
    "  ```bash\n",
    "  ollama run llama3.2 \"Explain the concept of quantum computing.\" > output.txt\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb3949-a71a-4a6c-bd41-f60665a55272",
   "metadata": {},
   "source": [
    "## **6. Common Use Cases**\n",
    "\n",
    "- **Text Generation:**\n",
    "\n",
    "  - *Content Creation:*\n",
    "\n",
    "    Generate an article on a specific topic:\n",
    "\n",
    "    ```bash\n",
    "    ollama run llama3.2 \"Write a short article on the benefits of renewable energy.\" > article.txt\n",
    "    ```\n",
    "\n",
    "  - *Question Answering:*\n",
    "\n",
    "    Answer specific queries:\n",
    "\n",
    "    ```bash\n",
    "    ollama run llama3.2 \"What are the latest advancements in artificial intelligence?\"\n",
    "    ```\n",
    "\n",
    "- **Data Analysis:**\n",
    "\n",
    "  - *Sentiment Analysis:*\n",
    "\n",
    "    Analyze the sentiment of a given text:\n",
    "\n",
    "    ```bash\n",
    "    ollama run llama3.2 \"Determine the sentiment of this review: 'The product exceeded my expectations.'\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea66d3-aa27-44f6-9870-e1556e16ba79",
   "metadata": {},
   "source": [
    "## **7. Python Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73633df6-5396-4b58-b190-289fe2a22a8d",
   "metadata": {},
   "source": [
    "### Ollama Python Library\n",
    "\n",
    "The Ollama Python library provides the easiest way to integrate Python 3.8+ projects with [Ollama](https://github.com/ollama/ollama)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2164a91-b651-4937-be63-db837e2ef2a4",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "- [Ollama](https://ollama.com/download) should be installed and running\n",
    "- Pull a model to use with the library: `ollama pull <model>` e.g. `ollama pull llama3.2`\n",
    "  - See [Ollama.com](https://ollama.com/search) for more information on the models available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621fda2-7304-4da2-8b09-1047d8a062ad",
   "metadata": {},
   "source": [
    "### Install\n",
    "\n",
    "```sh\n",
    "pip install ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e4941-305e-405b-aabc-db9969f7f439",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "```python\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)\n",
    "```\n",
    "\n",
    "See [_types.py](ollama/_types.py) for more information on the response types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de768f-e0ac-454e-a9e7-884b84fd7cc0",
   "metadata": {},
   "source": [
    "### Streaming responses\n",
    "\n",
    "Response streaming can be enabled by setting `stream=True`.\n",
    "\n",
    "```python\n",
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffbac4-7d00-4154-9efd-143488c852c6",
   "metadata": {},
   "source": [
    "### Custom client\n",
    "A custom client can be created by instantiating `Client` or `AsyncClient` from `ollama`.\n",
    "\n",
    "All extra keyword arguments are passed into the [`httpx.Client`](https://www.python-httpx.org/api/#client).\n",
    "\n",
    "```python\n",
    "from ollama import Client\n",
    "client = Client(\n",
    "  host='http://localhost:11434',\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "response = client.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c0a0c-0266-444e-befa-e1254554d4f2",
   "metadata": {},
   "source": [
    "### Async client\n",
    "\n",
    "The `AsyncClient` class is used to make asynchronous requests. It can be configured with the same fields as the `Client` class.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "  response = await AsyncClient().chat(model='llama3.2', messages=[message])\n",
    "\n",
    "asyncio.run(chat())\n",
    "```\n",
    "\n",
    "Setting `stream=True` modifies functions to return a Python asynchronous generator:\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "  async for part in await AsyncClient().chat(model='llama3.2', messages=[message], stream=True):\n",
    "    print(part['message']['content'], end='', flush=True)\n",
    "\n",
    "asyncio.run(chat())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae65c53-4924-49e1-a05f-5ebce1cb821b",
   "metadata": {},
   "source": [
    "### API\n",
    "\n",
    "The Ollama Python library's API is designed around the [Ollama REST API](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "\n",
    "#### Chat\n",
    "\n",
    "```python\n",
    "ollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n",
    "```\n",
    "\n",
    "#### Generate\n",
    "\n",
    "```python\n",
    "ollama.generate(model='llama3.2', prompt='Why is the sky blue?')\n",
    "```\n",
    "\n",
    "#### List\n",
    "\n",
    "```python\n",
    "ollama.list()\n",
    "```\n",
    "\n",
    "#### Show\n",
    "\n",
    "```python\n",
    "ollama.show('llama3.2')\n",
    "```\n",
    "\n",
    "#### Create\n",
    "\n",
    "```python\n",
    "ollama.create(model='example', from_='llama3.2', system=\"You are Mario from Super Mario Bros.\")\n",
    "```\n",
    "\n",
    "#### Copy\n",
    "\n",
    "```python\n",
    "ollama.copy('llama3.2', 'user/llama3.2')\n",
    "```\n",
    "\n",
    "#### Delete\n",
    "\n",
    "```python\n",
    "ollama.delete('llama3.2')\n",
    "```\n",
    "\n",
    "#### Pull\n",
    "\n",
    "```python\n",
    "ollama.pull('llama3.2')\n",
    "```\n",
    "\n",
    "#### Push\n",
    "\n",
    "```python\n",
    "ollama.push('user/llama3.2')\n",
    "```\n",
    "\n",
    "#### Embed\n",
    "\n",
    "```python\n",
    "ollama.embed(model='llama3.2', input='The sky is blue because of rayleigh scattering')\n",
    "```\n",
    "\n",
    "#### Embed (batch)\n",
    "\n",
    "```python\n",
    "ollama.embed(model='llama3.2', input=['The sky is blue because of rayleigh scattering', 'Grass is green because of chlorophyll'])\n",
    "```\n",
    "\n",
    "#### Ps\n",
    "\n",
    "```python\n",
    "ollama.ps()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7e7a5-e62c-4280-82fd-ad0ffbc46696",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "Errors are raised if requests return an error status or if an error is detected while streaming.\n",
    "\n",
    "```python\n",
    "model = 'does-not-yet-exist'\n",
    "\n",
    "try:\n",
    "  ollama.chat(model)\n",
    "except ollama.ResponseError as e:\n",
    "  print('Error:', e.error)\n",
    "  if e.status_code == 404:\n",
    "    ollama.pull(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d976d5e-3db2-4ca9-ad19-06818f032f01",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe153a0-959d-4845-b001-7944c3ef9649",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "response = ollama.generate(model='deepseek-r1:1.5b', prompt='How to create a django project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69273a3d-a5bd-413e-82bf-85ea86c47c98",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I need to figure out how to create a Django project. Hmm, where do I even start? I've heard about Django before, but I'm not entirely sure how it all works. Let me think.\n",
       "\n",
       "First, I guess I should check if Django is available on my system. I can try running the command `python --m dj` or maybe `python-dotnet install`. That sounds familiar; I remember seeing that before. So, if the command succeeds, that means I have a Django project set up and ready to go.\n",
       "\n",
       "Now, what's the structure of a typical Django project? I think there are directories named after different things. Oh right! There are app-related directories like `auth`, `models`, `backends`, and so on. Each of these has its own purpose. The main one is ` Auth`, which contains all the settings related to authentication.\n",
       "\n",
       "Let me list out what I see in that directory:\n",
       "- `auth/__init__.py`: That's where most of the basic settings for auth are defined, right? I remember something about middleware like `authentiative` and maybe `auth_token`. There are options for using TSL instances or custom tokens.\n",
       "- `auth/settings.py`: This is probably where all the environment variables go. Variables like `DEFAULT_AUTO_FIELD` might be set here because I think that determines how fields are stored, either as Python objects or as strings. I've heard about models being serialized to either strings or objects, so maybe this file handles that.\n",
       "- `auth/keys.py`: That must be the keys provider for auth settings. It's responsible for generating encryption keys necessary for authentication.\n",
       "- `backends` and other app-related directories: Those probably handle the database backends like SQLite, MySQL, etc.\n",
       "\n",
       "I'm a bit confused about what each of these files does exactly. Let me try to break it down. The `auth/__init__.py` is where I set up the default middleware for authentication. So if I want to use TSL instances instead of the built-in ones, that's probably in there. But how do I enable it? Oh right, you have to import the settings into other files.\n",
       "\n",
       "Then there's `settings.py`, which defines environment variables like TSL secp256r1 or something similar. That seems important because if your app is using a specific elliptic curve, you need to set that variable. But sometimes I don't want to use an external one; maybe just the default? How does that work?\n",
       "\n",
       "I've also heard about autowiring. Oh yeah, in Django, when you create models, you can define them autowireable by setting `autowireable=True`. That way, the model gets a `Model` instance automatically, which might make things simpler if I'm not doing all the setup manually.\n",
       "\n",
       "What about middleware for login and logout? There's something like `authMiddleware`, but how do I enable that in settings.py? Maybe it's an environment variable or another setting. Or perhaps it's handled through the app's autowiring configuration?\n",
       "\n",
       "I think I should make sure my project has exactly one database backend, which could be a SQLite or MySQL instance. So I need to set this up in the `db` directory. How do I do that? Do I create a separate file, maybe `db.py`, and configure it there?\n",
       "\n",
       "Another thing is using user accounts for authentication. I've heard about using user accounts like `auth_USER` with an `.Authentication` alias or something similar. That way, the settings are more accessible through commands.\n",
       "\n",
       "I'm also wondering how to handle security tokens. If my app uses TSL instances, I need to define a custom token file and make sure it's in the right place. Maybe that's part of the `auth/settings.py`.\n",
       "\n",
       "Let me think about dependencies. Django comes with some models and backends, but maybe for production use, I need a third-party database like SQLite or MySQL. How do I handle both development and production environments? Oh right, in production, you might need to configure your database separately with environment variables.\n",
       "\n",
       "I should probably set up my project structure first. Create an `app.py` file where all the models will be defined. Then create separate files for each app's settings if needed. But how about the dependencies between apps? I think it's okay as long as they have their own database backends.\n",
       "\n",
       "Wait, what about if I don't want to use a custom auth instance or TSL? Maybe I can just use the default built-in one by setting environment variables like `DEFAULT_AUTO_FIELD` in settings.py. That might be simpler for new projects.\n",
       "\n",
       "I'm also curious about how to manage my Django project during development versus production. For development, maybe it's easier to keep things simple and not include all the configuration files. But for a well-optimized app, I should probably have all necessary parts set up and use environment variables appropriately.\n",
       "\n",
       "Let me try to outline the steps I need to take:\n",
       "1. Install Django using `python-dotnet install` if needed.\n",
       "2. Create an app directory.\n",
       "3. Set up the app-specific directories with their respective files, handling auth settings, database backends, etc.\n",
       "4. Ensure that all necessary dependencies are installed in development or production separately.\n",
       "\n",
       "Wait, but when I run `python-dotnet install`, does it automatically create the Django project in my directory? Or do I need to explicitly set up some things before that?\n",
       "\n",
       "Also, how do I access an app if I'm running this on multiple machines? Maybe through a shared repository so others can load it without rebuilding everything. That might be beyond my current task.\n",
       "\n",
       "I should probably start by creating the app and setting up its settings. Then move on to installing dependencies in development. But for production, I need separate configuration files or maybe use environment variables directly.\n",
       "\n",
       "Hmm, another thing: what about custom backends that are not part of Django's default list? I think it's allowed as long as they fit within the core requirements.\n",
       "\n",
       "I'm also a bit fuzzy on how Django handles user accounts and authentication middleware. Maybe if I want to create an API endpoint with a different authentication method, I need to set specific variables or configurations there.\n",
       "\n",
       "Let me try to write down what I know so far:\n",
       "- Start by creating an app directory.\n",
       "- Navigate into `auth` and create the necessary files for auth settings.\n",
       "- Define environment variables like TSL parameters if needed.\n",
       "- Set up autowiring for models.\n",
       "- Configure database backends in separate files.\n",
       "- Manage dependencies appropriately during development vs production.\n",
       "\n",
       "I think that's a good start. Now, I should probably try to sketch out an example project structure and see what each file does. Maybe look at existing Django examples online to see how they handle the app setup.\n",
       "\n",
       "Oh, and there are some best practices in Django, like using relative imports when defining models and having clean configuration files. That might help keep my code organized as I build the project.\n",
       "\n",
       "I guess the next step after setting up the basic structure is to write the `auth/__init__.py` file with the necessary settings. Then move on from there. For now, that's where I should focus to get a working base for Django.\n",
       "</think>\n",
       "\n",
       "To create a Django project, follow these organized steps:\n",
       "\n",
       "1. **Install Dependencies**: Use `python-dotnet install` to set up Django in your current directory.\n",
       "\n",
       "2. **Create an App Directory**:\n",
       "   - Navigate into the `auth` subdirectory.\n",
       "   - Create and run `__init__.py`, which sets default settings for auth middleware, including enabling authentication-related settings.\n",
       "\n",
       "3. **Set Up Database Backends (if needed)**: In separate files like `db.py`, configure SQLite or MySQL with environment variables if required.\n",
       "\n",
       "4. **Define Environment Variables**:\n",
       "   - Ensure custom TSL parameters or `.Authentication` alias are set in production environments.\n",
       "   - Avoid duplicating common values in production to prevent conflicts.\n",
       "\n",
       "5. **Use Autowiring for Models**: Define models autowireable by setting `autowireable=True` so they fetch instances from Python automatically.\n",
       "\n",
       "6. **Access Settings and Database**:\n",
       "   - For development, access settings via the console or CLI.\n",
       "   - Access database connections using environment variables in production.\n",
       "\n",
       "7. **Install Environment Variables (for Production)**: Use separate configuration files if your app requires specific database setups for production.\n",
       "\n",
       "8. **Manage Dependencies**: In development, install necessary dependencies with `pip3 install ...`. For production, use a distinct repository or configure separately.\n",
       "\n",
       "By following these steps, you can create a Django project that starts with setting up core components and gradually adds more features as needed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf88d62-13fe-4d35-9f8d-22f31c22e1f3",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
