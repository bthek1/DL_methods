{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Tensor Flow graphs\n",
    "output-file: tensorflow.html\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "title: TensorFlow\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8eaea2-dc4d-403f-acba-328d0481b03a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c871d-1642-4595-a6f6-b0ed5168d3ab",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 431396), started 0:35:57 ago. (Use '!kill 431396' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6ba3e92a8050fa20\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6ba3e92a8050fa20\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4221d6-18f7-445c-bfac-658ecbee4fbf",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No known TensorBoard instances running.\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bff949-6250-4758-bc51-30e158935f32",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7c8e5bf8c39e0608\" width=\"100%\" height=\"1000\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7c8e5bf8c39e0608\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook.display(port=6006, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e6636-692c-4144-b45a-df2dd5104246",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29ddb7-2a27-46f0-bdba-50483ea7e12f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "############## TENSORBOARD ########################\n",
    "import sys\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter()\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa7cfc0-1a7e-4a53-8e56-51f85ad50902",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./Data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./Data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65845fc0-6d49-4d2a-b77a-52ac37584bed",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "############## TENSORBOARD ########################\n",
    "img_grid = torchvision.utils.make_grid(example_data)\n",
    "img_grid\n",
    "\n",
    "writer.add_image('mnist_images', img_grid)\n",
    "writer.flush()\n",
    "#sys.exit()\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa13380-d13d-4bc5-b914-702a71a0f020",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd5783-e6e8-46b3-9af6-1c5148aa2977",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c523cb-ecd8-4ac2-9053-41bf60d5dfa6",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load ResNet model without the final classification layer\n",
    "model = timm.create_model('resnet18', pretrained=True, num_classes=10)\n",
    "# Modify the first convolution layer to accept single-channel images\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b8ecf-9dc5-454b-b54d-955795967f5e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "############## TENSORBOARD ########################\n",
    "# writer.add_graph(model, example_data.reshape(-1, 28*28).to(device))\n",
    "writer.add_graph(model, example_data.to(device))\n",
    "writer.flush()\n",
    "#sys.exit()\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5bf54-e0b3-43d3-b54b-db366b00969c",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/938], Loss: 0.4240\n",
      "Epoch [1/1], Step [200/938], Loss: 0.2560\n",
      "Epoch [1/1], Step [300/938], Loss: 0.1293\n",
      "Epoch [1/1], Step [400/938], Loss: 0.1558\n",
      "Epoch [1/1], Step [500/938], Loss: 0.1048\n",
      "Epoch [1/1], Step [600/938], Loss: 0.0294\n",
      "Epoch [1/1], Step [700/938], Loss: 0.1048\n",
      "Epoch [1/1], Step [800/938], Loss: 0.1394\n",
      "Epoch [1/1], Step [900/938], Loss: 0.0257\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        # images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            ############## TENSORBOARD ########################\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n",
    "            running_accuracy = running_correct / 100 / predicted.size(0)\n",
    "            writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n",
    "            running_correct = 0\n",
    "            running_loss = 0.0\n",
    "            writer.flush()\n",
    "            ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d554de-0e57-4673-91e8-61f7f72b8abc",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.53 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "class_labels = []\n",
    "class_preds = []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        values, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
    "\n",
    "        class_preds.append(class_probs_batch)\n",
    "        class_labels.append(labels)\n",
    "\n",
    "    # 10000, 10, and 10000, 1\n",
    "    # stack concatenates tensors along a new dimension\n",
    "    # cat concatenates tensors in the given dimension\n",
    "    class_preds = torch.cat([torch.stack(batch) for batch in class_preds])\n",
    "    class_labels = torch.cat(class_labels)\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
    "\n",
    "    ############## TENSORBOARD ########################\n",
    "    classes = range(10)\n",
    "    for i in classes:\n",
    "        labels_i = class_labels == i\n",
    "        preds_i = class_preds[:, i]\n",
    "        writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0)\n",
    "        writer.flush()\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21d674-dbf6-4d63-941a-62f2a8d6d692",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5117d9-b8cd-4f0f-8755-b2f6fa95ce86",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
