{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: The task of identifying and localizing objects within an image by drawing\n",
    "  bounding boxes around each detected object and classifying them.\n",
    "output-file: object_detection.html\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "title: Object Detection\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b62a15e-45fe-43a3-b234-debbb4003fbf",
   "metadata": {},
   "source": [
    "## Popular Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e104ba-98be-4ba9-a06b-a9c6cc8aca99",
   "metadata": {},
   "source": [
    "1. COCO (Common Objects in Context)\n",
    "- **Description**: Large-scale object detection, segmentation, and captioning dataset with over 200,000 labeled images.\n",
    "- **URL**: [COCO](http://cocodataset.org/)\n",
    "\n",
    "2. PASCAL VOC\n",
    "- **Description**: Dataset for object detection with 20 classes, providing images, annotations, and segmentation masks.\n",
    "- **URL**: [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)\n",
    "\n",
    "3. ImageNet\n",
    "- **Description**: Over 14 million images with 1,000 object categories, also includes a subset for object detection.\n",
    "- **URL**: [ImageNet](http://www.image-net.org/)\n",
    "\n",
    "4. Open Images Dataset\n",
    "- **Description**: Contains ~9 million images annotated with image-level labels, object bounding boxes, and segmentation masks.\n",
    "- **URL**: [Open Images](https://storage.googleapis.com/openimages/web/index.html)\n",
    "\n",
    "5. KITTI\n",
    "- **Description**: Dataset for autonomous driving with images, 3D point clouds, and annotations for object detection and tracking.\n",
    "- **URL**: [KITTI](http://www.cvlibs.net/datasets/kitti/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20140b2-4e5b-44c4-b24a-7dae26680770",
   "metadata": {},
   "source": [
    "## Popular Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64175a1-4604-41ce-a229-d64a46f44c39",
   "metadata": {},
   "source": [
    "1. R-CNN (Region-based Convolutional Neural Networks)\n",
    "- **Variants**: R-CNN, Fast R-CNN, Faster R-CNN\n",
    "- **Description**: Uses region proposal networks to identify regions of interest and then classify objects within those regions.\n",
    "- **URL**: [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n",
    "\n",
    "2. YOLO (You Only Look Once)\n",
    "- **Variants**: YOLOv1, YOLOv2, YOLOv3, YOLOv4, YOLOv5\n",
    "- **Description**: Real-time object detection system that predicts bounding boxes and class probabilities directly from full images.\n",
    "- **URL**: [YOLO](https://arxiv.org/abs/1506.02640)\n",
    "\n",
    "3. SSD (Single Shot MultiBox Detector)\n",
    "- **Description**: Detects objects in images using a single deep neural network.\n",
    "- **URL**: [SSD](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "4. RetinaNet\n",
    "- **Description**: Combines a backbone network for feature extraction with a novel Focal Loss to handle class imbalance.\n",
    "- **URL**: [RetinaNet](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "5. EfficientDet\n",
    "- **Description**: Scalable and efficient object detector, part of the EfficientNet family.\n",
    "- **URL**: [EfficientDet](https://arxiv.org/abs/1911.09070)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088692c2-3679-4eee-9e05-e04a942f3f2a",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b833d7-a6db-44b4-9720-ce5762db7798",
   "metadata": {},
   "source": [
    "1. Learning Rate\n",
    "- **Description**: Controls the step size at each iteration while moving towards a minimum of the loss function.\n",
    "\n",
    "2. Batch Size\n",
    "- **Description**: The number of training examples used in one iteration.\n",
    "\n",
    "3. Number of Epochs\n",
    "- **Description**: The number of complete passes through the training dataset.\n",
    "\n",
    "4. Anchor Boxes\n",
    "- **Description**: Predefined bounding boxes of different sizes and aspect ratios used for detection.\n",
    "\n",
    "5. IoU Threshold\n",
    "- **Description**: Intersection over Union (IoU) threshold for determining true positive detections.\n",
    "\n",
    "6. Non-Maximum Suppression (NMS) Threshold\n",
    "- **Description**: Threshold for filtering out overlapping bounding boxes.\n",
    "\n",
    "7. Backbone Network\n",
    "- **Examples**: ResNet, VGG, MobileNet\n",
    "\n",
    "8. Optimizer\n",
    "- **Examples**: SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e25e0-926d-4dad-a0e2-634015380b7e",
   "metadata": {},
   "source": [
    "## Popular Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05d8af-d9c8-406c-af21-d099e23fa1a4",
   "metadata": {},
   "source": [
    "1. Cross-Entropy Loss\n",
    "- **Description**: Measures the classification error in object detection tasks.\n",
    "\n",
    "2. Smooth L1 Loss\n",
    "- **Description**: Used for bounding box regression, combining L1 and L2 loss.\n",
    "\n",
    "3. Focal Loss\n",
    "- **Description**: Addresses class imbalance by focusing on hard examples.\n",
    "- **URL**: [Focal Loss](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "4. IoU Loss\n",
    "- **Description**: Directly optimizes the Intersection over Union metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347500d-18fc-4172-8f8f-83defccea6f5",
   "metadata": {},
   "source": [
    "## Popular Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b2855-ed14-4eea-acf5-228cd4b859ae",
   "metadata": {},
   "source": [
    "1. Mean Average Precision (mAP)\n",
    "- **Description**: The average precision across all classes.\n",
    "\n",
    "2. Intersection over Union (IoU)\n",
    "- **Description**: Measures the overlap between the predicted bounding box and the ground truth.\n",
    "\n",
    "3. Precision-Recall Curve\n",
    "- **Description**: Plots precision against recall for different threshold values.\n",
    "\n",
    "4. F1 Score\n",
    "- **Description**: The harmonic mean of precision and recall.\n",
    "\n",
    "5. Average Precision (AP)\n",
    "- **Description**: The area under the precision-recall curve for a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933912bf-72d5-47ad-bdd7-aec600611c89",
   "metadata": {},
   "source": [
    "## Other Important Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df30c3-4050-4536-9e46-4549b848f436",
   "metadata": {},
   "source": [
    "1. Data Augmentation\n",
    "- **Description**: Techniques to increase the diversity of the training dataset without collecting new data.\n",
    "- **Examples**: Scaling, Translation, Rotation, Flipping, Adding Noise\n",
    "\n",
    "2. Transfer Learning\n",
    "- **Description**: Using a pre-trained model on a new, related task.\n",
    "- **Example**: Fine-tuning a model pre-trained on COCO for a custom object detection task.\n",
    "\n",
    "3. Fine-Tuning\n",
    "- **Description**: Adjusting a pre-trained model's parameters on a new dataset.\n",
    "\n",
    "4. Hyperparameter Tuning\n",
    "- **Techniques**: Grid Search, Random Search, Bayesian Optimization\n",
    "\n",
    "5. Model Interpretability\n",
    "- **Techniques**: Visualization of feature maps, Activation maximization\n",
    "\n",
    "6. Post-Processing Techniques\n",
    "- **Examples**: Non-Maximum Suppression (NMS), Soft-NMS\n",
    "\n",
    "7. Frameworks and Libraries\n",
    "- **Examples**: TensorFlow Object Detection API, Detectron2, MMDetection\n",
    "\n",
    "8. Edge and Real-Time Object Detection\n",
    "- **Description**: Deploying object detection models on edge devices for real-time applications.\n",
    "- **Examples**: TensorFlow Lite, NVIDIA Jetson, OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d58e2-445f-46f8-9149-7ad31349542d",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafa81b-9452-487a-bbef-9b1ea9dc3d8a",
   "metadata": {},
   "source": [
    "- [Deep Learning Book by Ian Goodfellow](http://www.deeplearningbook.org/)\n",
    "- [Stanford CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [TensorFlow Documentation](https://www.tensorflow.org/learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0beea-b57c-45cb-89bb-6f16e6ee4019",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
