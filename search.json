[
  {
    "objectID": "save_and_load.html",
    "href": "save_and_load.html",
    "title": "Save and Load",
    "section": "",
    "text": "import torch\nimport torch.nn as nn",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#save-arg-dict-with-python-pickle",
    "href": "save_and_load.html#save-arg-dict-with-python-pickle",
    "title": "Save and Load",
    "section": "Save arg dict with python pickle",
    "text": "Save arg dict with python pickle\ntorch.save(arg, PATH)\ntorch.load(PATH)\nmodel.load_state_dict(arg)",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#save-model-with-python-pickle",
    "href": "save_and_load.html#save-model-with-python-pickle",
    "title": "Save and Load",
    "section": "Save model with python pickle",
    "text": "Save model with python pickle\ntorch.save(model, PATH)\n\nmodel = torch.load(PATH)\n\nmodel.eval()",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#recommented-method",
    "href": "save_and_load.html#recommented-method",
    "title": "Save and Load",
    "section": "Recommented Method",
    "text": "Recommented Method\ntorch.save(model.state_dict(), PATH)\n\nmodel = Model(*args, **kwargs)\n\nmodel.load_state_dict(torch.load(PATH))\n\nmodel.eval()",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#test-model",
    "href": "save_and_load.html#test-model",
    "title": "Save and Load",
    "section": "Test model",
    "text": "Test model\n\nclass Model(nn.Module):\n    def __init__(self, n_input_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(n_input_features, 1)\n\n    def forward(self, x):\n        y_pred = torch.sigmoid(self.linear(x))\n        return y_pred\n\n\ndevice = torch.device(\"cuda\")\nmodel = Model(n_input_features = 6)\nmodel\n\nModel(\n  (linear): Linear(in_features=6, out_features=1, bias=True)\n)",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#method-1",
    "href": "save_and_load.html#method-1",
    "title": "Save and Load",
    "section": "Method 1",
    "text": "Method 1\n\nFILE = \"model.pth1\"\ntorch.save(model, FILE)\n\n\nmodel = torch.load(FILE)\nmodel.eval()\n\nModel(\n  (linear): Linear(in_features=6, out_features=1, bias=True)\n)\n\n\n\nfor param in model.parameters():\n    print(param)\n\nParameter containing:\ntensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]],\n       requires_grad=True)\nParameter containing:\ntensor([-0.3233], requires_grad=True)",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#method-2",
    "href": "save_and_load.html#method-2",
    "title": "Save and Load",
    "section": "Method 2",
    "text": "Method 2\n\nFILE = \"model.pth2\"\ntorch.save(model.state_dict(), FILE)\n\n\nmodel = Model(n_input_features = 6)\n\n\nmodel.load_state_dict(torch.load(FILE))\n\nmodel.eval()\n\nModel(\n  (linear): Linear(in_features=6, out_features=1, bias=True)\n)\n\n\n\nfor param in model.parameters():\n    print(param)\n\nParameter containing:\ntensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]],\n       requires_grad=True)\nParameter containing:\ntensor([-0.3233], requires_grad=True)\n\n\n\nmodel.state_dict()\n\nOrderedDict([('linear.weight',\n              tensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]])),\n             ('linear.bias', tensor([-0.3233]))])\n\n\n\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\noptimizer.state_dict()\n\n{'state': {},\n 'param_groups': [{'lr': 0.01,\n   'momentum': 0,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'maximize': False,\n   'foreach': None,\n   'differentiable': False,\n   'params': [0, 1]}]}\n\n\n\ncheckpoint = {\n    \"epoch\": 90,\n    \"model_state\": model.state_dict(),\n    \"optim_state\": optimizer.state_dict()\n}\ncheckpoint\n\n{'epoch': 90,\n 'model_state': OrderedDict([('linear.weight',\n               tensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]])),\n              ('linear.bias', tensor([-0.3233]))]),\n 'optim_state': {'state': {},\n  'param_groups': [{'lr': 0.01,\n    'momentum': 0,\n    'dampening': 0,\n    'weight_decay': 0,\n    'nesterov': False,\n    'maximize': False,\n    'foreach': None,\n    'differentiable': False,\n    'params': [0, 1]}]}}\n\n\n\ntorch.save(checkpoint, \"checkpoint.pth\")\n\n\nloaded_checkpoint = torch.load(\"checkpoint.pth\")\n\n\nloaded_checkpoint['epoch']\n\n90\n\n\n\nmodel.load_state_dict(loaded_checkpoint['model_state'])\nmodel.state_dict()\n\nOrderedDict([('linear.weight',\n              tensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]])),\n             ('linear.bias', tensor([-0.3233]))])\n\n\n\noptimizer.load_state_dict(loaded_checkpoint['optim_state'])\noptimizer.state_dict()\n\n{'state': {},\n 'param_groups': [{'lr': 0.01,\n   'momentum': 0,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'maximize': False,\n   'foreach': None,\n   'differentiable': False,\n   'params': [0, 1]}]}",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "LLM/ollama.html",
    "href": "LLM/ollama.html",
    "title": "Ollama",
    "section": "",
    "text": "For Linux:\nOpen your terminal and execute:\ncurl -fsSL https://ollama.com/install.sh | sh\nThis command downloads and installs Ollama on your system.",
    "crumbs": [
      "Blog",
      "LLM",
      "Ollama"
    ]
  },
  {
    "objectID": "LLM/ollama.html#installation",
    "href": "LLM/ollama.html#installation",
    "title": "Ollama",
    "section": "",
    "text": "For Linux:\nOpen your terminal and execute:\ncurl -fsSL https://ollama.com/install.sh | sh\nThis command downloads and installs Ollama on your system.",
    "crumbs": [
      "Blog",
      "LLM",
      "Ollama"
    ]
  },
  {
    "objectID": "LLM/ollama.html#system-requirements",
    "href": "LLM/ollama.html#system-requirements",
    "title": "Ollama",
    "section": "2. System Requirements",
    "text": "2. System Requirements\n\nOperating System: macOS or Linux\nMemory (RAM): Minimum 8GB; 16GB or more recommended\nStorage: At least 10GB of free space\nProcessor: Modern CPU from the last 5 years",
    "crumbs": [
      "Blog",
      "LLM",
      "Ollama"
    ]
  },
  {
    "objectID": "LLM/ollama.html#basic-cli-commands",
    "href": "LLM/ollama.html#basic-cli-commands",
    "title": "Ollama",
    "section": "3. Basic CLI Commands",
    "text": "3. Basic CLI Commands\n\nStart the Ollama Server:\nTo run Ollama without the desktop application:\nollama serve\nDownload a Model:\nTo download a specific model:\nollama pull &lt;model-name&gt;\nReplace &lt;model-name&gt; with the desired modelâ€™s name, e.g., llama3.2.\nList Downloaded Models:\nTo view all models available on your system:\nollama list\nRun a Model:\nTo start a model and enter an interactive session:\nollama run &lt;model-name&gt;\nFor example:\nollama run llama3.2\nStop a Running Model:\nTo stop a specific model:\nollama stop &lt;model-name&gt;\nRemove a Model:\nTo delete a model from your system:\nollama rm &lt;model-name&gt;\nDisplay Model Information:\nTo view details about a specific model:\nollama show &lt;model-name&gt;\nList Running Models:\nTo see which models are currently active:\nollama ps\nAccess Help:\nFor a list of available commands and their descriptions:\nollama help",
    "crumbs": [
      "Blog",
      "LLM",
      "Ollama"
    ]
  },
  {
    "objectID": "LLM/ollama.html#model-customization",
    "href": "LLM/ollama.html#model-customization",
    "title": "Ollama",
    "section": "4. Model Customization",
    "text": "4. Model Customization\nOllama allows users to customize models using a Modelfile. This file specifies the base model and any modifications, such as system prompts or parameters.\n\nCreate a Modelfile:\nCreate a file named Modelfile with the following content:\nFROM llama3.2\n\nSYSTEM \"You are an AI assistant specializing in environmental science. Answer all questions with a focus on sustainability.\"\n\nPARAMETER temperature 0.7\nBuild the Custom Model:\nUse the ollama create command to build the model:\nollama create my_custom_model -f ./Modelfile\nRun the Custom Model:\nStart the customized model:\nollama run my_custom_model",
    "crumbs": [
      "Blog",
      "LLM",
      "Ollama"
    ]
  },
  {
    "objectID": "LLM/ollama.html#using-ollama-with-files",
    "href": "LLM/ollama.html#using-ollama-with-files",
    "title": "Ollama",
    "section": "5. Using Ollama with Files",
    "text": "5. Using Ollama with Files\n\nSummarize Text from a File:\nTo summarize the content of input.txt:\nollama run llama3.2 \"Summarize the content of this file.\" &lt; input.txt\nSave Model Responses to a File:\nTo save the modelâ€™s response to output.txt:\nollama run llama3.2 \"Explain the concept of quantum computing.\" &gt; output.txt",
    "crumbs": [
      "Blog",
      "LLM",
      "Ollama"
    ]
  },
  {
    "objectID": "LLM/ollama.html#common-use-cases",
    "href": "LLM/ollama.html#common-use-cases",
    "title": "Ollama",
    "section": "6. Common Use Cases",
    "text": "6. Common Use Cases\n\nText Generation:\n\nContent Creation:\nGenerate an article on a specific topic:\nollama run llama3.2 \"Write a short article on the benefits of renewable energy.\" &gt; article.txt\nQuestion Answering:\nAnswer specific queries:\nollama run llama3.2 \"What are the latest advancements in artificial intelligence?\"\n\nData Analysis:\n\nSentiment Analysis:\nAnalyze the sentiment of a given text:\nollama run llama3.2 \"Determine the sentiment of this review: 'The product exceeded my expectations.'\"",
    "crumbs": [
      "Blog",
      "LLM",
      "Ollama"
    ]
  },
  {
    "objectID": "LLM/ollama.html#python-integration",
    "href": "LLM/ollama.html#python-integration",
    "title": "Ollama",
    "section": "7. Python Integration",
    "text": "7. Python Integration\n\nOllama Python Library\nThe Ollama Python library provides the easiest way to integrate Python 3.8+ projects with Ollama.\n\n\nPrerequisites\n\nOllama should be installed and running\nPull a model to use with the library: ollama pull &lt;model&gt; e.g.Â ollama pull llama3.2\n\nSee Ollama.com for more information on the models available.\n\n\n\n\nInstall\npip install ollama\n\n\nUsage\nfrom ollama import chat\nfrom ollama import ChatResponse\n\nresponse: ChatResponse = chat(model='llama3.2', messages=[\n  {\n    'role': 'user',\n    'content': 'Why is the sky blue?',\n  },\n])\nprint(response['message']['content'])\n# or access fields directly from the response object\nprint(response.message.content)\nSee _types.py for more information on the response types.\n\n\nStreaming responses\nResponse streaming can be enabled by setting stream=True.\nfrom ollama import chat\n\nstream = chat(\n    model='llama3.2',\n    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n    stream=True,\n)\n\nfor chunk in stream:\n  print(chunk['message']['content'], end='', flush=True)\n\n\nCustom client\nA custom client can be created by instantiating Client or AsyncClient from ollama.\nAll extra keyword arguments are passed into the httpx.Client.\nfrom ollama import Client\nclient = Client(\n  host='http://localhost:11434',\n  headers={'x-some-header': 'some-value'}\n)\nresponse = client.chat(model='llama3.2', messages=[\n  {\n    'role': 'user',\n    'content': 'Why is the sky blue?',\n  },\n])\n\n\nAsync client\nThe AsyncClient class is used to make asynchronous requests. It can be configured with the same fields as the Client class.\nimport asyncio\nfrom ollama import AsyncClient\n\nasync def chat():\n  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n  response = await AsyncClient().chat(model='llama3.2', messages=[message])\n\nasyncio.run(chat())\nSetting stream=True modifies functions to return a Python asynchronous generator:\nimport asyncio\nfrom ollama import AsyncClient\n\nasync def chat():\n  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n  async for part in await AsyncClient().chat(model='llama3.2', messages=[message], stream=True):\n    print(part['message']['content'], end='', flush=True)\n\nasyncio.run(chat())\n\n\nAPI\nThe Ollama Python libraryâ€™s API is designed around the Ollama REST API\n\nChat\nollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n\n\nGenerate\nollama.generate(model='llama3.2', prompt='Why is the sky blue?')\n\n\nList\nollama.list()\n\n\nShow\nollama.show('llama3.2')\n\n\nCreate\nollama.create(model='example', from_='llama3.2', system=\"You are Mario from Super Mario Bros.\")\n\n\nCopy\nollama.copy('llama3.2', 'user/llama3.2')\n\n\nDelete\nollama.delete('llama3.2')\n\n\nPull\nollama.pull('llama3.2')\n\n\nPush\nollama.push('user/llama3.2')\n\n\nEmbed\nollama.embed(model='llama3.2', input='The sky is blue because of rayleigh scattering')\n\n\nEmbed (batch)\nollama.embed(model='llama3.2', input=['The sky is blue because of rayleigh scattering', 'Grass is green because of chlorophyll'])\n\n\nPs\nollama.ps()\n\n\n\nErrors\nErrors are raised if requests return an error status or if an error is detected while streaming.\nmodel = 'does-not-yet-exist'\n\ntry:\n  ollama.chat(model)\nexcept ollama.ResponseError as e:\n  print('Error:', e.error)\n  if e.status_code == 404:\n    ollama.pull(model)\n\nimport ollama\nfrom IPython.display import Markdown, display\n\nresponse = ollama.generate(model='deepseek-r1:7b', prompt='How to create a django project')\n\ndisplay(Markdown(response['response']))\n\n Okay, so I need to figure out how to create a Django project. Iâ€™ve heard about Django before; itâ€™s a Python framework for building web apps. But Iâ€™m not entirely sure where to start. Let me think through this step by step.\nFirst, I know that Django requires some setup files and environment configuration. So maybe the first thing is to install Python on my machine if I donâ€™t already have it. I remember that Python 3 is recommended for Django because it has better support from Django itself. So Iâ€™ll check if I have Python installed or not.\nNext, after having Python installed, I need to make sure Iâ€™m using the correct version. Iâ€™ve heard about virtual environments being important for managing project dependencies. So maybe I should create a virtual environment with an activated one. That way, all my Django projects can use the same Python version without conflicts.\nOnce thatâ€™s set up, I need to install Django itself. From what I remember, you can install it using pip. So Iâ€™ll run â€˜pip install djangoâ€™ in my terminal or command prompt. After that, I should activate my virtual environment again because thatâ€™s where the installed packages will be located.\nNow comes creating a new project. I think Django uses an app directory structure under settings.py. So to create a new project, I probably go into my project directory and then run some command related to Django projects. Maybe something like â€˜django-admin startprojectâ€™ or similar? Oh wait, I think itâ€™s just â€˜python manage.py startprojectâ€™ because sometimes the admin interface isnâ€™t available until after migrations are applied.\nAfter starting the project, I should set up a settings file. The settings file is crucial because it configures various aspects of Django like middleware, URLs, templates, etc. So I need to create a new settings.py file in my project directory with some default configurations or maybe import an existing one if thereâ€™s a template.\nIâ€™m not entirely sure about all the parts that should be included in settings.py, but from what I remember, it includes things like ROOT_URLCONF, MiddlewareStack, Templates, TemplateSyntax, LoginView, etc. Maybe I can look up a basic Django settings file as a starting point to get an idea of how to structure mine.\nAlso, after creating the project and setting up the settings file, I probably need to create some initial apps or models. For example, maybe I should add users, orders, products, or other relevant apps depending on what kind of application Iâ€™m building. Each app would contain models, which define the database tables for my web application.\nWait, how do I create an app? I think it involves creating a new app directory in the apps folder and adding a main.py file that imports the models. Then, I can run migrations to adjust the database schema based on the models Iâ€™ve created. Oh right, after creating a model, I have to make sure to apply migrations so Django knows how to structure the database.\nI also remember something about URLs. When you start a project with â€˜startprojectâ€™, the URL configuration is already set up, but sometimes itâ€™s not what you want. So maybe I need to go into urls.py and adjust the paths or include some custom apps there.\nAnother thing: permissions. Django handles user permissions, so I think after creating users app, I can assign superuser permissions to a specific user using â€˜python manage.py migrateâ€™ followed by adding the user as admin with â€˜superuserâ€™.\nIâ€™m a bit confused about whatâ€™s included in the default project. Maybe it includes a welcome app and other base apps that set up some initial functionality like logging in users or handling requests.\nOh, I also need to think about how to run migrations once the models are created. The Django console is helpful for managing projects, apps, models, and running migrations manually or automatically with â€˜makemigrationsâ€™ commands.\nWait, sometimes when you start a project, the URLs might not point correctly because of an incorrect path in settings.py. So maybe I need to adjust that after creating it. Or perhaps I should use the â€˜runserverâ€™ command at some point to test if everything is working as expected.\nIâ€™m also wondering about templates and views. Once the models are set up, how do I create views for handling HTTP requests? Probably by creating view functions in urls.py or using DRF (Django REST Framework) if itâ€™s a REST API. But maybe thatâ€™s beyond my current setup; perhaps Iâ€™ll focus on standard views first.\nTesting is another aspect. I know Django has built-in testing tools, so after setting up everything correctly, I can run tests to make sure all parts of the application are working as intended.\nWait, but when creating a new project with â€˜startprojectâ€™, it actually creates an app called â€˜baseâ€™ by default. So maybe thatâ€™s where some initial configurations are placed. Do I need to modify anything in baseapp.py or other files?\nI think Iâ€™m getting closer. Maybe the steps are: install Python 3, create a virtual environment and activate it, install Django, create a new project with â€˜startprojectâ€™, set up settings, create apps/models, apply migrations, run tests, etc.\nBut what if something goes wrong? Like, maybe when I try to run migrations manually after creating models, it doesnâ€™t work. Or perhaps the URLs arenâ€™t set correctly and cause 404 errors. So debugging is important too.\nI also heard about dev tools in Django for local development, like debug: true in settings.py or using the â€˜devâ€™ server with host=â€˜localhostâ€™ and port=8000. Maybe I should enable that to test my application without leaving the server running all the time.\nIn summary, my plan is:\n\nInstall Python 3 if not already present.\nCreate a virtual environment and activate it.\nInstall Django using pip.\nStart a new Django project with â€˜startprojectâ€™.\nConfigure settings.py with necessary configurations.\nCreate apps (e.g., users, products) with models.\nRun migrations to apply database changes.\nSet up URLs and tests as needed.\nDebug if issues arise.\n\nI think Iâ€™ve covered the main steps. Now, Iâ€™ll try to write this out in a clear, step-by-step manner without being too vague or technical beyond my current understanding. \nCreating a Django project involves several structured steps, each ensuring that your web application is set up correctly and efficiently. Hereâ€™s a concise guide based on the thought process:\n\nStep-by-Step Guide to Creating a Django Project\n\nInstall Python 3\n\nEnsure Python 3 is installed as Django is compatible with it.\n\nSet Up Virtual Environment\n\nCreate a virtual environment to manage project dependencies:\npython -m venv myproject\nsource myproject/bin/activate  # On Windows, use myproject\\myproject.bat\nActivate the virtual environment for all subsequent commands.\n\nInstall Django\n\nInstall Django using pip within the activated environment:\npip install django\n\nStart a New Django Project\n\nCreate a new project by running:\npython manage.py startproject myproject\nThis generates a myproject directory with initial files.\n\nConfigure Django Settings\n\nCreate or modify settings.py in the myproject/settings/ directory to set your environment variables and application URLs.\nfrom pathlib import Path\n\nBASE_DIR = str(Path(__file__).resolve().parent.parent)\nSECURITY middlewares are configured here.\n\nROOT_URLCONF = 'myproject.urls'\nEnsure necessary Django features like logging, middleware, and templates are enabled.\n\nCreate Applications/Models\n\nAdd apps (e.g., users, products) by creating directories and initial Python files.\nCreate models for your database tables within each app directory:\nfrom django.db import models\n\nclass User(models.Model):\n    username = models.CharField(max_length=100)\n    email = models.EmailField()\n    ...\n\nRun Migrations\n\nApply database migrations after creating models:\npython manage.py migrate\nThis adjusts the database schema based on your models.\n\nSet Up URLs Configuration\n\nConfigure URLs by editing urls.py or using Djangoâ€™s admin interface.\nEnsure paths are correctly set up, possibly adjusting after settings configuration adjustments.\n\nEnable Developer Tools (Optional)\n\nAdd dev middleware and enable debug mode in your settings for easier testing:\nMIDDLEWARE = (\n    ...\n    'django.middleware.clickjacking.D ClickJackingMiddleware',\n    ...\n)\nor set DEBUG: True in settings.py.\n\nRun the Django Server\n\nStart the development server to test your application within a local web browser:\npython manage.py runserver\n\nTest and Debug\n\nUse Djangoâ€™s built-in tests or external tools like Postman, Firefox, or Chrome to access your application.\nAddress any issues arising from misconfigured URLs, missing models, or migration errors.\n\n\nBy following these steps, youâ€™ll have a functional Django project ready for development. Each step ensures that your application is properly configured and set up for a smooth user experience.",
    "crumbs": [
      "Blog",
      "LLM",
      "Ollama"
    ]
  },
  {
    "objectID": "LLM/llmclass.html",
    "href": "LLM/llmclass.html",
    "title": "LLM Class",
    "section": "",
    "text": "from abc import ABC, abstractmethod\nfrom datetime import datetime\nimport uuid\nimport json\nimport os\nfrom dotenv import load_dotenv\nimport ollama\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nload_dotenv()\n\n# ---------------- Adapter Pattern (LangChain LLM Wrapper) ----------------\nclass LLMAdapter(ABC):\n    @abstractmethod\n    def generate_response(self, messages: list) -&gt; str:\n        pass\n\nclass LangChainOpenAIAdapter(LLMAdapter):\n    def __init__(self, model=\"gpt-4o\"):\n        self.llm = ChatOpenAI(\n            model=model,\n            temperature=0.7,\n            api_key=os.getenv(\"openai_api_key\")\n        )\n\n    def generate_response(self, messages: list) -&gt; str:\n        return self.llm.invoke(messages).content.strip()\n\nclass OllamaAdapter(LLMAdapter):\n    def __init__(self, model=\"deepseek-r1:7b\"):\n        self.model = model\n\n    def generate_response(self, messages: list) -&gt; str:\n        def get_role(msg):\n            if isinstance(msg, HumanMessage):\n                return \"User\"\n            elif isinstance(msg, SystemMessage):\n                return \"System\"\n            elif isinstance(msg, AIMessage):\n                return \"Assistant\"\n            return \"Unknown\"\n\n        prompt = \"\\n\".join([f\"{get_role(msg)}: {msg.content}\" for msg in messages])\n        response = ollama.generate(model=self.model, prompt=prompt)\n        return response[\"response\"].strip()\n        \n# ---------------- Prompt Strategy ----------------\nclass PromptStrategy(ABC):\n    @abstractmethod\n    def build_messages(self, user_input: str) -&gt; list:\n        pass\n\nclass QAStrategy(PromptStrategy):\n    def build_messages(self, user_input: str) -&gt; list:\n        return [\n            SystemMessage(content=\"You are a helpful assistant.\"),\n            HumanMessage(content=f\"Q: {user_input}\\nA:\")\n        ]\n\nclass SummarizeStrategy(PromptStrategy):\n    def build_messages(self, user_input: str) -&gt; list:\n        return [\n            SystemMessage(content=\"You summarize input text.\"),\n            HumanMessage(content=f\"Summarize:\\n{user_input}\")\n        ]\n\n# ---------------- Chat History ----------------\nclass ChatHistory:\n    def __init__(self):\n        self.data = {}\n\n    def log(self, session_id, role, message):\n        self.data.setdefault(session_id, []).append({\n            \"timestamp\": datetime.now().isoformat(),\n            \"role\": role,\n            \"message\": message\n        })\n\n    def show(self, session_id):\n        for entry in self.data.get(session_id, []):\n            print(f\"{entry['timestamp']} [{entry['role']}]: {entry['message']}\")\n\n    def export(self, session_id):\n        return json.dumps(self.data.get(session_id, []), indent=2)\n\n# ---------------- LLM Client ----------------\nclass LLMClient:\n    def __init__(self, adapter: LLMAdapter, strategy: PromptStrategy):\n        self.adapter = adapter\n        self.strategy = strategy\n        self.history = ChatHistory()\n        self.session_id = str(uuid.uuid4())\n\n    def chat(self, user_input):\n        messages = self.strategy.build_messages(user_input)\n        self.history.log(self.session_id, \"user\", user_input)\n        response = self.adapter.generate_response(messages)\n        self.history.log(self.session_id, \"assistant\", response)\n        return response\n\n    def view_history(self):\n        self.history.show(self.session_id)\n\n    def export_history(self):\n        return self.history.export(self.session_id)",
    "crumbs": [
      "Blog",
      "LLM",
      "LLM Class"
    ]
  },
  {
    "objectID": "LLM/llmclass.html#with-design-pattern",
    "href": "LLM/llmclass.html#with-design-pattern",
    "title": "LLM Class",
    "section": "",
    "text": "from abc import ABC, abstractmethod\nfrom datetime import datetime\nimport uuid\nimport json\nimport os\nfrom dotenv import load_dotenv\nimport ollama\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nload_dotenv()\n\n# ---------------- Adapter Pattern (LangChain LLM Wrapper) ----------------\nclass LLMAdapter(ABC):\n    @abstractmethod\n    def generate_response(self, messages: list) -&gt; str:\n        pass\n\nclass LangChainOpenAIAdapter(LLMAdapter):\n    def __init__(self, model=\"gpt-4o\"):\n        self.llm = ChatOpenAI(\n            model=model,\n            temperature=0.7,\n            api_key=os.getenv(\"openai_api_key\")\n        )\n\n    def generate_response(self, messages: list) -&gt; str:\n        return self.llm.invoke(messages).content.strip()\n\nclass OllamaAdapter(LLMAdapter):\n    def __init__(self, model=\"deepseek-r1:7b\"):\n        self.model = model\n\n    def generate_response(self, messages: list) -&gt; str:\n        def get_role(msg):\n            if isinstance(msg, HumanMessage):\n                return \"User\"\n            elif isinstance(msg, SystemMessage):\n                return \"System\"\n            elif isinstance(msg, AIMessage):\n                return \"Assistant\"\n            return \"Unknown\"\n\n        prompt = \"\\n\".join([f\"{get_role(msg)}: {msg.content}\" for msg in messages])\n        response = ollama.generate(model=self.model, prompt=prompt)\n        return response[\"response\"].strip()\n        \n# ---------------- Prompt Strategy ----------------\nclass PromptStrategy(ABC):\n    @abstractmethod\n    def build_messages(self, user_input: str) -&gt; list:\n        pass\n\nclass QAStrategy(PromptStrategy):\n    def build_messages(self, user_input: str) -&gt; list:\n        return [\n            SystemMessage(content=\"You are a helpful assistant.\"),\n            HumanMessage(content=f\"Q: {user_input}\\nA:\")\n        ]\n\nclass SummarizeStrategy(PromptStrategy):\n    def build_messages(self, user_input: str) -&gt; list:\n        return [\n            SystemMessage(content=\"You summarize input text.\"),\n            HumanMessage(content=f\"Summarize:\\n{user_input}\")\n        ]\n\n# ---------------- Chat History ----------------\nclass ChatHistory:\n    def __init__(self):\n        self.data = {}\n\n    def log(self, session_id, role, message):\n        self.data.setdefault(session_id, []).append({\n            \"timestamp\": datetime.now().isoformat(),\n            \"role\": role,\n            \"message\": message\n        })\n\n    def show(self, session_id):\n        for entry in self.data.get(session_id, []):\n            print(f\"{entry['timestamp']} [{entry['role']}]: {entry['message']}\")\n\n    def export(self, session_id):\n        return json.dumps(self.data.get(session_id, []), indent=2)\n\n# ---------------- LLM Client ----------------\nclass LLMClient:\n    def __init__(self, adapter: LLMAdapter, strategy: PromptStrategy):\n        self.adapter = adapter\n        self.strategy = strategy\n        self.history = ChatHistory()\n        self.session_id = str(uuid.uuid4())\n\n    def chat(self, user_input):\n        messages = self.strategy.build_messages(user_input)\n        self.history.log(self.session_id, \"user\", user_input)\n        response = self.adapter.generate_response(messages)\n        self.history.log(self.session_id, \"assistant\", response)\n        return response\n\n    def view_history(self):\n        self.history.show(self.session_id)\n\n    def export_history(self):\n        return self.history.export(self.session_id)",
    "crumbs": [
      "Blog",
      "LLM",
      "LLM Class"
    ]
  },
  {
    "objectID": "LLM/llmclass.html#chatgpt",
    "href": "LLM/llmclass.html#chatgpt",
    "title": "LLM Class",
    "section": "ChatGPT",
    "text": "ChatGPT\n\n# ---------------- Entry Point ----------------\nadapter = LangChainOpenAIAdapter()\nstrategy = QAStrategy()  # or SummarizeStrategy()\nbot = LLMClient(adapter, strategy)\n\n\nprint(\"\\nðŸ§  Chat Session Started â€” Type 'exit' to quit.\\n\")\n\nwhile True:\n    msg = input(\"ðŸ‘¤ You: \")\n    if msg.strip().lower() in {\"exit\", \"quit\"}:\n        break\n    response = bot.chat(msg)\n    print(f\"ðŸ¤– Bot: {response}\")\n\n\n# View history in Jupyter\nbot.view_history()\n\n# Export as JSON\njson_output = bot.export_history()\nprint(json_output)\n\n2025-04-21T14:38:45.428523 [user]: how is the weather today\n2025-04-21T14:38:46.342267 [assistant]: I'm sorry, but I don't have real-time data access to provide current weather updates. You can check the latest weather information through a weather app or website for your location.\n2025-04-21T14:38:52.994441 [user]: what day is it?\n2025-04-21T14:38:54.119662 [assistant]: I'm sorry, I can't provide real-time information or current dates. Please check your device's calendar or clock for the current day.\n2025-04-21T14:39:10.178454 [user]: whats the difference between django and fastapi\n2025-04-21T14:39:17.439090 [assistant]: Django and FastAPI are both web frameworks for Python, but they have different design philosophies and use cases. Here's a comparison of the two:\n\n1. **Speed and Performance**:\n   - **FastAPI**: As the name suggests, FastAPI is designed for high performance. It is built on top of Starlette and is asynchronous by nature, making it well-suited for handling lots of concurrent requests efficiently. It's particularly beneficial for applications that require high throughput and low latency, such as APIs and microservices.\n   - **Django**: Django is a synchronous framework, which can be less efficient for handling a large number of concurrent requests. However, with Django Channels, you can add asynchronous capabilities if needed. Django is more focused on rapid development and follows a \"batteries-included\" philosophy.\n\n2. **Features and Ecosystem**:\n   - **Django**: It is a full-fledged web framework that comes with a wide range of features out of the box, such as an ORM (Object-Relational Mapping), an admin panel, authentication, forms, and more. This makes it ideal for building traditional monolithic web applications quickly.\n   - **FastAPI**: It is minimalistic and focused on building APIs. It provides automatic generation of OpenAPI documentation and JSON Schema, which is very useful for API development. It doesnâ€™t come with built-in components like Djangoâ€™s ORM or admin interface, but it integrates well with third-party libraries.\n\n3. **Ease of Use and Learning Curve**:\n   - **Django**: It is known for its \"Django way\" of doing things and has a steeper learning curve for beginners. However, once you are familiar with its conventions, it can significantly speed up development.\n   - **FastAPI**: It is relatively easy to get started with, especially if you are familiar with modern Python features like type hints. The framework leverages Python type hints to provide input validation and serialization automatically.\n\n4. **Use Cases**:\n   - **Django**: Ideal for building complex web applications that require a lot of built-in functionality and rapid prototyping. It's popular for building content management systems, social networks, and other web applications.\n   - **FastAPI**: Best suited for building high-performance APIs and microservices. It is often used in projects that require asynchronous capabilities and high throughput, like real-time data processing.\n\n5. **Community and Support**:\n   - **Django**: Has a large and mature community with extensive documentation and a wide range of third-party packages.\n   - **FastAPI**: A newer framework with a rapidly growing community. It has excellent documentation and is becoming increasingly popular for API development.\n\nIn summary, if you're building a traditional web application with a need for a lot of built-in tools and a structured framework, Django might be the better choice. If you're focused on building fast, efficient RESTful APIs with modern Python practices, FastAPI could be more suitable.\n2025-04-21T14:39:24.569336 [user]: exit/\"\n2025-04-21T14:39:26.610750 [assistant]: It looks like your query might be incomplete or contain a typo. Could you please provide more context or clarify what you're asking about \"exit/\"? Are you looking for information on how to exit a program or command in a specific context, or is there something else you need help with? Let me know so I can assist you better!\n[\n  {\n    \"timestamp\": \"2025-04-21T14:38:45.428523\",\n    \"role\": \"user\",\n    \"message\": \"how is the weather today\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T14:38:46.342267\",\n    \"role\": \"assistant\",\n    \"message\": \"I'm sorry, but I don't have real-time data access to provide current weather updates. You can check the latest weather information through a weather app or website for your location.\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T14:38:52.994441\",\n    \"role\": \"user\",\n    \"message\": \"what day is it?\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T14:38:54.119662\",\n    \"role\": \"assistant\",\n    \"message\": \"I'm sorry, I can't provide real-time information or current dates. Please check your device's calendar or clock for the current day.\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T14:39:10.178454\",\n    \"role\": \"user\",\n    \"message\": \"whats the difference between django and fastapi\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T14:39:17.439090\",\n    \"role\": \"assistant\",\n    \"message\": \"Django and FastAPI are both web frameworks for Python, but they have different design philosophies and use cases. Here's a comparison of the two:\\n\\n1. **Speed and Performance**:\\n   - **FastAPI**: As the name suggests, FastAPI is designed for high performance. It is built on top of Starlette and is asynchronous by nature, making it well-suited for handling lots of concurrent requests efficiently. It's particularly beneficial for applications that require high throughput and low latency, such as APIs and microservices.\\n   - **Django**: Django is a synchronous framework, which can be less efficient for handling a large number of concurrent requests. However, with Django Channels, you can add asynchronous capabilities if needed. Django is more focused on rapid development and follows a \\\"batteries-included\\\" philosophy.\\n\\n2. **Features and Ecosystem**:\\n   - **Django**: It is a full-fledged web framework that comes with a wide range of features out of the box, such as an ORM (Object-Relational Mapping), an admin panel, authentication, forms, and more. This makes it ideal for building traditional monolithic web applications quickly.\\n   - **FastAPI**: It is minimalistic and focused on building APIs. It provides automatic generation of OpenAPI documentation and JSON Schema, which is very useful for API development. It doesn\\u2019t come with built-in components like Django\\u2019s ORM or admin interface, but it integrates well with third-party libraries.\\n\\n3. **Ease of Use and Learning Curve**:\\n   - **Django**: It is known for its \\\"Django way\\\" of doing things and has a steeper learning curve for beginners. However, once you are familiar with its conventions, it can significantly speed up development.\\n   - **FastAPI**: It is relatively easy to get started with, especially if you are familiar with modern Python features like type hints. The framework leverages Python type hints to provide input validation and serialization automatically.\\n\\n4. **Use Cases**:\\n   - **Django**: Ideal for building complex web applications that require a lot of built-in functionality and rapid prototyping. It's popular for building content management systems, social networks, and other web applications.\\n   - **FastAPI**: Best suited for building high-performance APIs and microservices. It is often used in projects that require asynchronous capabilities and high throughput, like real-time data processing.\\n\\n5. **Community and Support**:\\n   - **Django**: Has a large and mature community with extensive documentation and a wide range of third-party packages.\\n   - **FastAPI**: A newer framework with a rapidly growing community. It has excellent documentation and is becoming increasingly popular for API development.\\n\\nIn summary, if you're building a traditional web application with a need for a lot of built-in tools and a structured framework, Django might be the better choice. If you're focused on building fast, efficient RESTful APIs with modern Python practices, FastAPI could be more suitable.\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T14:39:24.569336\",\n    \"role\": \"user\",\n    \"message\": \"exit/\\\"\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T14:39:26.610750\",\n    \"role\": \"assistant\",\n    \"message\": \"It looks like your query might be incomplete or contain a typo. Could you please provide more context or clarify what you're asking about \\\"exit/\\\"? Are you looking for information on how to exit a program or command in a specific context, or is there something else you need help with? Let me know so I can assist you better!\"\n  }\n]",
    "crumbs": [
      "Blog",
      "LLM",
      "LLM Class"
    ]
  },
  {
    "objectID": "LLM/llmclass.html#ollama-deepseek",
    "href": "LLM/llmclass.html#ollama-deepseek",
    "title": "LLM Class",
    "section": "Ollama Deepseek",
    "text": "Ollama Deepseek\n\n# ---------------- Entry Point ----------------\nadapter = OllamaAdapter()\nstrategy = QAStrategy()  # or SummarizeStrategy()\nollama_bot = LLMClient(adapter, strategy)\n\n\nprint(\"\\nðŸ§  Chat Session Started â€” Type 'exit' to quit.\\n\")\n\nwhile True:\n    msg = input(\"ðŸ‘¤ You: \")\n    if msg.strip().lower() in {\"exit\", \"quit\"}:\n        break\n    response = ollama_bot.chat(msg)\n    print(f\"ðŸ¤– Bot: {response}\")\n\n\nðŸ§  Chat Session Started â€” Type 'exit' to quit.\n\n\n\nðŸ‘¤ You:  exit\n\n\n\n# View history in Jupyter\nollama_bot.view_history()\n\n# Export as JSON\njson_output = ollama_bot.export_history()\nprint(json_output)\n\n2025-04-21T16:39:53.306768 [user]: what model is this?\n2025-04-21T16:39:57.890272 [assistant]: &lt;think&gt;\nOkay, so the user asked \"what model is this?\" and I responded with \"I'm unable to answer that question.\" Hmm, they might be referring to a specific product or item. Maybe it's related to cars or electronics? Since I can't see images or descriptions, I can only go by what's provided.\n\nWait, maybe the user was asking about a car model but didn't specify any details like make or year. Without more info, it's hard to pinpoint. I should let them know that without additional context, I can't provide an accurate answer.\n&lt;/think&gt;\n\nI'm sorry, but I need more information to answer your question. Could you please provide details about the item in question?\n2025-04-21T16:40:25.576062 [user]: whats the difference between fastapi and django\n2025-04-21T16:40:46.009118 [assistant]: &lt;think&gt;\nOkay, so I need to figure out the differences between FastAPI and Django. Both seem like modern web frameworks for Python, but they're used differently. Let me try to break this down step by step.\n\nFirst off, I know that Django is built with a specific set of principles in mind. It's part of the Django ecosystem, which includes Channels, DRF, etc., so it has a lot more built-in features out of the box compared to something like FastAPI. Django is server-side focusedâ€”so it handles all the backend stuff for you, including routing, templating, databases, and more.\n\nOn the other hand, FastAPI seems to be more streamlined. It's meant specifically for building APIs without handling as much else on the server side. So maybe if I just need an API, FastAPI is easier to set up because it doesn't require so many other Django features. But does that mean it's less feature-rich? Probably.\n\nPerformance-wise, Django might be better because it has built-in optimizations for things like database operations and request handling since it's handling all the server stuff itself. FastAPI, being lighter, could potentially serve requests faster because there's less overhead from handling more tasks on the server.\n\nDeployment also seems different. Django probably requires a full setup with all its componentsâ€”like an ORM, templating engines, etc.â€”which can take longer to set up and configure properly. FastAPI might have a quicker deployment process since it's focused only on the API part.\n\nI've heard that Django has strong community support for certain use cases, like full-fledged web apps with rich features, but maybe not as much for just APIs. FastAPI is probably growing more in popularity because of its simplicity and scalability when dealing solely with APIs.\n\nIn terms of security, both handle some aspects, but I think Django's built-in security features might be more comprehensive because it's part of a larger framework that takes care of a lot else on the server side. FastAPI would need to integrate additional security measures if needed, which could add complexity.\n\nFor development and testing, Django probably has better tools since it's a full-stack framework. It might have integrated IDE support and debugging features right out of the box for web applications. FastAPI, being more minimalistic, might require using other frameworks like pytest or separate testing tools for different parts of the API.\n\nI also remember that Django uses Python 2 by default now, which is an older version, but maybe they're moving to Python 3. FastAPI supports both versions though, so if I need Python 2 functionality, Django still works, whereas with FastAPI, I'd have to use either.\n\nThinking about scalability, Django might be a bit more resource-heavy because it's handling all the server-side stuff. FastAPI could scale better in environments where only APIs are needed without the extra overhead of Django's features.\n\nI'm also considering which one is easier to learn for someone new. Django has a steeper learning curve because of its complexity and the number of features, while FastAPI might be more approachable since it's focused on what it does bestâ€”building APIs quickly.\n\nPutting this all together, if I need a full-featured web application with everything server-side handled by Django, that's the way to go. But for an API-first project where simplicity and ease of setting up are key, FastAPI would be more suitable because it's lighter and easier to deploy.\n&lt;/think&gt;\n\n**Comparative Analysis of FastAPI vs Django**\n\n1. **Framework Scope:**\n   - **Django:** A full-stack web framework offering comprehensive features like routing, templating, databases, ORM, etc., ideal for complex web applications.\n   - **FastAPI:** Primarily designed for building APIs with minimal server-side handling, making it highly efficient for API-centric projects.\n\n2. **Server-Side Features:**\n   - **Django:** Handles a wide range of server-side tasks including database operations, templating, and routing, providing robust built-in optimizations.\n   - **FastAPI:** Focuses solely on the backend APIs, reducing server-side overhead and improving performance for API requests.\n\n3. **Deployment Complexity:**\n   - **Django:** Requires setting up multiple components (ORMs, templates), which can be time-consuming but ensures a fully functional web application.\n   - **FastAPI:** Easier to deploy due to its streamlined approach, ideal for projects focused solely on APIs.\n\n4. **Performance:**\n   - **Django:** May offer better performance due to optimized server-side tasks and built-in optimizations.\n   - **FastAPI:** Typically faster as it's lightweight, especially in API-heavy environments with reduced overhead.\n\n5. **Security Features:**\n   - **Django:** Built-in security features integrated into a robust framework, beneficial for full web applications.\n   - **FastAPI:** May require additional security measures beyond the framework, adding complexity.\n\n6. **Development & Testing:**\n   - **Django:** Integrated IDE support and comprehensive debugging tools suitable for complex web projects.\n   - **FastAPI:** May necessitate separate testing frameworks like pytest for effective development and testing.\n\n7. **Python Version Support:**\n   - **Django:** Currently on Python 2, transitioning to Python 3; may require additional steps for Python 2 features.\n   - **FastAPI:** Supports both Python 2 and 3, offering flexibility.\n\n8. **Scalability:**\n   - **Django:** May be less scalable due to server-side resource utilization.\n   - **FastAPI:** Better suited for environments where only APIs are needed, potentially scaling more efficiently.\n\n9. **Learning Curve:**\n   - **Django:** Steeper learning curve with its extensive features and complexity.\n   - **FastAPI:** More approachable for new developers focusing on API development.\n\n**Conclusion:**\n- **Choose Django** for comprehensive web applications requiring a full-stack framework with built-in optimizations and security.\n- **Choose FastAPI** for projects centered around APIs, offering simplicity, ease of deployment, and scalability without the overhead of server-side tasks.\n[\n  {\n    \"timestamp\": \"2025-04-21T16:39:53.306768\",\n    \"role\": \"user\",\n    \"message\": \"what model is this?\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T16:39:57.890272\",\n    \"role\": \"assistant\",\n    \"message\": \"&lt;think&gt;\\nOkay, so the user asked \\\"what model is this?\\\" and I responded with \\\"I'm unable to answer that question.\\\" Hmm, they might be referring to a specific product or item. Maybe it's related to cars or electronics? Since I can't see images or descriptions, I can only go by what's provided.\\n\\nWait, maybe the user was asking about a car model but didn't specify any details like make or year. Without more info, it's hard to pinpoint. I should let them know that without additional context, I can't provide an accurate answer.\\n&lt;/think&gt;\\n\\nI'm sorry, but I need more information to answer your question. Could you please provide details about the item in question?\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T16:40:25.576062\",\n    \"role\": \"user\",\n    \"message\": \"whats the difference between fastapi and django\"\n  },\n  {\n    \"timestamp\": \"2025-04-21T16:40:46.009118\",\n    \"role\": \"assistant\",\n    \"message\": \"&lt;think&gt;\\nOkay, so I need to figure out the differences between FastAPI and Django. Both seem like modern web frameworks for Python, but they're used differently. Let me try to break this down step by step.\\n\\nFirst off, I know that Django is built with a specific set of principles in mind. It's part of the Django ecosystem, which includes Channels, DRF, etc., so it has a lot more built-in features out of the box compared to something like FastAPI. Django is server-side focused\\u2014so it handles all the backend stuff for you, including routing, templating, databases, and more.\\n\\nOn the other hand, FastAPI seems to be more streamlined. It's meant specifically for building APIs without handling as much else on the server side. So maybe if I just need an API, FastAPI is easier to set up because it doesn't require so many other Django features. But does that mean it's less feature-rich? Probably.\\n\\nPerformance-wise, Django might be better because it has built-in optimizations for things like database operations and request handling since it's handling all the server stuff itself. FastAPI, being lighter, could potentially serve requests faster because there's less overhead from handling more tasks on the server.\\n\\nDeployment also seems different. Django probably requires a full setup with all its components\\u2014like an ORM, templating engines, etc.\\u2014which can take longer to set up and configure properly. FastAPI might have a quicker deployment process since it's focused only on the API part.\\n\\nI've heard that Django has strong community support for certain use cases, like full-fledged web apps with rich features, but maybe not as much for just APIs. FastAPI is probably growing more in popularity because of its simplicity and scalability when dealing solely with APIs.\\n\\nIn terms of security, both handle some aspects, but I think Django's built-in security features might be more comprehensive because it's part of a larger framework that takes care of a lot else on the server side. FastAPI would need to integrate additional security measures if needed, which could add complexity.\\n\\nFor development and testing, Django probably has better tools since it's a full-stack framework. It might have integrated IDE support and debugging features right out of the box for web applications. FastAPI, being more minimalistic, might require using other frameworks like pytest or separate testing tools for different parts of the API.\\n\\nI also remember that Django uses Python 2 by default now, which is an older version, but maybe they're moving to Python 3. FastAPI supports both versions though, so if I need Python 2 functionality, Django still works, whereas with FastAPI, I'd have to use either.\\n\\nThinking about scalability, Django might be a bit more resource-heavy because it's handling all the server-side stuff. FastAPI could scale better in environments where only APIs are needed without the extra overhead of Django's features.\\n\\nI'm also considering which one is easier to learn for someone new. Django has a steeper learning curve because of its complexity and the number of features, while FastAPI might be more approachable since it's focused on what it does best\\u2014building APIs quickly.\\n\\nPutting this all together, if I need a full-featured web application with everything server-side handled by Django, that's the way to go. But for an API-first project where simplicity and ease of setting up are key, FastAPI would be more suitable because it's lighter and easier to deploy.\\n&lt;/think&gt;\\n\\n**Comparative Analysis of FastAPI vs Django**\\n\\n1. **Framework Scope:**\\n   - **Django:** A full-stack web framework offering comprehensive features like routing, templating, databases, ORM, etc., ideal for complex web applications.\\n   - **FastAPI:** Primarily designed for building APIs with minimal server-side handling, making it highly efficient for API-centric projects.\\n\\n2. **Server-Side Features:**\\n   - **Django:** Handles a wide range of server-side tasks including database operations, templating, and routing, providing robust built-in optimizations.\\n   - **FastAPI:** Focuses solely on the backend APIs, reducing server-side overhead and improving performance for API requests.\\n\\n3. **Deployment Complexity:**\\n   - **Django:** Requires setting up multiple components (ORMs, templates), which can be time-consuming but ensures a fully functional web application.\\n   - **FastAPI:** Easier to deploy due to its streamlined approach, ideal for projects focused solely on APIs.\\n\\n4. **Performance:**\\n   - **Django:** May offer better performance due to optimized server-side tasks and built-in optimizations.\\n   - **FastAPI:** Typically faster as it's lightweight, especially in API-heavy environments with reduced overhead.\\n\\n5. **Security Features:**\\n   - **Django:** Built-in security features integrated into a robust framework, beneficial for full web applications.\\n   - **FastAPI:** May require additional security measures beyond the framework, adding complexity.\\n\\n6. **Development & Testing:**\\n   - **Django:** Integrated IDE support and comprehensive debugging tools suitable for complex web projects.\\n   - **FastAPI:** May necessitate separate testing frameworks like pytest for effective development and testing.\\n\\n7. **Python Version Support:**\\n   - **Django:** Currently on Python 2, transitioning to Python 3; may require additional steps for Python 2 features.\\n   - **FastAPI:** Supports both Python 2 and 3, offering flexibility.\\n\\n8. **Scalability:**\\n   - **Django:** May be less scalable due to server-side resource utilization.\\n   - **FastAPI:** Better suited for environments where only APIs are needed, potentially scaling more efficiently.\\n\\n9. **Learning Curve:**\\n   - **Django:** Steeper learning curve with its extensive features and complexity.\\n   - **FastAPI:** More approachable for new developers focusing on API development.\\n\\n**Conclusion:**\\n- **Choose Django** for comprehensive web applications requiring a full-stack framework with built-in optimizations and security.\\n- **Choose FastAPI** for projects centered around APIs, offering simplicity, ease of deployment, and scalability without the overhead of server-side tasks.\"\n  }\n]",
    "crumbs": [
      "Blog",
      "LLM",
      "LLM Class"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html",
    "href": "LLM/openai_langchain.html",
    "title": "OpenAI using LangChain",
    "section": "",
    "text": "Install LangChain and OpenAI SDK:\npip install langchain openai\nOr with Poetry:\npoetry add langchain openai\n\npip install -U langchain-openai langchain-core langchain-community",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#install-dependencies",
    "href": "LLM/openai_langchain.html#install-dependencies",
    "title": "OpenAI using LangChain",
    "section": "",
    "text": "Install LangChain and OpenAI SDK:\npip install langchain openai\nOr with Poetry:\npoetry add langchain openai\n\npip install -U langchain-openai langchain-core langchain-community",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#setup-environment",
    "href": "LLM/openai_langchain.html#setup-environment",
    "title": "OpenAI using LangChain",
    "section": "âœ… 2. Setup Environment",
    "text": "âœ… 2. Setup Environment\nSet your OpenAI API key:\nexport OPENAI_API_KEY=sk-...\nOr load via .env and use load_dotenv().",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#basic-usage-with-chatgpt-gpt-3.5-gpt-4-gpt-4o",
    "href": "LLM/openai_langchain.html#basic-usage-with-chatgpt-gpt-3.5-gpt-4-gpt-4o",
    "title": "OpenAI using LangChain",
    "section": "âœ… 3. Basic Usage with ChatGPT (gpt-3.5 / gpt-4 / gpt-4o)",
    "text": "âœ… 3. Basic Usage with ChatGPT (gpt-3.5 / gpt-4 / gpt-4o)\n\nfrom dotenv import load_dotenv\nimport os\nload_dotenv()\n\nTrue\n\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nchat = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=os.getenv(\"openai_api_key\"),\n)\n\nresponse = chat([\n    HumanMessage(content=\"What's the capital of France?\")\n])\n\nprint(response.content)",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#using-memory-chatbot-style",
    "href": "LLM/openai_langchain.html#using-memory-chatbot-style",
    "title": "OpenAI using LangChain",
    "section": "âœ… 4. Using Memory (ChatBot Style)",
    "text": "âœ… 4. Using Memory (ChatBot Style)\n\nfrom langchain_core.runnables import RunnableWithMessageHistory\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# 1. Chat model\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    temperature=0,\n    api_key=os.getenv(\"openai_api_key\")\n)\n\n# 2. Store for chat history\nstore = {}\n\ndef get_session_history(session_id: str):\n    if session_id not in store:\n        store[session_id] = InMemoryChatMessageHistory()\n    return store[session_id]\n\n# 3. Wrap with memory\nchat = RunnableWithMessageHistory(\n    runnable=llm,\n    get_session_history=get_session_history,  # âœ… updated arg name\n)\n\n# 4. Simulate chat\nsession_id = \"ben-session\"\n\nres1 = chat.invoke(\n    [HumanMessage(content=\"Hi, I'm Ben\")],\n    config={\"configurable\": {\"session_id\": session_id}}\n)\nprint(res1.content)\n\nres2 = chat.invoke(\n    [HumanMessage(content=\"What did I just tell you?\")],\n    config={\"configurable\": {\"session_id\": session_id}}\n)\nprint(res2.content)\n\nHello, Ben! How can I assist you today?\nYou just told me your name, Ben. If there's anything else you'd like to share or ask, feel free to let me know!",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#prompt-templates",
    "href": "LLM/openai_langchain.html#prompt-templates",
    "title": "OpenAI using LangChain",
    "section": "âœ… 5. Prompt Templates",
    "text": "âœ… 5. Prompt Templates\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableMap\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\nload_dotenv()\n\n# Define the prompt using ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a senior software developer\"),\n    (\"human\", \"{question}\")\n])\n\n# Chat model\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=os.getenv(\"openai_api_key\")\n)\n\n# Output parser (converts from message to string)\nparser = StrOutputParser()\n\n# Chain composition: Prompt â†’ LLM â†’ Parser\nchain = prompt | llm | parser\n\n# Run it\nres = chain.invoke({\"question\": \"how to use chatGPT api?\"})\nprint(res)\n\nTo use the ChatGPT API, you'll need to access the API provided by OpenAI. Here's a general guide on how to use it:\n\n1. **Set Up an OpenAI Account:**\n   - Go to the [OpenAI website](https://www.openai.com/) and sign up or log in if you already have an account.\n\n2. **Access API Keys:**\n   - Once logged in, navigate to the API section to obtain your API key. This key will be used to authenticate your requests.\n\n3. **Install Required Libraries:**\n   - Make sure you have the necessary libraries to make HTTP requests. If you're using Python, the `requests` library is commonly used.\n\n   ```bash\n   pip install requests\n   ```\n\n4. **Make API Requests:**\n   - Use the API key to make requests to the ChatGPT API endpoint. Hereâ€™s a basic example using Python:\n\n   ```python\n   import requests\n\n   # Define your API key and the endpoint URL\n   api_key = 'your-api-key-here'\n   url = 'https://api.openai.com/v1/chat/completions'\n\n   # Set up the headers for the request\n   headers = {\n       'Authorization': f'Bearer {api_key}',\n       'Content-Type': 'application/json'\n   }\n\n   # Define the parameters for the request\n   data = {\n       \"model\": \"gpt-3.5-turbo\",\n       \"messages\": [\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n       ],\n       \"max_tokens\": 150,\n       \"temperature\": 0.5\n   }\n\n   # Make the POST request to the API\n   response = requests.post(url, headers=headers, json=data)\n\n   # Parse the response\n   if response.status_code == 200:\n       response_data = response.json()\n       print(response_data['choices'][0]['message']['content'])\n   else:\n       print(f\"Request failed with status code {response.status_code}: {response.text}\")\n   ```\n\n5. **Handle the Response:**\n   - If the request is successful, the response will include the generated text along with additional information like the token usage.\n\n6. **Configure Parameters:**\n   - Adjust parameters such as `temperature`, `max_tokens`, and the conversation context in the `messages` array to achieve the desired behavior or output.\n\n7. **Security Considerations:**\n   - Keep your API key secure. Do not hardcode it in publicly accessible places. Consider using environment variables or secure vault services.\n\n8. **Read the Documentation:**\n   - OpenAI provides detailed [API documentation](https://platform.openai.com/docs/api-reference/introduction) that covers more advanced features and concepts you'll want to explore as you expand your implementation.\n\nWith these steps, you should be able to start using the ChatGPT API to interact with the language model programmatically.",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#streaming-output-from-chatgpt",
    "href": "LLM/openai_langchain.html#streaming-output-from-chatgpt",
    "title": "OpenAI using LangChain",
    "section": "âœ… 6. Streaming Output from ChatGPT",
    "text": "âœ… 6. Streaming Output from ChatGPT\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.callbacks.base import BaseCallbackHandler\nimport os\n\n# Define a proper callback handler\nclass PrintChunkHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token: str, **kwargs) -&gt; None:\n        print(token, end=\"\")\n\n# Set up the streaming model with callback\nchat = ChatOpenAI(\n    model=\"gpt-4o\",\n    streaming=True,\n    callbacks=[PrintChunkHandler()],\n    api_key=os.getenv(\"openai_api_key\")\n)\n\n# Invoke the chat model with a message\nres = chat.invoke([\n    HumanMessage(content=\"Explain quantum computing simply.\")\n])\n\nQuantum computing is a new type of computing that uses the principles of quantum mechanics, which is the science of very small things like atoms and particles, to process information. Traditional computers use bits, which can be either a 0 or a 1, to perform tasks and calculations. Quantum computers, on the other hand, use quantum bits or \"qubits.\"\n\nThe unique features of qubits are:\n\n1. **Superposition**: Unlike bits, qubits can be both 0 and 1 at the same time. This allows quantum computers to process a vast amount of possibilities simultaneously.\n\nits can be linked together in such a way that the state of one qubit can depend on the state of another, no matter how far apart they are. This can lead to much faster processing speeds for certain tasks.\n\n the wrong ones.e**: Quantum algorithms can use interference to amplify the right answers and cancel out\n\n, they hold promise for complex problems in cryptography, optimization, and simulations of molecular structures. However, they are still in the experimental stage and are not yet practical for most applications.",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#tool-using-agent-with-chatgpt",
    "href": "LLM/openai_langchain.html#tool-using-agent-with-chatgpt",
    "title": "OpenAI using LangChain",
    "section": "âœ… 7. Tool-Using Agent (with ChatGPT)",
    "text": "âœ… 7. Tool-Using Agent (with ChatGPT)\n\nimport os\nfrom dotenv import load_dotenv\nimport numexpr\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain.agents import Tool\n\nload_dotenv()\n\n# Define a safe calculator tool\ndef safe_calculator(expression: str) -&gt; str:\n    try:\n        return str(numexpr.evaluate(expression).item())\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ntools = [\n    Tool(\n        name=\"Calculator\",\n        func=safe_calculator,\n        description=\"Useful for math operations like addition, subtraction, multiplication, and division.\"\n    )\n]\n\n# Initialize the LLM\nchat_model = ChatOpenAI(\n    model=\"gpt-4o\",\n    temperature=0,\n    api_key=os.getenv(\"openai_api_key\")\n)\n\n# Create the LangGraph ReAct agent executor\nagent_executor = create_react_agent(\n    model=chat_model,\n    tools=tools,\n)\n\n# Create a message input for the agent\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is 7 * (4 + 3)?\"}\n]\n\n# Run the agent with a prompt\nresponse = agent_executor.invoke({\"messages\": messages})\n\n# Extract the final AI message from the returned state\nai_messages = response.get(\"messages\", [])\n\nif ai_messages:\n    final_message = ai_messages[-1].content  # âœ… use .content for LangChain message objects\n    print(\"\\nFinal Answer:\", final_message)\nelse:\n    print(\"No response from agent.\")\n\n\nFinal Answer: The result of \\(7 \\times (4 + 3)\\) is 49.",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#rag-with-chatgpt-retrieval-augmented-generation",
    "href": "LLM/openai_langchain.html#rag-with-chatgpt-retrieval-augmented-generation",
    "title": "OpenAI using LangChain",
    "section": "âœ… 8. RAG with ChatGPT (Retrieval-Augmented Generation)",
    "text": "âœ… 8. RAG with ChatGPT (Retrieval-Augmented Generation)\n\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain_core.documents import Document\nimport os\n\n# Sample documents\ndocs = [\n    Document(page_content=\"Climate change refers to long-term shifts in temperatures and weather patterns.\"),\n    Document(page_content=\"Greenhouse gases are a major contributor to global warming.\"),\n]\n\n# Create and save FAISS index\nembeddings = OpenAIEmbeddings(api_key=os.getenv(\"openai_api_key\"))\ndb = FAISS.from_documents(docs, embeddings)\ndb.save_local(\"my_index\")  # This creates my_index/index.faiss and index.pkl\n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.chains.retrieval import create_retrieval_chain\n\nload_dotenv()\n\n# 1. Load vector store\nembeddings = OpenAIEmbeddings(api_key=os.getenv(\"openai_api_key\"))\nvectorstore = FAISS.load_local(\"my_index\", embeddings, allow_dangerous_deserialization=True)\nretriever = vectorstore.as_retriever()\n\n# 2. Correct prompt with {context}\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant who answers questions based on the context.\"),\n    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{input}\")\n])\n\n# 3. LLM setup\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=os.getenv(\"openai_api_key\"),\n    temperature=0,\n)\n\n# 4. Chain setup\nquestion_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\nrag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=question_answer_chain)\n\n# 5. Query\nresponse = rag_chain.invoke({\"input\": \"What did the document say about climate change?\"})\nprint(response[\"answer\"])\n\nThe document mentioned that climate change refers to long-term shifts in temperatures and weather patterns.",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_langchain.html#summary-updated-langchain-patterns-2024",
    "href": "LLM/openai_langchain.html#summary-updated-langchain-patterns-2024",
    "title": "OpenAI using LangChain",
    "section": "âœ… Summary: Updated LangChain Patterns (2024+)",
    "text": "âœ… Summary: Updated LangChain Patterns (2024+)\n\n\n\n\n\n\n\nFeature\nModern Usage with LangChain\n\n\n\n\nBasic Chat\nChatOpenAI(model=\"gpt-4o\") with invoke()\n\n\nStreaming\nChatOpenAI(streaming=True) + callbacks\n\n\nMemory\nRunnableWithMessageHistory + InMemoryChatMessageHistory\n\n\nPrompt Template\nChatPromptTemplate + LLMChain\n\n\nTools / Agents\ncreate_react_agent() + Tool[] (LangGraph)\n\n\nRAG\ncreate_retrieval_chain() + FAISS / Chroma\n\n\n\n\nLet me know if youâ€™d like this in Markdown table format, visual diagram, or as a quick reference card!",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI using LangChain"
    ]
  },
  {
    "objectID": "torch_basics.html",
    "href": "torch_basics.html",
    "title": "Pytorch Basics",
    "section": "",
    "text": "import torch\nimport numpy as np\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#data-types",
    "href": "torch_basics.html#data-types",
    "title": "Pytorch Basics",
    "section": "Data Types",
    "text": "Data Types\n\nfloat_tensor = torch.ones(1, dtype=torch.float)\nfloat_tensor.dtype\n\ntorch.float32\n\n\n\ndouble_tensor = torch.ones(1, dtype=torch.double)\ndouble_tensor.dtype\n\ntorch.float64\n\n\n\ncomplex_float_tensor = torch.ones(1, dtype=torch.complex64)\ncomplex_float_tensor.dtype\n\ntorch.complex64\n\n\n\ncomplex_double_tensor = torch.ones(1, dtype=torch.complex128)\ncomplex_double_tensor.dtype\n\ntorch.complex128\n\n\n\nint_tensor = torch.ones(1, dtype=torch.int)\nint_tensor.dtype\n\ntorch.int32\n\n\n\nlong_tensor = torch.ones(1, dtype=torch.long)\nlong_tensor.dtype\n\ntorch.int64\n\n\n\nuint_tensor = torch.ones(1, dtype=torch.uint8)\nuint_tensor.dtype\n\ntorch.uint8\n\n\n\ndouble_tensor = torch.ones(1, dtype=torch.double)\ndouble_tensor.dtype\n\ntorch.float64\n\n\n\nbool_tensor = torch.ones(1, dtype=torch.bool)\nbool_tensor.dtype\n\ntorch.bool",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#creation-operations",
    "href": "torch_basics.html#creation-operations",
    "title": "Pytorch Basics",
    "section": "Creation Operations",
    "text": "Creation Operations\n\ntorch.is_tensor\n\nx = torch.tensor([1, 2, 3])\nx, torch.is_tensor(x)\n\n(tensor([1, 2, 3]), True)\n\n\n\n\ntorch.set_default_device\n\ntorch.tensor([1.2, 3]).device\n\ndevice(type='cpu')\n\n\n\ntorch.set_default_device('cuda')  # current device is 0\ntorch.tensor([1.2, 3]).device\n\ndevice(type='cuda', index=0)\n\n\n\ntorch.set_default_device('cpu')\na = torch.arange(1000000)\na\n\ntensor([     0,      1,      2,  ..., 999997, 999998, 999999])\n\n\n\na + 1\n\nCPU times: user 20.7 ms, sys: 25.4 ms, total: 46.1 ms\nWall time: 7.37 ms\n\n\ntensor([      1,       2,       3,  ...,  999998,  999999, 1000000])\n\n\n\ntorch.set_default_device('cuda')\na = torch.arange(1000000)\na\n\ntensor([     0,      1,      2,  ..., 999997, 999998, 999999], device='cuda:0')\n\n\n\na + 1\n\nCPU times: user 6.29 ms, sys: 0 ns, total: 6.29 ms\nWall time: 912 Âµs\n\n\ntensor([      1,       2,       3,  ...,  999998,  999999, 1000000],\n       device='cuda:0')\n\n\n\n\ntorch.get_default_dtype\n\ntorch.get_default_dtype()  # initial default for floating point is torch.float32\n\ntorch.float32\n\n\n\ntorch.set_default_dtype(torch.float64)\ntorch.get_default_dtype()  # default is now changed to torch.float64\n\ntorch.float64\n\n\n\n\ntorch.set_printoptions\n\n# Limit the precision of elements\ntorch.set_printoptions(precision=2)\ntorch.tensor([1.12345])\n\ntensor([1.12], device='cuda:0')\n\n\n\n# Limit the number of elements shown\ntorch.set_printoptions(threshold=5)\ntorch.arange(10)\n\ntensor([0, 1, 2,  ..., 7, 8, 9], device='cuda:0')\n\n\n\n# Restore defaults\ntorch.set_printoptions(profile='default')\ntorch.tensor([1.12345])\n\ntensor([1.1235], device='cuda:0')\n\n\n\ntorch.set_default_device('cpu')\ntorch.arange(10)\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\ntorch.as_tensor\n\na = np.array([1, 2, 3])\nt = torch.as_tensor(a)\nt\n\ntensor([1, 2, 3])\n\n\n\nt[0] = -1\na\n\narray([-1,  2,  3])\n\n\n\na = np.array([1, 2, 3])\nt = torch.as_tensor(a, device=torch.device('cuda'))\nt\n\ntensor([1, 2, 3], device='cuda:0')\n\n\n\nt[0] = -1\na\n\narray([1, 2, 3])\n\n\n\nt\n\ntensor([-1,  2,  3], device='cuda:0')\n\n\n\n\ntorch.zeros\n\ntorch.empty((2,2))\n\ntensor([[1.3554e-20, 3.0851e-41],\n        [1.3552e-20, 3.0851e-41]])\n\n\n\ntorch.zeros(2, 3)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntorch.zeros(5)\n\ntensor([0., 0., 0., 0., 0.])\n\n\n\ntorch.ones(2, 3)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\ntorch.ones(5)\n\ntensor([1., 1., 1., 1., 1.])\n\n\n\n\ntorch.range\n\ntorch.arange(5), torch.arange(1, 4), torch.arange(1, 2.5, 0.5)\n\n(tensor([0, 1, 2, 3, 4]), tensor([1, 2, 3]), tensor([1.0000, 1.5000, 2.0000]))\n\n\n\ntorch.linspace(3, 10, steps=5),\\\ntorch.linspace(-10, 10, steps=5),\\\ntorch.linspace(start=-10, end=10, steps=5),\\\ntorch.linspace(start=-10, end=10, steps=1)\n\n(tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000]),\n tensor([-10.,  -5.,   0.,   5.,  10.]),\n tensor([-10.,  -5.,   0.,   5.,  10.]),\n tensor([-10.]))\n\n\n\ntorch.logspace(start=-10, end=10, steps=5),\\\ntorch.logspace(start=0.1, end=1.0, steps=5),\\\ntorch.logspace(start=0.1, end=1.0, steps=1),\\\ntorch.logspace(start=2, end=2, steps=1, base=2)\n\n(tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]),\n tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000]),\n tensor([1.2589]),\n tensor([4.]))\n\n\n\ntorch.empty((2,3), dtype=torch.int64)\n\ntensor([[    140634107035024,     140634107035024, 7454421801564381752],\n        [2322206376936961119, 7310597164893758754,                 145]])\n\n\n\ntorch.full((2, 3), 3.141592)\n\ntensor([[3.1416, 3.1416, 3.1416],\n        [3.1416, 3.1416, 3.1416]])\n\n\n\n\ntorch.quantize_per_tensor\n\ntorch.set_default_dtype(torch.float32)\n\n\ntorch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8),\\\ntorch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr(),\\\ntorch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.tensor(0.1), torch.tensor(10), torch.quint8)\n\n(tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n        quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10),\n tensor([ 0, 10, 20, 30], dtype=torch.uint8),\n tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n        quantization_scheme=torch.per_tensor_affine, scale=0.10000000149011612,\n        zero_point=10))\n\n\n\n\ntorch.complex\n\nreal = torch.tensor([1, 2], dtype=torch.float32)\nimag = torch.tensor([3, 4], dtype=torch.float32)\nz = torch.complex(real, imag)\nz.dtype\n\ntorch.complex64\n\n\n\nreal, imag, z\n\n(tensor([1., 2.]), tensor([3., 4.]), tensor([1.+3.j, 2.+4.j]))\n\n\n\n\ntorch.polar\n\nimport numpy as np\nabs = torch.tensor([1, 2], dtype=torch.float64)\nangle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\nz = torch.polar(abs, angle)\nz\n\ntensor([ 6.1232e-17+1.0000j, -1.4142e+00-1.4142j], dtype=torch.complex128)",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#indexing-slicing-joining-mutating-ops",
    "href": "torch_basics.html#indexing-slicing-joining-mutating-ops",
    "title": "Pytorch Basics",
    "section": "Indexing, Slicing, Joining, Mutating Ops",
    "text": "Indexing, Slicing, Joining, Mutating Ops\n\ntorch.cat\n\nx = torch.randn(2, 3)\nx\n\ntensor([[-0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459]])\n\n\n\ntorch.cat((x, x, x), 0)\n\ntensor([[-0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459],\n        [-0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459],\n        [-0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459]])\n\n\n\ntorch.cat((x, x), 1)\n\ntensor([[-0.1340,  0.5254, -0.3770, -0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459, -2.0310, -0.8961, -0.6459]])\n\n\n\n\ntorch.conj\n\nx = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])\nx, x.is_conj()\n\n(tensor([-1.+1.j, -2.+2.j,  3.-3.j]), False)\n\n\n\ny = torch.conj(x)\ny, y.is_conj()\n\n(tensor([-1.-1.j, -2.-2.j,  3.+3.j]), True)\n\n\n\n\ntorch.permute\n\nx = torch.randn(2, 3, 5)\nx.size()\n\ntorch.Size([2, 3, 5])\n\n\n\ntorch.permute(x, (2, 0, 1)).size()\n\ntorch.Size([5, 2, 3])\n\n\n\n\ntorch.reshape\n\na = torch.arange(4.)\na\n\ntensor([0., 1., 2., 3.])\n\n\n\ntorch.reshape(a, (2, 2))\n\ntensor([[0., 1.],\n        [2., 3.]])\n\n\n\nb = torch.tensor([[0, 1], [2, 3]])\nb\n\ntensor([[0, 1],\n        [2, 3]])\n\n\n\ntorch.reshape(b, (-1,))\n\ntensor([0, 1, 2, 3])\n\n\n\n\ntorch.movedim\n\nt = torch.randn(2,3,5)\nt.size()\n\ntorch.Size([2, 3, 5])\n\n\n\ntorch.movedim(t, 1, 0).shape\n\ntorch.Size([3, 2, 5])\n\n\n\ntorch.movedim(t, 1, 0)\n\ntensor([[[-1.0370, -0.2811,  0.2693,  0.5935, -0.1354],\n         [-2.7575, -2.4650,  0.8077, -0.2873, -1.2993]],\n\n        [[-0.6173, -0.0460, -0.6329,  1.0519, -0.1674],\n         [-0.8958, -0.2828,  1.2355, -1.1782, -1.3597]],\n\n        [[-2.0996, -0.6692, -0.7840,  0.1171,  0.0334],\n         [ 0.4422,  0.2438,  1.0947, -1.2390, -1.4378]]])\n\n\n\ntorch.movedim(t, (1, 2), (0, 1)).shape\n\ntorch.Size([3, 5, 2])\n\n\n\n\ntorch.split\n\na = torch.arange(10).reshape(5, 2)\na\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]])\n\n\n\ntorch.split(a, 2)\n\n(tensor([[0, 1],\n         [2, 3]]),\n tensor([[4, 5],\n         [6, 7]]),\n tensor([[8, 9]]))\n\n\n\ntorch.split(a, [1, 4])\n\n(tensor([[0, 1]]),\n tensor([[2, 3],\n         [4, 5],\n         [6, 7],\n         [8, 9]]))\n\n\n\n\ntorch.t\n\nx = torch.randn(())\nx, torch.t(x)\n\n(tensor(-0.6261), tensor(-0.6261))\n\n\n\nx = torch.randn(3)\nx, torch.t(x)\n\n(tensor([-1.2207, -0.6549, -0.0028]), tensor([-1.2207, -0.6549, -0.0028]))\n\n\n\nx = torch.randn(2, 3)\nx, torch.t(x)\n\n(tensor([[-2.0021,  1.3072, -0.9742],\n         [-1.8025,  0.5369,  0.2517]]),\n tensor([[-2.0021, -1.8025],\n         [ 1.3072,  0.5369],\n         [-0.9742,  0.2517]]))",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#rand",
    "href": "torch_basics.html#rand",
    "title": "Pytorch Basics",
    "section": "Rand",
    "text": "Rand\n\ntorch.rand\n\ntorch.rand(4)\n\ntensor([0.7377, 0.8273, 0.2958, 0.8372])\n\n\n\ntorch.rand(2, 3)\n\ntensor([[0.5028, 0.1841, 0.1133],\n        [0.4431, 0.0016, 0.8662]])\n\n\n\n\ntorch.randint\n\ntorch.randint(3, 5, (3,))\n\ntensor([4, 3, 3])\n\n\n\ntorch.randint(10, (2, 2))\n\ntensor([[5, 3],\n        [8, 1]])\n\n\n\ntorch.randint(3, 10, (2, 2))\n\ntensor([[8, 3],\n        [8, 3]])\n\n\n\n\ntorch.randn\n\ntorch.randn(4)\n\ntensor([0.2036, 0.3526, 0.7444, 1.0029])\n\n\n\ntorch.randn(2, 3)\n\ntensor([[-0.3317,  3.1649,  2.7242],\n        [ 0.2243,  1.2105,  0.9819]])\n\n\n\n\ntorch.normal\n\ntorch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n\ntensor([ 0.3426,  0.6076,  3.0707,  3.1876,  5.1126,  6.5160,  6.9380,  8.2192,\n         8.9378, 10.0406])\n\n\n\ntorch.normal(mean=0.5, std=torch.arange(1., 6.))\n\ntensor([1.4364, 1.6189, 0.1503, 2.7895, 3.0156])\n\n\n\ntorch.normal(mean=torch.arange(1., 6.))\n\ntensor([0.8270, 0.5561, 2.5076, 2.7576, 2.9344])\n\n\n\ntorch.normal(2, 3, size=(1, 4))\n\ntensor([[-1.9720,  2.4059,  3.3461,  0.6155]])",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#save-and-load",
    "href": "torch_basics.html#save-and-load",
    "title": "Pytorch Basics",
    "section": "Save and Load",
    "text": "Save and Load\n\n# Save to file\nimport io\nx = torch.tensor([0, 1, 2, 3, 4])\ntorch.save(x, 'Data/tensor.pt')\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.save(x, buffer)\n\ntorch.load('tensors.pt', weights_only=True)\n\ntorch.load('tensors.pt', map_location=torch.device('cpu'), weights_only=True)\ntorch.load('tensors.pt', map_location=lambda storage, loc: storage, weights_only=True)\ntorch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1), weights_only=True)\ntorch.load('tensors.pt', map_location={'cuda:1': 'cuda:0'}, weights_only=True)\nwith open('tensor.pt', 'rb') as f:\n    buffer = io.BytesIO(f.read())\ntorch.load(buffer, weights_only=False)\ntorch.load('module.pt', encoding='ascii', weights_only=False)",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#parallelism",
    "href": "torch_basics.html#parallelism",
    "title": "Pytorch Basics",
    "section": "Parallelism",
    "text": "Parallelism\n\ntorch.get_num_threads()\n\n6\n\n\n\ntorch.set_num_threads(12)\n\n\ntorch.get_num_interop_threads()\n\n6\n\n\n\ntorch.set_num_interop_threads(12)",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#locally-disabling-gradient-computation",
    "href": "torch_basics.html#locally-disabling-gradient-computation",
    "title": "Pytorch Basics",
    "section": "Locally disabling gradient computation",
    "text": "Locally disabling gradient computation\n\nx = torch.zeros(1, requires_grad=True)\nwith torch.no_grad():\n    y = x * 2\ny.requires_grad\n\nFalse\n\n\n\nis_train = False\nwith torch.set_grad_enabled(is_train):\n    y = x * 2\ny.requires_grad\n\nFalse\n\n\n\ntorch.set_grad_enabled(True)  # this can also be used as a function\ny = x * 2\ny.requires_grad\n\nTrue\n\n\n\ntorch.set_grad_enabled(False)\ny = x * 2\ny.requires_grad\n\nFalse\n\n\n\nno_grad\n\nx = torch.tensor([1.], requires_grad=True)\nwith torch.no_grad():\n    y = x * 2\ny.requires_grad\n\nFalse\n\n\n\n@torch.no_grad()\ndef doubler(x):\n    return x * 2\nz = doubler(x)\nz.requires_grad\n\nFalse\n\n\n\n@torch.no_grad()\ndef tripler(x):\n    return x * 3\nz = tripler(x)\nz.requires_grad\n# factory function exception\nwith torch.no_grad():\n    a = torch.nn.Parameter(torch.rand(10))\na.requires_grad\n\nTrue",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#math-operations",
    "href": "torch_basics.html#math-operations",
    "title": "Pytorch Basics",
    "section": "Math operations",
    "text": "Math operations\n\ntorch.abs(torch.tensor([-1, -2, 3]))\n\ntensor([1, 2, 3])\n\n\n\na = torch.randn(4)\na\n\ntensor([ 0.1512, -0.5116,  1.4073, -0.9758])\n\n\n\ntorch.add(a, 20)\n\ntensor([20.1512, 19.4884, 21.4073, 19.0242])\n\n\n\nb = torch.randn(4)\nc = torch.randn(4, 1)\nb,c, torch.add(b, c, alpha=10)\n\n(tensor([-0.5080, -0.2541,  0.7946, -0.7497]),\n tensor([[ 0.3399],\n         [-0.8642],\n         [-1.4262],\n         [ 0.3894]]),\n tensor([[  2.8912,   3.1451,   4.1939,   2.6495],\n         [ -9.1502,  -8.8963,  -7.8475,  -9.3918],\n         [-14.7701, -14.5162, -13.4674, -15.0117],\n         [  3.3861,   3.6400,   4.6887,   3.1444]]))\n\n\n\ntorch.asin(a)\n\ntensor([ 0.1518, -0.5370,     nan, -1.3503])\n\n\n\ntorch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\n\ntensor([ 0,  1, -4], dtype=torch.int8)\n\n\n\ntorch.ceil(a)\n\ntensor([1., -0., 2., -0.])\n\n\n\ntorch.clamp(a, min=-0.5, max=0.5)\n\ntensor([ 0.1512, -0.5000,  0.5000, -0.5000])\n\n\n\nmin = torch.linspace(-1, 1, steps=4)\nmin, torch.clamp(a, min=min)\n\n(tensor([-1.0000, -0.3333,  0.3333,  1.0000]),\n tensor([ 0.1512, -0.3333,  1.4073,  1.0000]))\n\n\n\nGradient\n\n# Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4]\ncoordinates = (torch.tensor([-2., -1., 1., 4.]),)\nvalues = torch.tensor([4., 1., 1., 16.], )\ntorch.gradient(values, spacing = coordinates)\n\n(tensor([-3., -2.,  2.,  5.]),)\n\n\n\n# Estimates the gradient of the R^2 -&gt; R function whose samples are\n# described by the tensor t. Implicit coordinates are [0, 1] for the outermost\n# dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates\n# partial derivative for both dimensions.\nt = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]])\ntorch.gradient(t)\n\n(tensor([[ 9., 18., 36., 72.],\n         [ 9., 18., 36., 72.]]),\n tensor([[ 1.0000,  1.5000,  3.0000,  4.0000],\n         [10.0000, 15.0000, 30.0000, 40.0000]]))\n\n\n\n# A scalar value for spacing modifies the relationship between tensor indices\n# and input coordinates by multiplying the indices to find the\n# coordinates. For example, below the indices of the innermost\n# 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of\n# the outermost dimension 0, 1 translate to coordinates of [0, 2].\ntorch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1])\n# doubling the spacing between samples halves the estimated partial gradients.\n\n(tensor([[ 4.5000,  9.0000, 18.0000, 36.0000],\n         [ 4.5000,  9.0000, 18.0000, 36.0000]]),\n tensor([[ 0.5000,  0.7500,  1.5000,  2.0000],\n         [ 5.0000,  7.5000, 15.0000, 20.0000]]))\n\n\n\n# Estimates only the partial derivative for dimension 1\ntorch.gradient(t, dim = 1) # spacing = None (implicitly 1.)\n\n(tensor([[ 1.0000,  1.5000,  3.0000,  4.0000],\n         [10.0000, 15.0000, 30.0000, 40.0000]]),)\n\n\n\n# When spacing is a list of scalars, the relationship between the tensor\n# indices and input coordinates changes based on dimension.\n# For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate\n# to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension\n# 0, 1 translate to coordinates of [0, 2].\ntorch.gradient(t, spacing = [3., 2.])\n\n(tensor([[ 3.,  6., 12., 24.],\n         [ 3.,  6., 12., 24.]]),\n tensor([[ 0.5000,  0.7500,  1.5000,  2.0000],\n         [ 5.0000,  7.5000, 15.0000, 20.0000]]))\n\n\n\n# The following example is a replication of the previous one with explicit\n# coordinates.\ncoords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9]))\ntorch.gradient(t, spacing = coords)\n\n(tensor([[ 4.5000,  9.0000, 18.0000, 36.0000],\n         [ 4.5000,  9.0000, 18.0000, 36.0000]]),\n tensor([[ 0.3333,  0.5000,  1.0000,  1.3333],\n         [ 3.3333,  5.0000, 10.0000, 13.3333]]))\n\n\n\n\nReduction Ops\n\na = torch.randn(4, 4)\na\n\ntensor([[ 0.5653, -1.6250, -1.7234,  1.6898],\n        [ 0.0964,  0.4920, -0.8990,  0.2050],\n        [-0.0388, -0.3062, -2.7269,  2.2214],\n        [-0.4256, -1.3614,  1.6437,  1.3073]])\n\n\n\ntorch.argmax(a), torch.argmin(a)\n\n(tensor(11), tensor(10))\n\n\n\ntorch.argmax(a, dim=1), torch.argmin(a, dim=1)\n\n(tensor([3, 1, 3, 2]), tensor([2, 2, 2, 1]))\n\n\n\ntorch.argmin(a, dim=1, keepdim=True)\n\ntensor([[2],\n        [2],\n        [2],\n        [1]])\n\n\n\ntorch.amax(a, 0)\n\ntensor([0.5653, 0.4920, 1.6437, 2.2214])\n\n\n\ntorch.amin(a, 0)\n\ntensor([-0.4256, -1.6250, -2.7269,  0.2050])\n\n\nTests if all element in input evaluates to True.\n\ntorch.all(a)\n\ntensor(True)\n\n\nTests if any element in input evaluates to True.\n\ntorch.any(a)\n\ntensor(True)\n\n\n\nx = torch.randn(4)\ny = torch.randn(4)\nx,y\n\n(tensor([-0.8039, -1.4679,  0.4484, -0.5348]),\n tensor([-0.5979, -1.1656, -1.2298, -0.2573]))\n\n\n\ntorch.dist(x, y, 3.5)\n\ntensor(1.6806)\n\n\n\ntorch.dist(x, y, 3)\n\ntensor(1.6850)\n\n\n\ntorch.dist(x, y, 0)\n\ntensor(4.)\n\n\n\ntorch.dist(x, y, 2)\n\ntensor(1.7399)\n\n\n\na = torch.randn(1, 3)\na\n\ntensor([[-0.7396, -1.3435,  1.0013]])\n\n\n\ntorch.mean(a)\n\ntensor(-0.3606)\n\n\n\ntorch.prod(a)\n\ntensor(0.9949)\n\n\n\na = torch.randn(4, 2)\na\n\ntensor([[-0.0451, -0.4858],\n        [-0.1007,  0.4423],\n        [-0.2149,  0.4494],\n        [ 0.7059,  0.0417]])\n\n\n\ntorch.prod(a, 1)\n\ntensor([ 0.0219, -0.0446, -0.0966,  0.0295])\n\n\n\ntorch.std(a, dim=1, keepdim=True)\n\ntensor([[0.3116],\n        [0.3840],\n        [0.4697],\n        [0.4696]])\n\n\n\ntorch.sum(a)\n\ntensor(0.7927)\n\n\n\ntorch.sum(a, 0)\n\ntensor([0.3451, 0.4476])\n\n\n\ntorch.var(a, dim=0, keepdim=True)\n\ntensor([[0.1756, 0.1951]])\n\n\n\ntorch.var_mean(a, dim=0, keepdim=True)\n\n(tensor([[0.1756, 0.1951]]), tensor([[0.0863, 0.1119]]))\n\n\n\n\nComparison Ops\n\ntorch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\n\ntensor([[ True, False],\n        [False,  True]])\n\n\n\ntorch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\n\nFalse\n\n\n\ntorch.isnan(torch.tensor([1, float('nan'), 2]))\n\ntensor([False,  True, False])\n\n\n\n\nBroadcast\n\nx = torch.tensor([1, 2, 3])\ntorch.broadcast_to(x, (3, 3))\n\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n\n\n\ntorch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\n\ntorch.Size([1, 3, 2])\n\n\n\nx = torch.tensor([[0, 2], [1, 1], [2, 0]]).T\nx\nx, torch.cov(x)\n\n(tensor([[0, 1, 2],\n         [2, 1, 0]]),\n tensor([[ 1., -1.],\n         [-1.,  1.]]))\n\n\n\n\nCovolution\n\ntorch.cov(x, correction=0)\n\ntensor([[ 0.6667, -0.6667],\n        [-0.6667,  0.6667]])\n\n\n\nfw = torch.randint(1, 10, (3,))\naw = torch.rand(3)\nfw, aw, torch.cov(x, fweights=fw, aweights=aw)\n\n(tensor([2, 2, 9]),\n tensor([0.1281, 0.5609, 0.1982]),\n tensor([[ 0.4583, -0.4583],\n         [-0.4583,  0.4583]]))\n\n\n\n\nDiagonal\n\na = torch.randn(3)\na, torch.diag(a)\n\n(tensor([-0.0159, -0.2550, -0.3652]),\n tensor([[-0.0159,  0.0000,  0.0000],\n         [ 0.0000, -0.2550,  0.0000],\n         [ 0.0000,  0.0000, -0.3652]]))\n\n\n\ntorch.diag(a, 1)\n\ntensor([[ 0.0000, -0.0159,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.2550,  0.0000],\n        [ 0.0000,  0.0000,  0.0000, -0.3652],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n\n\na = torch.randn(3, 3)\na\n\ntensor([[ 0.8558, -0.8227,  1.5082],\n        [-0.6674, -0.0815, -0.5271],\n        [-0.1011, -0.3513,  0.1919]])\n\n\n\ntorch.diag(a, 0)\n\ntensor([ 0.8558, -0.0815,  0.1919])\n\n\n\ntorch.diag(a, 1)\n\ntensor([-0.8227, -0.5271])\n\n\n\n\nDiff\n\na = torch.tensor([1, 3, 2])\na, torch.diff(a)\n\n(tensor([1, 3, 2]), tensor([ 2, -1]))\n\n\n\ntorch.gradient(a)\n\n(tensor([ 2.0000,  0.5000, -1.0000]),)\n\n\n\nb = torch.tensor([4, 5])\ntorch.diff(a, append=b)\n\ntensor([ 2, -1,  2,  1])\n\n\n\nc = torch.tensor([[1, 2, 3], [3, 4, 5]])\nc\n\ntensor([[1, 2, 3],\n        [3, 4, 5]])\n\n\n\ntorch.diff(c, dim=0)\n\ntensor([[2, 2, 2]])\n\n\n\ntorch.diff(c, dim=1)\n\ntensor([[1, 1],\n        [1, 1]])\n\n\n\n\nEinsum\n\n# trace\ntorch.einsum('ii', torch.randn(4, 4))\n\ntensor(2.8873)\n\n\n\n# diagonal\ntorch.einsum('ii-&gt;i', torch.randn(4, 4))\n\ntensor([ 0.1727, -1.2934,  0.1134,  0.6699])\n\n\n\n# outer product\nx = torch.randn(5)\ny = torch.randn(4)\nx,y\n\n(tensor([ 0.8261,  2.2608,  0.5666, -2.3195, -1.1706]),\n tensor([-0.1575,  1.3682, -1.6248, -0.4177]))\n\n\n\ntorch.einsum('i,j-&gt;ij', x, y)\n\ntensor([[-0.1301,  1.1302, -1.3423, -0.3451],\n        [-0.3561,  3.0932, -3.6735, -0.9444],\n        [-0.0892,  0.7751, -0.9206, -0.2367],\n        [ 0.3653, -3.1735,  3.7688,  0.9689],\n        [ 0.1844, -1.6016,  1.9021,  0.4890]])\n\n\n\n# batch matrix multiplication\nAs = torch.randn(3, 2, 5)\nBs = torch.randn(3, 5, 4)\nAs, Bs\n\n(tensor([[[ 1.7602,  1.3836, -0.8395,  1.2415,  0.2106],\n          [-1.0765, -1.6569,  1.0785,  2.6039, -0.2173]],\n \n         [[-0.3606,  0.7737, -0.3265, -0.3982, -0.1795],\n          [ 0.4787,  0.3987,  0.5030,  2.0617, -1.2417]],\n \n         [[ 1.2606, -1.7532,  1.2267,  0.2588, -0.6794],\n          [ 0.4158,  0.3457, -0.3235, -1.4921, -1.0168]]]),\n tensor([[[-1.1582, -0.5702, -0.5820,  1.4487],\n          [ 1.6068,  0.6026,  1.4152,  0.7676],\n          [-1.3955, -0.1634, -0.9673, -1.1214],\n          [ 0.8568, -1.2561, -1.0304,  0.8901],\n          [ 0.1840,  0.1531,  1.4496,  1.6163]],\n \n         [[-0.0585, -0.2001,  0.1084, -0.9644],\n          [-0.6568,  1.1768, -1.4877,  0.0249],\n          [-0.1339,  0.2070,  1.4734,  0.0627],\n          [ 0.3322,  0.5430, -0.5031,  0.1328],\n          [ 0.0214,  1.1490,  0.4914,  0.2774]],\n \n         [[ 0.3791,  0.3227, -2.6659,  0.6282],\n          [ 1.6384,  0.2688, -2.1299,  1.0743],\n          [ 0.1086, -0.0778, -0.2566,  0.0419],\n          [ 0.0142,  1.9122, -1.7333,  1.9668],\n          [ 0.1300,  0.8281, -1.0565, -1.4234]]]))\n\n\n\ntorch.einsum('bij,bjk-&gt;bik', As, Bs)\n\ntensor([[[ 2.4586, -1.5598,  0.7718,  5.9990],\n         [-0.7296, -3.8648, -5.7596, -2.0744]],\n\n        [[-0.5795,  0.4926, -1.5591,  0.2439],\n         [ 0.3011,  0.1702, -1.4475, -0.4909]],\n\n        [[-2.3460, -0.2277,  0.3280,  0.4358],\n         [ 0.5354, -3.4429,  1.8989, -0.8684]]])\n\n\n\n# with sublist format and ellipsis\ntorch.einsum(As, [..., 0, 1], Bs, [..., 1, 2], [..., 0, 2])\n\ntensor([[[ 2.4586, -1.5598,  0.7718,  5.9990],\n         [-0.7296, -3.8648, -5.7596, -2.0744]],\n\n        [[-0.5795,  0.4926, -1.5591,  0.2439],\n         [ 0.3011,  0.1702, -1.4475, -0.4909]],\n\n        [[-2.3460, -0.2277,  0.3280,  0.4358],\n         [ 0.5354, -3.4429,  1.8989, -0.8684]]])\n\n\n\n# batch permute\nA = torch.randn(2, 3, 4, 5)\ntorch.einsum('...ij-&gt;...ji', A).shape\n\ntorch.Size([2, 3, 5, 4])\n\n\n\n# equivalent to torch.nn.functional.bilinear\nA = torch.randn(3, 5, 4)\nl = torch.randn(2, 5)\nr = torch.randn(2, 4)\nA,l,r\n\n(tensor([[[-0.5127, -0.0817, -0.5872, -2.0090],\n          [-0.3169, -1.0569, -0.2818,  1.8631],\n          [-1.5130, -0.7615, -0.3052,  0.7982],\n          [-0.3297,  1.6522,  0.9849, -1.5223],\n          [-0.5275,  0.1215, -0.5165, -0.4254]],\n \n         [[ 0.7555, -0.9271,  2.2486, -0.5548],\n          [ 0.0759, -0.3391, -1.3095, -0.2525],\n          [-0.2529, -1.0799,  0.5418,  0.4821],\n          [ 0.8987, -0.0494, -0.5371, -1.5568],\n          [-0.2188,  0.9023,  0.8624,  0.7310]],\n \n         [[ 2.1191,  0.3084, -0.8052, -0.2008],\n          [-0.3211, -0.4985,  0.3982, -1.0806],\n          [ 0.6403, -0.8154, -2.6253,  2.4096],\n          [ 0.5290, -1.3181, -0.8800,  0.9082],\n          [-0.8891, -1.0462,  1.2305, -0.7983]]]),\n tensor([[ 1.1455, -1.6298,  0.2697,  1.6974, -1.9843],\n         [ 0.5881, -0.7756,  0.5787, -0.1486, -1.0844]]),\n tensor([[-0.4899,  0.0820, -0.1346, -0.0343],\n         [-0.6835,  0.4302, -0.0725,  0.6343]]))\n\n\n\ntorch.einsum('bn,anm,bm-&gt;ba', l, A, r)\n\ntensor([[ 0.2351, -1.6662, -2.1157],\n        [-0.7526, -1.4344,  0.7918]])\n\n\n\n\ntorch.flatten\n\nt = torch.tensor([[[1, 2],\n                   [3, 4]],\n                  [[5, 6],\n                   [7, 8]]])\ntorch.flatten(t)\n\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n\n\n\ntorch.flatten(t, start_dim=1)\n\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\n\n\n\n\ntorch.histogram\n\ntorch.histogram(torch.tensor([1., 2, 1]),\n                bins=4,\n                range=(0., 3.),\n                weight=torch.tensor([1., 2., 4.]))\n\ntorch.return_types.histogram(\nhist=tensor([0., 5., 2., 0.]),\nbin_edges=tensor([0.0000, 0.7500, 1.5000, 2.2500, 3.0000]))\n\n\n\ntorch.histogram(torch.tensor([1., 2, 1]),\n                bins=4, range=(0., 3.),\n                weight=torch.tensor([1., 2., 4.]),\n                density=True)\n\ntorch.return_types.histogram(\nhist=tensor([0.0000, 0.9524, 0.3810, 0.0000]),\nbin_edges=tensor([0.0000, 0.7500, 1.5000, 2.2500, 3.0000]))\n\n\n\n\ntorch.meshgrid\n\nx = torch.tensor([1, 2, 3])\ny = torch.tensor([4, 5, 6])\nx,y\n\n(tensor([1, 2, 3]), tensor([4, 5, 6]))\n\n\n\ngrid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\ngrid_x,grid_y\n\n(tensor([[1, 1, 1],\n         [2, 2, 2],\n         [3, 3, 3]]),\n tensor([[4, 5, 6],\n         [4, 5, 6],\n         [4, 5, 6]]))\n\n\n\ntorch.equal(torch.cat(tuple(torch.dstack([grid_x, grid_y]))),\n            torch.cartesian_prod(x, y))\n\nimport matplotlib.pyplot as plt\nxs = torch.linspace(-5, 5, steps=100)\nys = torch.linspace(-5, 5, steps=100)\nx, y = torch.meshgrid(xs, ys, indexing='xy')\nz = torch.sin(torch.sqrt(x * x + y * y))\nax = plt.axes(projection='3d')\nax.plot_surface(x.numpy(), y.numpy(), z.numpy())\nplt.show()",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#torch.matmul",
    "href": "torch_basics.html#torch.matmul",
    "title": "Pytorch Basics",
    "section": "torch.matmul",
    "text": "torch.matmul\n\n# vector x vector\ntensor1 = torch.randn(3)\ntensor2 = torch.randn(3)\ntensor1, tensor2\n\n(tensor([0.6342, 0.6817, 0.5164]), tensor([-0.5737,  1.6013,  0.5605]))\n\n\n\ntorch.matmul(tensor1, tensor2)\n\ntensor(1.0173)\n\n\n\n# matrix x vector\ntensor1 = torch.randn(3, 4)\ntensor2 = torch.randn(4)\ntensor1, tensor2\n\n(tensor([[-0.7989, -0.2968, -0.5672, -0.3673],\n         [ 0.3948,  0.9695, -0.9593,  0.5856],\n         [-0.6744,  1.4336, -1.3985, -0.2974]]),\n tensor([-2.0189, -1.3822, -0.2220,  0.0440]))\n\n\n\ntorch.matmul(tensor1, tensor2).size(), torch.matmul(tensor1, tensor2)\n\n(torch.Size([3]), tensor([ 2.1329, -1.8983, -0.3226]))\n\n\n\n# batched matrix x broadcasted vector\ntensor1 = torch.randn(10, 3, 4)\ntensor2 = torch.randn(4)\ntorch.matmul(tensor1, tensor2).size()\n\ntorch.Size([10, 3])\n\n\n\n# batched matrix x batched matrix\ntensor1 = torch.randn(10, 3, 4)\ntensor2 = torch.randn(10, 4, 5)\ntorch.matmul(tensor1, tensor2).size()\n\ntorch.Size([10, 3, 5])\n\n\n\n# batched matrix x broadcasted matrix\ntensor1 = torch.randn(10, 3, 4)\ntensor2 = torch.randn(4, 5)\ntorch.matmul(tensor1, tensor2).size()\n\ntorch.Size([10, 3, 5])",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "Computer_Vision/image_classification.html",
    "href": "Computer_Vision/image_classification.html",
    "title": "Image classification",
    "section": "",
    "text": "CIFAR-10 and CIFAR-100\n\nCIFAR-10: 60,000 32x32 color images in 10 classes, with 6,000 images per class.\nCIFAR-100: Similar to CIFAR-10 but with 100 classes containing 600 images each.\nUse Case: Benchmarking small-scale image classification models.\n\nImageNet\n\nDescription: Over 14 million images across 1,000 classes.\nUse Case: Large-scale image classification, used for pre-training models.\n\nMNIST and Fashion-MNIST\n\nMNIST: 70,000 grayscale images of handwritten digits (0-9).\nFashion-MNIST: 70,000 grayscale images of fashion items in 10 categories.\nUse Case: Benchmarking simple image classification models.\n\nCOCO (Common Objects in Context)\n\nDescription: 330,000 images with objects segmented into 80 categories.\nUse Case: Object detection, segmentation, and image classification.\n\nSVHN (Street View House Numbers)\n\nDescription: Over 600,000 32x32 color images of house numbers from Google Street View.\nUse Case: Real-world digit classification.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_classification.html#dataset",
    "href": "Computer_Vision/image_classification.html#dataset",
    "title": "Image classification",
    "section": "",
    "text": "CIFAR-10 and CIFAR-100\n\nCIFAR-10: 60,000 32x32 color images in 10 classes, with 6,000 images per class.\nCIFAR-100: Similar to CIFAR-10 but with 100 classes containing 600 images each.\nUse Case: Benchmarking small-scale image classification models.\n\nImageNet\n\nDescription: Over 14 million images across 1,000 classes.\nUse Case: Large-scale image classification, used for pre-training models.\n\nMNIST and Fashion-MNIST\n\nMNIST: 70,000 grayscale images of handwritten digits (0-9).\nFashion-MNIST: 70,000 grayscale images of fashion items in 10 categories.\nUse Case: Benchmarking simple image classification models.\n\nCOCO (Common Objects in Context)\n\nDescription: 330,000 images with objects segmented into 80 categories.\nUse Case: Object detection, segmentation, and image classification.\n\nSVHN (Street View House Numbers)\n\nDescription: Over 600,000 32x32 color images of house numbers from Google Street View.\nUse Case: Real-world digit classification.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_classification.html#models",
    "href": "Computer_Vision/image_classification.html#models",
    "title": "Image classification",
    "section": "Models",
    "text": "Models\n\nConvolutional Neural Networks (CNNs)\n\nLeNet: One of the earliest CNN architectures, used for digit recognition.\nAlexNet: Won the 2012 ImageNet competition, significantly deep with ReLU activations.\nVGGNet: Known for using very small (3x3) convolution filters, 16-19 weight layers.\nGoogLeNet (Inception): Uses a network-in-network architecture, significantly reducing parameters.\nResNet (Residual Networks): Introduces residual connections to train very deep networks.\nDenseNet: Each layer receives input from all previous layers, promoting feature reuse.\n\n\n\nVision Transformers (ViT)\n\nDescription: Adapted the Transformer architecture to image classification tasks.\nUse Case: Competes with CNNs on image classification benchmarks.\n\n\n\nEfficientNet\n\nDescription: Uses a compound scaling method to uniformly scale width, depth, and resolution.\nUse Case: Balances accuracy and efficiency, outperforming many existing models.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_classification.html#hyperparameters",
    "href": "Computer_Vision/image_classification.html#hyperparameters",
    "title": "Image classification",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nLearning Rate\n\n\nDescription: Controls the step size at each iteration while moving towards a minimum of the loss function.\nTuning: Start with a moderate value (e.g., 0.001), use learning rate schedules (e.g., step decay, exponential decay).\n\n\nBatch Size\n\n\nDescription: Number of samples processed before the model is updated.\nTuning: Common values range from 32 to 256. Larger batches require more memory but can leverage better parallelism.\n\n\nNumber of Epochs\n\n\nDescription: Number of complete passes through the training dataset.\nTuning: Monitor validation loss to avoid overfitting, typically between 10 to 100 epochs.\n\n\nOptimizer\n\n\nPopular Choices: SGD, Adam, RMSprop.\nTuning: Adam is a good default choice; try SGD with momentum for potentially better convergence.\n\n\nRegularization Parameters\n\n\nWeight Decay (L2 Regularization): Penalizes large weights, reducing overfitting.\nDropout Rate: Randomly sets a fraction of input units to 0 at each update during training, preventing overfitting.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_classification.html#loss-functions",
    "href": "Computer_Vision/image_classification.html#loss-functions",
    "title": "Image classification",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nCross-Entropy Loss\n\n\nDescription: Measures the performance of a classification model whose output is a probability value between 0 and 1.\nUse Case: Standard for multi-class classification problems.\n\n\nMean Squared Error (MSE)\n\n\nDescription: Measures the average of the squares of the errors between predicted and actual values.\nUse Case: More common in regression, but sometimes used in classification problems with continuous labels.\n\n\nCategorical Hinge Loss\n\n\nDescription: Measures the performance for â€œone-versus-allâ€ classification tasks.\nUse Case: Useful in scenarios with class imbalance.\n\n\nFocal Loss\n\n\nDescription: Modifies cross-entropy loss to address class imbalance by down-weighting the loss assigned to well-classified examples.\nUse Case: Effective in highly imbalanced datasets.\n\n\nOther Important Topics\n\nData Augmentation\n\nDescription: Techniques to artificially increase the size of a dataset by creating modified versions of images.\nMethods: Rotation, flipping, scaling, cropping, color jittering.\nUse Case: Helps improve model generalization.\n\nTransfer Learning\n\nDescription: Using a pre-trained model on a new, but related task.\nApproach: Fine-tuning the pre-trained model on the new dataset.\nUse Case: Effective when the new dataset is small.\n\nEvaluation Metrics\n\nAccuracy: Proportion of correctly predicted instances.\nPrecision, Recall, F1-Score: Useful for class imbalance scenarios.\nConfusion Matrix: Provides insight into the performance of the classification model on each class.\n\nModel Interpretability\n\nGrad-CAM (Gradient-weighted Class Activation Mapping): Visualizes which parts of the image are important for the modelâ€™s predictions.\nSHAP (SHapley Additive exPlanations): Explains the output of machine learning models.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image classification"
    ]
  },
  {
    "objectID": "Computer_Vision/panoptic_segmentation.html#models",
    "href": "Computer_Vision/panoptic_segmentation.html#models",
    "title": "Panoptic Segmentation",
    "section": "Models",
    "text": "Models\nCommon Models: Panoptic FPN, UPSNet.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Panoptic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/panoptic_segmentation.html#hyperparameters",
    "href": "Computer_Vision/panoptic_segmentation.html#hyperparameters",
    "title": "Panoptic Segmentation",
    "section": "Hyperparameters",
    "text": "Hyperparameters",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Panoptic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/panoptic_segmentation.html#loss-functions",
    "href": "Computer_Vision/panoptic_segmentation.html#loss-functions",
    "title": "Panoptic Segmentation",
    "section": "Loss Functions",
    "text": "Loss Functions",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Panoptic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/object_detection.html",
    "href": "Computer_Vision/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "COCO (Common Objects in Context)\n\n\nDescription: Large-scale object detection, segmentation, and captioning dataset with over 200,000 labeled images.\nURL: COCO\n\n\nPASCAL VOC\n\n\nDescription: Dataset for object detection with 20 classes, providing images, annotations, and segmentation masks.\nURL: PASCAL VOC\n\n\nImageNet\n\n\nDescription: Over 14 million images with 1,000 object categories, also includes a subset for object detection.\nURL: ImageNet\n\n\nOpen Images Dataset\n\n\nDescription: Contains ~9 million images annotated with image-level labels, object bounding boxes, and segmentation masks.\nURL: Open Images\n\n\nKITTI\n\n\nDescription: Dataset for autonomous driving with images, 3D point clouds, and annotations for object detection and tracking.\nURL: KITTI",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "Computer_Vision/object_detection.html#popular-datasets",
    "href": "Computer_Vision/object_detection.html#popular-datasets",
    "title": "Object Detection",
    "section": "",
    "text": "COCO (Common Objects in Context)\n\n\nDescription: Large-scale object detection, segmentation, and captioning dataset with over 200,000 labeled images.\nURL: COCO\n\n\nPASCAL VOC\n\n\nDescription: Dataset for object detection with 20 classes, providing images, annotations, and segmentation masks.\nURL: PASCAL VOC\n\n\nImageNet\n\n\nDescription: Over 14 million images with 1,000 object categories, also includes a subset for object detection.\nURL: ImageNet\n\n\nOpen Images Dataset\n\n\nDescription: Contains ~9 million images annotated with image-level labels, object bounding boxes, and segmentation masks.\nURL: Open Images\n\n\nKITTI\n\n\nDescription: Dataset for autonomous driving with images, 3D point clouds, and annotations for object detection and tracking.\nURL: KITTI",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "Computer_Vision/object_detection.html#popular-models",
    "href": "Computer_Vision/object_detection.html#popular-models",
    "title": "Object Detection",
    "section": "Popular Models",
    "text": "Popular Models\n\nR-CNN (Region-based Convolutional Neural Networks)\n\n\nVariants: R-CNN, Fast R-CNN, Faster R-CNN\nDescription: Uses region proposal networks to identify regions of interest and then classify objects within those regions.\nURL: Faster R-CNN\n\n\nYOLO (You Only Look Once)\n\n\nVariants: YOLOv1, YOLOv2, YOLOv3, YOLOv4, YOLOv5\nDescription: Real-time object detection system that predicts bounding boxes and class probabilities directly from full images.\nURL: YOLO\n\n\nSSD (Single Shot MultiBox Detector)\n\n\nDescription: Detects objects in images using a single deep neural network.\nURL: SSD\n\n\nRetinaNet\n\n\nDescription: Combines a backbone network for feature extraction with a novel Focal Loss to handle class imbalance.\nURL: RetinaNet\n\n\nEfficientDet\n\n\nDescription: Scalable and efficient object detector, part of the EfficientNet family.\nURL: EfficientDet",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "Computer_Vision/object_detection.html#hyperparameters",
    "href": "Computer_Vision/object_detection.html#hyperparameters",
    "title": "Object Detection",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nLearning Rate\n\n\nDescription: Controls the step size at each iteration while moving towards a minimum of the loss function.\n\n\nBatch Size\n\n\nDescription: The number of training examples used in one iteration.\n\n\nNumber of Epochs\n\n\nDescription: The number of complete passes through the training dataset.\n\n\nAnchor Boxes\n\n\nDescription: Predefined bounding boxes of different sizes and aspect ratios used for detection.\n\n\nIoU Threshold\n\n\nDescription: Intersection over Union (IoU) threshold for determining true positive detections.\n\n\nNon-Maximum Suppression (NMS) Threshold\n\n\nDescription: Threshold for filtering out overlapping bounding boxes.\n\n\nBackbone Network\n\n\nExamples: ResNet, VGG, MobileNet\n\n\nOptimizer\n\n\nExamples: SGD, Adam",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "Computer_Vision/object_detection.html#popular-loss-functions",
    "href": "Computer_Vision/object_detection.html#popular-loss-functions",
    "title": "Object Detection",
    "section": "Popular Loss Functions",
    "text": "Popular Loss Functions\n\nCross-Entropy Loss\n\n\nDescription: Measures the classification error in object detection tasks.\n\n\nSmooth L1 Loss\n\n\nDescription: Used for bounding box regression, combining L1 and L2 loss.\n\n\nFocal Loss\n\n\nDescription: Addresses class imbalance by focusing on hard examples.\nURL: Focal Loss\n\n\nIoU Loss\n\n\nDescription: Directly optimizes the Intersection over Union metric.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "Computer_Vision/object_detection.html#popular-evaluation-metrics",
    "href": "Computer_Vision/object_detection.html#popular-evaluation-metrics",
    "title": "Object Detection",
    "section": "Popular Evaluation Metrics",
    "text": "Popular Evaluation Metrics\n\nMean Average Precision (mAP)\n\n\nDescription: The average precision across all classes.\n\n\nIntersection over Union (IoU)\n\n\nDescription: Measures the overlap between the predicted bounding box and the ground truth.\n\n\nPrecision-Recall Curve\n\n\nDescription: Plots precision against recall for different threshold values.\n\n\nF1 Score\n\n\nDescription: The harmonic mean of precision and recall.\n\n\nAverage Precision (AP)\n\n\nDescription: The area under the precision-recall curve for a single class.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "Computer_Vision/object_detection.html#other-important-topics",
    "href": "Computer_Vision/object_detection.html#other-important-topics",
    "title": "Object Detection",
    "section": "Other Important Topics",
    "text": "Other Important Topics\n\nData Augmentation\n\n\nDescription: Techniques to increase the diversity of the training dataset without collecting new data.\nExamples: Scaling, Translation, Rotation, Flipping, Adding Noise\n\n\nTransfer Learning\n\n\nDescription: Using a pre-trained model on a new, related task.\nExample: Fine-tuning a model pre-trained on COCO for a custom object detection task.\n\n\nFine-Tuning\n\n\nDescription: Adjusting a pre-trained modelâ€™s parameters on a new dataset.\n\n\nHyperparameter Tuning\n\n\nTechniques: Grid Search, Random Search, Bayesian Optimization\n\n\nModel Interpretability\n\n\nTechniques: Visualization of feature maps, Activation maximization\n\n\nPost-Processing Techniques\n\n\nExamples: Non-Maximum Suppression (NMS), Soft-NMS\n\n\nFrameworks and Libraries\n\n\nExamples: TensorFlow Object Detection API, Detectron2, MMDetection\n\n\nEdge and Real-Time Object Detection\n\n\nDescription: Deploying object detection models on edge devices for real-time applications.\nExamples: TensorFlow Lite, NVIDIA Jetson, OpenVINO",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "Computer_Vision/object_detection.html#references",
    "href": "Computer_Vision/object_detection.html#references",
    "title": "Object Detection",
    "section": "References",
    "text": "References\n\nDeep Learning Book by Ian Goodfellow\nStanford CS231n: Convolutional Neural Networks for Visual Recognition\nPyTorch Documentation\nTensorFlow Documentation",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "pytorch_setup.html#computer-setup",
    "href": "pytorch_setup.html#computer-setup",
    "title": "OLD: Pytorch Setup",
    "section": "Computer Setup",
    "text": "Computer Setup\n\nUninstall\n\nOnedrive\n\nDownload and Install:\n\nMatlab\nGit\nvscode\ndocker\nanaconda\nnvidia:\n\ncuda toolkit",
    "crumbs": [
      "Blog",
      "OLD: Pytorch Setup"
    ]
  },
  {
    "objectID": "pytorch_setup.html#program-setup",
    "href": "pytorch_setup.html#program-setup",
    "title": "OLD: Pytorch Setup",
    "section": "Program Setup",
    "text": "Program Setup\n\nnbdev - see nbdev page\nquarto - see quarto page\njupyter lab - see nbdev page\nOptional installs\n\nvim\n\nconda install -c conda-forge vim\n\ngrep\n\nconda install -c conda-forge grep",
    "crumbs": [
      "Blog",
      "OLD: Pytorch Setup"
    ]
  },
  {
    "objectID": "pytorch_setup.html#prerequisites-for-pytorch",
    "href": "pytorch_setup.html#prerequisites-for-pytorch",
    "title": "OLD: Pytorch Setup",
    "section": "Prerequisites for pytorch",
    "text": "Prerequisites for pytorch\n\nInstall Anaconda\nInstall CUDA, if your machine has a CUDA-enabled GPU.\nIf you want to build on Windows, Visual Studio with MSVC toolset, and NVTX are also needed. The exact requirements of those dependencies could be found out here.\nFollow the steps described here: https://github.com/pytorch/pytorch#from-source\n\nFor installing pytorch\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n\nCode\nimport torch\n\n\n\nx = torch.rand(5,3)\nprint(x)\n\ntensor([[0.9269, 0.1003, 0.0353],\n        [0.4144, 0.8482, 0.3824],\n        [0.1120, 0.7599, 0.8723],\n        [0.0077, 0.7378, 0.8241],\n        [0.3825, 0.3843, 0.2078]])\n\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ntorch.cuda.device_count()\n\n2\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\n!nvcc --version\n\n/bin/bash: line 1: nvcc: command not found\n\n\n\n!nvidia-smi\n\nThu Jul 27 01:53:39 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 520.61.03    Driver Version: 522.06       CUDA Version: 11.8     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n| N/A   42C    P8     4W /  N/A |    150MiB /  6144MiB |      2%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A        23      G   /Xwayland                       N/A      |\n+-----------------------------------------------------------------------------+\n\n\n\n!which python\n\n/home/ben/mambaforge/envs/fast/bin/python\n\n\n\nimport sys\n\n\nprint(sys.executable)\nprint(sys.version)\n\n/home/ben/mambaforge/envs/fast/bin/python3.11\n3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]",
    "crumbs": [
      "Blog",
      "OLD: Pytorch Setup"
    ]
  },
  {
    "objectID": "transfer_learning_resnet18.html",
    "href": "transfer_learning_resnet18.html",
    "title": "Transfer Learning",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torchvision.transforms import ToPILImage\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport copy\nimport time\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ntorch.cuda.empty_cache()\n\n\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n}\n\n\ndata_dir = 'Data/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=0)\n              for x in ['train', 'val']}\n\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(class_names)\n\n['ants', 'bees']\n\n\n\ndef imshow(inp, title):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    plt.title(title)\n    plt.show()\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n\n\n\n\n\n\n\n\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\nimport timm\n\n\nprint(timm.list_models('resnet*')[:10])\n\n['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34']\n\n\n\n# Load ResNet-18 model\nmodel = timm.create_model('resnet18', pretrained=True)\nmodel\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\n#### Finetuning the convnet ####\n# Load a pretrained model and reset final fully connected layer.\n\nmodel = models.resnet18(pretrained=True)\nmodel\n\n/home/ben/mambaforge/envs/pfast/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/ben/mambaforge/envs/pfast/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\nnum_ftrs = model.fc.in_features\nnum_ftrs\n\n512\n\n\n\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nmodel.fc = nn.Linear(model.fc.in_features, 2)\nmodel\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n\n\n\nmodel = model.to(device)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\n# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n# Decay LR by a factor of 0.1 every 7 epochs\n# Learning rate scheduling should be applied after optimizerâ€™s update\n# e.g., you should write your code this way:\n# for epoch in range(100):\n#     train(...)\n#     validate(...)\n#     scheduler.step()\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nmodel = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)\n\nEpoch 0/24\n----------\ntrain Loss: 1.3138 Acc: 0.6270\nval Loss: 2.4487 Acc: 0.4837\n\nEpoch 1/24\n----------\ntrain Loss: 1.1552 Acc: 0.6721\nval Loss: 2.3901 Acc: 0.5033\n\nEpoch 2/24\n----------\ntrain Loss: 0.9656 Acc: 0.7254\nval Loss: 2.0542 Acc: 0.5229\n\nEpoch 3/24\n----------\ntrain Loss: 1.0340 Acc: 0.6762\nval Loss: 1.8399 Acc: 0.5425\n\nEpoch 4/24\n----------\ntrain Loss: 0.8638 Acc: 0.7049\nval Loss: 1.8519 Acc: 0.5686\n\nEpoch 5/24\n----------\ntrain Loss: 0.9689 Acc: 0.6598\nval Loss: 1.9669 Acc: 0.5817\n\nEpoch 6/24\n----------\ntrain Loss: 0.8450 Acc: 0.7008\nval Loss: 1.7546 Acc: 0.6209\n\nEpoch 7/24\n----------\ntrain Loss: 0.9041 Acc: 0.6598\nval Loss: 1.6658 Acc: 0.5948\n\nEpoch 8/24\n----------\ntrain Loss: 0.8533 Acc: 0.6926\nval Loss: 1.6936 Acc: 0.6209\n\nEpoch 9/24\n----------\ntrain Loss: 0.8974 Acc: 0.6393\nval Loss: 1.5477 Acc: 0.6013\n\nEpoch 10/24\n----------\ntrain Loss: 0.9109 Acc: 0.6516\nval Loss: 1.7137 Acc: 0.6078\n\nEpoch 11/24\n----------\ntrain Loss: 0.8369 Acc: 0.6475\nval Loss: 1.7870 Acc: 0.6078\n\nEpoch 12/24\n----------\ntrain Loss: 0.8221 Acc: 0.6844\nval Loss: 1.6008 Acc: 0.6078\n\nEpoch 13/24\n----------\ntrain Loss: 0.7777 Acc: 0.6926\nval Loss: 1.4073 Acc: 0.6340\n\nEpoch 14/24\n----------\ntrain Loss: 0.8776 Acc: 0.6352\nval Loss: 1.6360 Acc: 0.5948\n\nEpoch 15/24\n----------\ntrain Loss: 0.8583 Acc: 0.6639\nval Loss: 1.5304 Acc: 0.6405\n\nEpoch 16/24\n----------\ntrain Loss: 0.7772 Acc: 0.6926\nval Loss: 1.6465 Acc: 0.6275\n\nEpoch 17/24\n----------\ntrain Loss: 0.8548 Acc: 0.6762\nval Loss: 1.7349 Acc: 0.6340\n\nEpoch 18/24\n----------\ntrain Loss: 0.8174 Acc: 0.7008\nval Loss: 1.6733 Acc: 0.6209\n\nEpoch 19/24\n----------\ntrain Loss: 0.7678 Acc: 0.7172\nval Loss: 1.5187 Acc: 0.6209\n\nEpoch 20/24\n----------\ntrain Loss: 0.7592 Acc: 0.7295\nval Loss: 1.6524 Acc: 0.6209\n\nEpoch 21/24\n----------\ntrain Loss: 0.7918 Acc: 0.6926\nval Loss: 1.6008 Acc: 0.6013\n\nEpoch 22/24\n----------\ntrain Loss: 0.8519 Acc: 0.6721\nval Loss: 1.6299 Acc: 0.6013\n\nEpoch 23/24\n----------\ntrain Loss: 0.8987 Acc: 0.6680\nval Loss: 1.7279 Acc: 0.6013\n\nEpoch 24/24\n----------\ntrain Loss: 0.8057 Acc: 0.6680\nval Loss: 1.6766 Acc: 0.6536\n\nTraining complete in 1m 22s\nBest val Acc: 0.653595\n\n\n\n#### ConvNet as fixed feature extractor ####\n# Here, we need to freeze all the network except the final layer.\n# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\nmodel_conv = models.resnet18(pretrained=True)\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\nmodel_conv = model_conv.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\nmodel_conv = train_model(model_conv, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=25)\n\nEpoch 0/24\n----------\ntrain Loss: 0.6815 Acc: 0.6393\nval Loss: 0.2546 Acc: 0.8889\n\nEpoch 1/24\n----------\ntrain Loss: 0.5108 Acc: 0.7541\nval Loss: 0.1813 Acc: 0.9477\n\nEpoch 2/24\n----------\ntrain Loss: 0.4495 Acc: 0.8156\nval Loss: 0.4027 Acc: 0.8235\n\nEpoch 3/24\n----------\ntrain Loss: 0.4571 Acc: 0.7951\nval Loss: 0.2098 Acc: 0.9085\n\nEpoch 4/24\n----------\ntrain Loss: 0.4004 Acc: 0.8320\nval Loss: 0.2219 Acc: 0.9085\n\nEpoch 5/24\n----------\ntrain Loss: 0.4751 Acc: 0.8115\nval Loss: 0.1881 Acc: 0.9477\n\nEpoch 6/24\n----------\ntrain Loss: 0.4426 Acc: 0.8279\nval Loss: 0.2063 Acc: 0.9216\n\nEpoch 7/24\n----------\ntrain Loss: 0.4091 Acc: 0.8238\nval Loss: 0.2144 Acc: 0.9216\n\nEpoch 8/24\n----------\ntrain Loss: 0.3693 Acc: 0.8320\nval Loss: 0.1896 Acc: 0.9216\n\nEpoch 9/24\n----------\ntrain Loss: 0.3681 Acc: 0.8279\nval Loss: 0.1911 Acc: 0.9281\n\nEpoch 10/24\n----------\ntrain Loss: 0.2974 Acc: 0.8689\nval Loss: 0.1962 Acc: 0.9216\n\nEpoch 11/24\n----------\ntrain Loss: 0.3128 Acc: 0.8525\nval Loss: 0.1886 Acc: 0.9346\n\nEpoch 12/24\n----------\ntrain Loss: 0.3634 Acc: 0.8361\nval Loss: 0.1868 Acc: 0.9412\n\nEpoch 13/24\n----------\ntrain Loss: 0.3299 Acc: 0.8484\nval Loss: 0.1979 Acc: 0.9216\n\nEpoch 14/24\n----------\ntrain Loss: 0.3036 Acc: 0.8893\nval Loss: 0.2028 Acc: 0.9216\n\nEpoch 15/24\n----------\ntrain Loss: 0.3533 Acc: 0.8361\nval Loss: 0.1694 Acc: 0.9477\n\nEpoch 16/24\n----------\ntrain Loss: 0.3248 Acc: 0.8525\nval Loss: 0.1838 Acc: 0.9281\n\nEpoch 17/24\n----------\ntrain Loss: 0.3293 Acc: 0.8648\nval Loss: 0.1941 Acc: 0.9216\n\nEpoch 18/24\n----------\ntrain Loss: 0.2718 Acc: 0.8484\nval Loss: 0.1880 Acc: 0.9346\n\nEpoch 19/24\n----------\ntrain Loss: 0.3811 Acc: 0.8074\nval Loss: 0.2232 Acc: 0.9150\n\nEpoch 20/24\n----------\ntrain Loss: 0.3523 Acc: 0.8402\nval Loss: 0.1787 Acc: 0.9346\n\nEpoch 21/24\n----------\ntrain Loss: 0.2430 Acc: 0.8893\nval Loss: 0.2104 Acc: 0.9281\n\nEpoch 22/24\n----------\ntrain Loss: 0.2858 Acc: 0.8730\nval Loss: 0.1836 Acc: 0.9346\n\nEpoch 23/24\n----------\ntrain Loss: 0.3786 Acc: 0.8443\nval Loss: 0.1786 Acc: 0.9412\n\nEpoch 24/24\n----------\ntrain Loss: 0.2935 Acc: 0.8770\nval Loss: 0.1835 Acc: 0.9346\n\nTraining complete in 1m 5s\nBest val Acc: 0.947712\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Transfer Learning"
    ]
  },
  {
    "objectID": "model_training_loop.html",
    "href": "model_training_loop.html",
    "title": "Model Training Loop",
    "section": "",
    "text": "import torch\nx = torch.randn(3, requires_grad = True)\nprint(x)\n\ntensor([-1.2561, -3.5029, -1.4459], requires_grad=True)\ny = x + 2\ny.retain_grad()\nz = y * y*2\nz.retain_grad()\nz = z.mean()\nprint(z)\n\ntensor(2.0794, grad_fn=&lt;MeanBackward0&gt;)\nz.backward(retain_graph=True)\nx,y, z\n\n(tensor([-1.2561, -3.5029, -1.4459], requires_grad=True),\n tensor([ 0.7439, -1.5029,  0.5541], grad_fn=&lt;AddBackward0&gt;),\n tensor(2.0794, grad_fn=&lt;MeanBackward0&gt;))\nx.grad, y.grad\n\n(tensor([ 0.9919, -2.0039,  0.7388]), tensor([ 0.9919, -2.0039,  0.7388]))",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#for-multiple-z-values",
    "href": "model_training_loop.html#for-multiple-z-values",
    "title": "Model Training Loop",
    "section": "For multiple z values",
    "text": "For multiple z values\n\nimport torch\n\n\nx = torch.randn(3, requires_grad = True)\n\n\nprint(x)\n\ntensor([-1.3521, -0.5026, -0.7557], requires_grad=True)\n\n\n\ny = x + 2\ny.retain_grad()\n\n\nz = y * y*2\nz.retain_grad()\n#z = z.mean()\nprint(z)\n\ntensor([0.8396, 4.4845, 3.0964], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nv = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\nz.backward(v, retain_graph=True)\n\n\nx, y, z\n\n(tensor([-1.3521, -0.5026, -0.7557], requires_grad=True),\n tensor([0.6479, 1.4974, 1.2443], grad_fn=&lt;AddBackward0&gt;),\n tensor([0.8396, 4.4845, 3.0964], grad_fn=&lt;MulBackward0&gt;))\n\n\n\nx.grad, y.grad\n\n(tensor([2.5917e-01, 5.9897e+00, 4.9771e-03]),\n tensor([2.5917e-01, 5.9897e+00, 4.9771e-03]))",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#stopping-gradient-descent",
    "href": "model_training_loop.html#stopping-gradient-descent",
    "title": "Model Training Loop",
    "section": "Stopping gradient descent",
    "text": "Stopping gradient descent\n\nx.requires_grad_(False)\nprint(x)\n\ntensor([-1.3521, -0.5026, -0.7557])\n\n\n\ny = x.detach()\nprint(y)\n\ntensor([-1.3521, -0.5026, -0.7557])\n\n\n\nwith torch.no_grad():\n    print(x)\n\ntensor([-1.3521, -0.5026, -0.7557])",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#zeroing-gradients",
    "href": "model_training_loop.html#zeroing-gradients",
    "title": "Model Training Loop",
    "section": "Zeroing Gradients",
    "text": "Zeroing Gradients\n\nweights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(5):\n    model_output = (weights * 3).sum()\n\n    model_output.backward()\n\n    print(weights.grad)\n\ntensor([3., 3., 3., 3.])\ntensor([6., 6., 6., 6.])\ntensor([9., 9., 9., 9.])\ntensor([12., 12., 12., 12.])\ntensor([15., 15., 15., 15.])\n\n\n\nweights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(5):\n    model_output = (weights * 3).sum()\n\n    model_output.backward()\n\n    print(weights.grad)\n\n    weights.grad.zero_()\n\ntensor([3., 3., 3., 3.])\ntensor([3., 3., 3., 3.])\ntensor([3., 3., 3., 3.])\ntensor([3., 3., 3., 3.])\ntensor([3., 3., 3., 3.])",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#backpropagation",
    "href": "model_training_loop.html#backpropagation",
    "title": "Model Training Loop",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nweights = torch.ones(4, requires_grad=True)\n\n\npip list| grep nbdevAuto\n\nnbdevAuto                 0.0.130        /home/ben/BENEDICT_Only/Benedict_Projects/Benedict_ML/nbdevAuto\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom graphviz import Digraph\nfrom nbdevAuto import functions\n\n\ndot = functions.graph()\n# Add nodes with different shapes and formatting\ndot.node('x', 'x')\ndot.node('a', 'a(x)', shape='circle')\ndot.node('y', 'y')\ndot.node('b', 'b(y)', shape='circle')\ndot.node('z', 'z')\n\n# Add edges with custom labels and formatting\ndot.edge('x', 'a')\ndot.edge('a', 'y')\ndot.edge('y', 'b')\ndot.edge('b', 'z')\n\n# Render the graph\ndot\n\n\n\n\n\n\n\n\nChain rule\n\\(\\dfrac{\\delta z}{\\delta x} =  \\dfrac{\\delta z}{\\delta y} \\cdot \\dfrac{\\delta y}{\\delta x}\\)",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#computational-graph",
    "href": "model_training_loop.html#computational-graph",
    "title": "Model Training Loop",
    "section": "Computational Graph",
    "text": "Computational Graph\n\ndot = functions.graph()\n# Add nodes with different shapes and formatting\ndot.node('x', 'x')\ndot.node('*', 'f=x*y', shape='circle')\ndot.node('y', 'y')\ndot.node('z', 'z')\n\n# Add edges with custom labels and formatting\ndot.edge('x', '*')\ndot.edge('y', '*')\ndot.edge('*', 'z')\n\n# Render the graph\ndot\n\n\n\n\n\n\n\n\n\\(\\dfrac{\\delta z}{\\delta x}  = \\dfrac{\\delta xy}{\\delta x} = y\\)\n\\(\\dfrac{\\delta z}{\\delta y}  = \\dfrac{\\delta xy}{\\delta y} = y\\)\n$ = $\n\nForward pass: Computer loss\nCompute local gradients\nBackward pass: Compute dLoss/dWeights using the Chain Rule\n\n\ndot = functions.graph()\n# Add nodes with different shapes and formatting\ndot.node('x', 'x')\ndot.node('w', 'w')\ndot.node('*', '*\\ny1=w*y', shape='circle')\n\ndot.node('y', 'y')\ndot.node('-', '-\\ns= y1-y')\n\ndot.node('^2', '^2\\n(y1-y)^2')\ndot.node('Loss', 'Loss')\n# Add edges with custom labels and formatting\ndot.edge('x', '*')\ndot.edge('w', '*')\n\ndot.edge('*', '-', label='y1')\ndot.edge('y', '-')\n\ndot.edge('-', '^2', label='s')\n\ndot.edge('^2', 'Loss')\n# Render the graph\ndot\n\n\n\n\n\n\n\n\n\\(Loss = (\\hat{y} - y)^2\\)\n\\(\\dfrac{\\delta loss}{\\delta s} = \\dfrac{s^2}{s} = 2s\\)\n\\(\\dfrac{\\delta s}{\\delta \\hat{y}} = \\dfrac{\\delta\\hat{y} - y}{\\delta \\hat{y}} = 1\\)\n\\(\\dfrac{\\delta \\hat{y}}{\\delta w} = \\dfrac{\\delta wx}{\\delta w} = x\\)\n\\(\\therefore \\dfrac{\\delta loss}{\\delta w} = \\dfrac{\\delta loss}{\\delta s} \\cdot \\dfrac{\\delta s}{\\delta y} \\cdot  \\dfrac{\\delta \\hat{y}}{\\delta w} = 2 \\cdot s \\cdot x = 2 \\cdot (-1) \\cdot (1) = -2\\)\n\nx = 1\ny = 2\nw = 1\n\ny1 = x * w \ns = y1-y\nloss = s**2\n\n\nprint(f'x:{x} w:{w} y1:{y1} y:{y} s:{s} loss:{loss}')\n\nx:1 w:1 y1:1 y:2 s:-1 loss:1\n\n\n\nimport torch\nx = torch.tensor(1.0)\ny = torch.tensor(2.0)\n\n\nw = torch.tensor(1.0, requires_grad = True)\n\nlr = 0.005\n\n\n#forward pass and compute the loss\ny1 = w * x\nloss = (y1-y)**2\n\nprint(y1)\nprint(loss)\n\ntensor(1., grad_fn=&lt;MulBackward0&gt;)\ntensor(1., grad_fn=&lt;PowBackward0&gt;)\n\n\n\n# backward pass\n\nloss.backward()\nprint(w.grad)\n\nw.grad.zero_()\n\ntensor(-2.)\n\n\ntensor(0.)",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#gradient-descent",
    "href": "model_training_loop.html#gradient-descent",
    "title": "Model Training Loop",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nNumpy\n\nPrediction: Manually\n\n\nGradients Computation: Manually\n\n\nLoss Computation: Manually\n\n\nParameter updates: Manually\n\n\nimport numpy as np\n\n\nx = np.array([1,2,3,4], dtype=np.float32)\ny = np.array([2,4,6,8], dtype=np.float32)\n\nw = 0.0\n\n\n# model\ndef forward(x):\n    return w * x\n\n\ndef loss(y, y_predicted):\n    return ((y_predicted - y)**2).mean()\n\n\n# gradient\n\n# MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N 2x (w*x - y)\n\ndef gradient (x, y, y_predicted):\n    return np.dot(2 * x, y_predicted-y).mean()\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\nPrediction before training: f(5) = 0.000\n\n\n\nlearning_rate = 0.01\nn_iters = 15\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    dw = gradient(x, y, y_pred)\n\n    # update weights\n    w -= learning_rate * dw\n\n    if epoch % 1 == 0:\n        print(f'epoc:{epoch}  w = {w:.3f} , y_pred={forward(5)}, y = {10}, loss = {l:.8f}, dw = {dw}')\n\nprint(f'Prediction after training: {forward(5):.3f}, y = {10}')\n\nepoc:0  w = 1.200 , y_pred=6.0, y = 10, loss = 30.00000000, dw = -120.0\nepoc:1  w = 1.680 , y_pred=8.399999809265136, y = 10, loss = 4.79999924, dw = -47.999996185302734\nepoc:2  w = 1.872 , y_pred=9.35999994277954, y = 10, loss = 0.76800019, dw = -19.200002670288086\nepoc:3  w = 1.949 , y_pred=9.743999934196472, y = 10, loss = 0.12288000, dw = -7.679999828338623\nepoc:4  w = 1.980 , y_pred=9.897600066661834, y = 10, loss = 0.01966083, dw = -3.072002649307251\nepoc:5  w = 1.992 , y_pred=9.95904014110565, y = 10, loss = 0.00314574, dw = -1.2288014888763428\nepoc:6  w = 1.997 , y_pred=9.983615934848784, y = 10, loss = 0.00050331, dw = -0.4915158748626709\nepoc:7  w = 1.999 , y_pred=9.993446409702301, y = 10, loss = 0.00008053, dw = -0.1966094970703125\nepoc:8  w = 1.999 , y_pred=9.997378492355345, y = 10, loss = 0.00001288, dw = -0.07864165306091309\nepoc:9  w = 2.000 , y_pred=9.998951268196105, y = 10, loss = 0.00000206, dw = -0.03145551681518555\nepoc:10  w = 2.000 , y_pred=9.999580299854276, y = 10, loss = 0.00000033, dw = -0.012580633163452148\nepoc:11  w = 2.000 , y_pred=9.999832069873808, y = 10, loss = 0.00000005, dw = -0.005035400390625\nepoc:12  w = 2.000 , y_pred=9.999932992458342, y = 10, loss = 0.00000001, dw = -0.002018451690673828\nepoc:13  w = 2.000 , y_pred=9.999973046779632, y = 10, loss = 0.00000000, dw = -0.00080108642578125\nepoc:14  w = 2.000 , y_pred=9.999989175796507, y = 10, loss = 0.00000000, dw = -0.00032258033752441406\nPrediction after training: 10.000, y = 10\n\n\n\n\nTorch\n\nPrediction: Manually\n\n\nGradients Computation: Autograd\n\n\nLoss Computation: Manually\n\n\nParameter updates: Manually\n\n\nx = torch.tensor([1,2,3,4], dtype=torch.float32)\ny = torch.tensor([2,4,6,8], dtype=torch.float32)\n\nw = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)\n\n\n# model\ndef forward(x):\n    return w * x\n\n\ndef loss(y, y_predicted):\n    return ((y_predicted - y)**2).mean()\n\n\n# gradient\n\n# MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N 2x (w*x - y)\n\nprint(f'Prediction before training: f(5) = {forward(5)}')\n\nPrediction before training: f(5) = tensor([0.], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nlearning_rate = 0.01\nn_iters = 50\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    l.backward()\n\n    # update weights\n    with torch.no_grad():\n        w -= learning_rate * w.grad\n\n    if epoch % 2 == 0:\n        print(f'epoc:{epoch}  w = {w.item():.3f}, y_pred={forward(5).item():.3f}, y = {10}, loss = {l.item():.7f}, dw = {w.grad.item():.7f}')\n\n    w.grad.zero_()\n\nprint(f'Prediction after training: {forward(5)}, y = {10}')\n\nepoc:0  w = 0.300, y_pred=1.500, y = 10, loss = 30.0000000, dw = -30.0000000\nepoc:2  w = 0.772, y_pred=3.859, y = 10, loss = 15.6601877, dw = -21.6749992\nepoc:4  w = 1.113, y_pred=5.563, y = 10, loss = 8.1747169, dw = -15.6601877\nepoc:6  w = 1.359, y_pred=6.794, y = 10, loss = 4.2672529, dw = -11.3144855\nepoc:8  w = 1.537, y_pred=7.684, y = 10, loss = 2.2275321, dw = -8.1747150\nepoc:10  w = 1.665, y_pred=8.327, y = 10, loss = 1.1627856, dw = -5.9062314\nepoc:12  w = 1.758, y_pred=8.791, y = 10, loss = 0.6069812, dw = -4.2672515\nepoc:14  w = 1.825, y_pred=9.126, y = 10, loss = 0.3168478, dw = -3.0830884\nepoc:16  w = 1.874, y_pred=9.369, y = 10, loss = 0.1653965, dw = -2.2275314\nepoc:18  w = 1.909, y_pred=9.544, y = 10, loss = 0.0863381, dw = -1.6093917\nepoc:20  w = 1.934, y_pred=9.671, y = 10, loss = 0.0450689, dw = -1.1627841\nepoc:22  w = 1.952, y_pred=9.762, y = 10, loss = 0.0235263, dw = -0.8401127\nepoc:24  w = 1.966, y_pred=9.828, y = 10, loss = 0.0122808, dw = -0.6069803\nepoc:26  w = 1.975, y_pred=9.876, y = 10, loss = 0.0064107, dw = -0.4385428\nepoc:28  w = 1.982, y_pred=9.910, y = 10, loss = 0.0033464, dw = -0.3168479\nepoc:30  w = 1.987, y_pred=9.935, y = 10, loss = 0.0017469, dw = -0.2289228\nepoc:32  w = 1.991, y_pred=9.953, y = 10, loss = 0.0009119, dw = -0.1653977\nepoc:34  w = 1.993, y_pred=9.966, y = 10, loss = 0.0004760, dw = -0.1194997\nepoc:36  w = 1.995, y_pred=9.976, y = 10, loss = 0.0002485, dw = -0.0863385\nepoc:38  w = 1.996, y_pred=9.982, y = 10, loss = 0.0001297, dw = -0.0623794\nepoc:40  w = 1.997, y_pred=9.987, y = 10, loss = 0.0000677, dw = -0.0450683\nepoc:42  w = 1.998, y_pred=9.991, y = 10, loss = 0.0000353, dw = -0.0325624\nepoc:44  w = 1.999, y_pred=9.993, y = 10, loss = 0.0000184, dw = -0.0235248\nepoc:46  w = 1.999, y_pred=9.995, y = 10, loss = 0.0000096, dw = -0.0169984\nepoc:48  w = 1.999, y_pred=9.997, y = 10, loss = 0.0000050, dw = -0.0122809\nPrediction after training: tensor([9.9970], grad_fn=&lt;MulBackward0&gt;), y = 10\n\n\n\n\nPytorch Loss and Pytorch Optimizer\n\nPrediction: Manually\n\n\nGradients Computation: Autograd\n\n\nLoss Computation: Pytorch Loss\n\n\nParameter updates: Pytorch Optimizer\n\n\nDesign Model = (input, output, size, forward pass)\nConstruct loss and optimizer\nTraining loop\n\nforward pass: compute prediction\nbackward pass: gradients\nupdate weights\n\n\n\nimport torch\nimport torch.nn as nn\n\nx = torch.tensor([1,2,3,4], dtype=torch.float32)\ny = torch.tensor([2,4,6,8], dtype=torch.float32)\n\nw = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)\n\n\n# model\ndef forward(x):\n    return w * x\n\n\nprint(f'Prediction before training: f(5) = {forward(5)}')\n\nPrediction before training: f(5) = tensor([0.], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nlearning_rate = 0.01\nn_iters = 50\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD([w], lr= learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    l.backward()\n\n    optimizer.step()\n\n\n    if epoch % 2 == 0:\n        print(f'epoc:{epoch}  w = {w.item():.3f}, y_pred={forward(5).item():.3f}, y = {10}, loss = {l.item():.7f}, dw = {w.grad.item():.7f}')\n\n    optimizer.zero_grad()\n\nprint(f'Prediction after training: {forward(5)}, y = {10}')\n\nepoc:0  w = 0.300, y_pred=1.500, y = 10, loss = 30.0000000, dw = -30.0000000\nepoc:2  w = 0.772, y_pred=3.859, y = 10, loss = 15.6601877, dw = -21.6749992\nepoc:4  w = 1.113, y_pred=5.563, y = 10, loss = 8.1747169, dw = -15.6601877\nepoc:6  w = 1.359, y_pred=6.794, y = 10, loss = 4.2672529, dw = -11.3144855\nepoc:8  w = 1.537, y_pred=7.684, y = 10, loss = 2.2275321, dw = -8.1747150\nepoc:10  w = 1.665, y_pred=8.327, y = 10, loss = 1.1627856, dw = -5.9062314\nepoc:12  w = 1.758, y_pred=8.791, y = 10, loss = 0.6069812, dw = -4.2672515\nepoc:14  w = 1.825, y_pred=9.126, y = 10, loss = 0.3168478, dw = -3.0830884\nepoc:16  w = 1.874, y_pred=9.369, y = 10, loss = 0.1653965, dw = -2.2275314\nepoc:18  w = 1.909, y_pred=9.544, y = 10, loss = 0.0863381, dw = -1.6093917\nepoc:20  w = 1.934, y_pred=9.671, y = 10, loss = 0.0450689, dw = -1.1627841\nepoc:22  w = 1.952, y_pred=9.762, y = 10, loss = 0.0235263, dw = -0.8401127\nepoc:24  w = 1.966, y_pred=9.828, y = 10, loss = 0.0122808, dw = -0.6069803\nepoc:26  w = 1.975, y_pred=9.876, y = 10, loss = 0.0064107, dw = -0.4385428\nepoc:28  w = 1.982, y_pred=9.910, y = 10, loss = 0.0033464, dw = -0.3168479\nepoc:30  w = 1.987, y_pred=9.935, y = 10, loss = 0.0017469, dw = -0.2289228\nepoc:32  w = 1.991, y_pred=9.953, y = 10, loss = 0.0009119, dw = -0.1653977\nepoc:34  w = 1.993, y_pred=9.966, y = 10, loss = 0.0004760, dw = -0.1194997\nepoc:36  w = 1.995, y_pred=9.976, y = 10, loss = 0.0002485, dw = -0.0863385\nepoc:38  w = 1.996, y_pred=9.982, y = 10, loss = 0.0001297, dw = -0.0623794\nepoc:40  w = 1.997, y_pred=9.987, y = 10, loss = 0.0000677, dw = -0.0450683\nepoc:42  w = 1.998, y_pred=9.991, y = 10, loss = 0.0000353, dw = -0.0325624\nepoc:44  w = 1.999, y_pred=9.993, y = 10, loss = 0.0000184, dw = -0.0235248\nepoc:46  w = 1.999, y_pred=9.995, y = 10, loss = 0.0000096, dw = -0.0169984\nepoc:48  w = 1.999, y_pred=9.997, y = 10, loss = 0.0000050, dw = -0.0122809\nPrediction after training: tensor([9.9970], grad_fn=&lt;MulBackward0&gt;), y = 10\n\n\n\n\nPytorch Automate\n\nPrediction: Manually\n\n\nGradients Computation: Autograd\n\n\nLoss Computation: Pytorch Loss\n\n\nParameter updates: Pytorch Optimizer\n\n\nimport torch\nimport torch.nn as nn\n\nx = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\ny = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n\nx_test = torch.tensor([5], dtype = torch.float32)\nn_samples, n_features = x.shape\nn_samples, n_features\n\n(4, 1)\n\n\n\nmodel = nn.Linear(in_features = n_features, out_features = 1)\nmodel\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\n[w,b] = model.parameters()\nw[0].item()\n\n-0.8376840353012085\n\n\n\nmodel.state_dict()['weight']\n\ntensor([[-0.8377]])\n\n\n\nprint(f'Prediction before training: f(5) = {model(x_test)}')\n\nPrediction before training: f(5) = tensor([-3.8722], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nlearning_rate = 0.1\nn_iters = 500\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = model(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    l.backward()\n\n    optimizer.step()\n\n\n    if epoch % 20 == 0:\n        [w,b] = model.parameters()\n        print(f'epoc:{epoch}  w = {w[0].item():.3f} {b[0].item():.3f}, y_pred={model(x_test).item():.3f}, y = {10}, loss = {l.item():.7f}, dw = {w.grad.item():.7f}')\n\n    optimizer.zero_grad()\n\nprint(f'Prediction after training: {model(x_test)}, y = {10}')\n\nepoc:0  w = 3.261 1.672, y_pred=17.975, y = 10, loss = 56.0067291, dw = -40.9841690\nepoc:20  w = 1.794 0.607, y_pred=9.578, y = 10, loss = 0.0653165, dw = -0.0772833\nepoc:40  w = 1.888 0.330, y_pred=9.769, y = 10, loss = 0.0193617, dw = -0.0346756\nepoc:60  w = 1.939 0.180, y_pred=9.874, y = 10, loss = 0.0057399, dw = -0.0188781\nepoc:80  w = 1.967 0.098, y_pred=9.931, y = 10, loss = 0.0017016, dw = -0.0102807\nepoc:100  w = 1.982 0.053, y_pred=9.963, y = 10, loss = 0.0005045, dw = -0.0055964\nepoc:120  w = 1.990 0.029, y_pred=9.980, y = 10, loss = 0.0001496, dw = -0.0030484\nepoc:140  w = 1.995 0.016, y_pred=9.989, y = 10, loss = 0.0000443, dw = -0.0016569\nepoc:160  w = 1.997 0.009, y_pred=9.994, y = 10, loss = 0.0000131, dw = -0.0009021\nepoc:180  w = 1.998 0.005, y_pred=9.997, y = 10, loss = 0.0000039, dw = -0.0004910\nepoc:200  w = 1.999 0.003, y_pred=9.998, y = 10, loss = 0.0000012, dw = -0.0002694\nepoc:220  w = 2.000 0.001, y_pred=9.999, y = 10, loss = 0.0000003, dw = -0.0001463\nepoc:240  w = 2.000 0.001, y_pred=9.999, y = 10, loss = 0.0000001, dw = -0.0000764\nepoc:260  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000430\nepoc:280  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000240\nepoc:300  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000120\nepoc:320  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000055\nepoc:340  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000049\nepoc:360  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000038\nepoc:380  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000001\nepoc:400  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000017\nepoc:420  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000015\nepoc:440  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = 0.0000001\nepoc:460  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000001\nepoc:480  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = 0.0000007\nPrediction after training: tensor([10.], grad_fn=&lt;ViewBackward0&gt;), y = 10\n\n\n\n\nPytorch Model\n\nPrediction: Manually\n\n\nGradients Computation: Autograd\n\n\nLoss Computation: Pytorch Loss\n\n\nParameter updates: Pytorch Optimizer\n\n\nDesign Model = (input, output, size, forward pass)\nConstruct loss and optimizer\nTraining loop\n\nforward pass: compute prediction\nbackward pass: gradients\nupdate weights\n\n\n\nimport torch\nimport torch.nn as nn\n\nx = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\ny = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n\nx_test = torch.tensor([5], dtype = torch.float32)\nn_samples, n_features = x.shape\nn_samples, n_features\n\n(4, 1)\n\n\n\nmodel = nn.Linear(in_features = n_features, out_features = 1)\nmodel\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nclass LinearRegression(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(LinearRegression, self).__init__()\n\n        self.lin = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        return self.lin(x)\n\nmodel = LinearRegression(in_features = n_features, out_features = 1)\nmodel\n\nLinearRegression(\n  (lin): Linear(in_features=1, out_features=1, bias=True)\n)\n\n\n\n[w,b] = model.parameters()\nw[0].item()\n\n-0.08443880081176758\n\n\n\nmodel.state_dict()['lin.weight']\n\ntensor([[-0.0844]])\n\n\n\nprint(f'Prediction before training: f(5) = {model(x_test)}')\n\nPrediction before training: f(5) = tensor([-0.1386], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nlearning_rate = 0.1\nn_iters = 500\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = model(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    l.backward()\n\n    optimizer.step()\n\n\n    if epoch % 20 == 0:\n        [w,b] = model.parameters()\n        print(f'epoc:{epoch}  w = {w[0].item():.3f} {b[0].item():.3f}, y_pred={model(x_test).item():.3f}, y = {10}, loss = {l.item():.7f}, dw = {w.grad.item():.7f}')\n\n    optimizer.zero_grad()\n\nprint(f'Prediction after training: {model(x_test)}, y = {10}')\n\nepoc:0  w = 2.900 1.269, y_pred=15.771, y = 10, loss = 29.7110996, dw = -29.8484650\nepoc:20  w = 1.841 0.470, y_pred=9.673, y = 10, loss = 0.0391924, dw = -0.0592351\nepoc:40  w = 1.913 0.256, y_pred=9.821, y = 10, loss = 0.0116179, dw = -0.0268617\nepoc:60  w = 1.953 0.139, y_pred=9.902, y = 10, loss = 0.0034442, dw = -0.0146208\nepoc:80  w = 1.974 0.076, y_pred=9.947, y = 10, loss = 0.0010210, dw = -0.0079615\nepoc:100  w = 1.986 0.041, y_pred=9.971, y = 10, loss = 0.0003027, dw = -0.0043370\nepoc:120  w = 1.992 0.022, y_pred=9.984, y = 10, loss = 0.0000897, dw = -0.0023587\nepoc:140  w = 1.996 0.012, y_pred=9.991, y = 10, loss = 0.0000266, dw = -0.0012866\nepoc:160  w = 1.998 0.007, y_pred=9.995, y = 10, loss = 0.0000079, dw = -0.0007011\nepoc:180  w = 1.999 0.004, y_pred=9.997, y = 10, loss = 0.0000023, dw = -0.0003816\nepoc:200  w = 1.999 0.002, y_pred=9.999, y = 10, loss = 0.0000007, dw = -0.0002074\nepoc:220  w = 2.000 0.001, y_pred=9.999, y = 10, loss = 0.0000002, dw = -0.0001137\nepoc:240  w = 2.000 0.001, y_pred=10.000, y = 10, loss = 0.0000001, dw = -0.0000589\nepoc:260  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000327\nepoc:280  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000174\nepoc:300  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000103\nepoc:320  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000072\nepoc:340  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000017\nepoc:360  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000023\nepoc:380  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000023\nepoc:400  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000010\nepoc:420  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000023\nepoc:440  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = 0.0000000\nepoc:460  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000007\nepoc:480  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000015\nPrediction after training: tensor([10.0000], grad_fn=&lt;ViewBackward0&gt;), y = 10",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#linear-regression",
    "href": "model_training_loop.html#linear-regression",
    "title": "Model Training Loop",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\n\nPrepare data\nmodel\nloss and optimizer\ntraining loop\n\n\nx_numpy, y_numpy = datasets.make_regression(n_samples= 100, n_features=1, noise = 20, random_state = 1)\n\n\nx = torch.from_numpy(x_numpy.astype(np.float32))\ny = torch.from_numpy(y_numpy.astype(np.float32))\nx[:5], y[:5]\n\n(tensor([[-0.6118],\n         [-0.2494],\n         [ 0.4885],\n         [ 0.7620],\n         [ 1.5198]]),\n tensor([-55.5386, -10.6620,  22.7574, 101.0961, 144.3376]))\n\n\n\nx_test = x[4]\ny_test = y[4]\nx_test, y_test\n\n(tensor([1.5198]), tensor(144.3376))\n\n\n\ny =y.view(y.shape[0], 1)\nx[:5], y[:5]\n\n(tensor([[-0.6118],\n         [-0.2494],\n         [ 0.4885],\n         [ 0.7620],\n         [ 1.5198]]),\n tensor([[-55.5386],\n         [-10.6620],\n         [ 22.7574],\n         [101.0961],\n         [144.3376]]))\n\n\n\nn_samples, n_features = x.shape\nn_samples, n_features\n\n(100, 1)\n\n\n\n#1.model\ninput_size = n_features\noutput_size = 1\nmodel = nn.Linear(input_size, output_size)\nmodel\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\n[a,b] = model.parameters()\na,b\n\n(Parameter containing:\n tensor([[-0.3357]], requires_grad=True),\n Parameter containing:\n tensor([0.3514], requires_grad=True))\n\n\n\n#2. loss and optimizer\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n\n\noptimizer\n\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.01\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n\n\n\n#3. training loop\nnum_epochs = 1000\n\nfor epoch in range(num_epochs):\n    #forward pass and loss\n    y_predicted = model(x)\n    loss=criterion(y_predicted, y)\n\n    # backward pass\n    loss.backward()\n\n    #update\n    optimizer.step()\n\n    if (epoch + 1) % 50 == 0:\n        [w,b] = model.parameters()\n        print(f'[epoc:{epoch}] (y = {w[0].item():.3f}x + {b[0].item():.3f}) y_pred:{model(x_test).item():.3f}, y:{y_test}, loss :{loss.item():.7f}, dw:{w.grad.item():.7f} db:{b.grad.item():.7f}')\n\n    optimizer.zero_grad()\n\n[epoc:49] (y = 45.054x + 4.782) y_pred:73.255, y:144.33755493164062, loss :1468.2329102, dw:-59.7857857 db:-3.2169607\n[epoc:99] (y = 65.507x + 5.259) y_pred:104.818, y:144.33755493164062, loss :565.6588745, dw:-27.0066071 db:0.3266662\n[epoc:149] (y = 74.762x + 4.920) y_pred:118.545, y:144.33755493164062, loss :380.9410095, dw:-12.2441845 db:0.7970295\n[epoc:199] (y = 78.964x + 4.564) y_pred:124.575, y:144.33755493164062, loss :342.6767883, dw:-5.5675182 db:0.5981486\n[epoc:249] (y = 80.877x + 4.328) y_pred:127.246, y:144.33755493164062, loss :334.6894531, dw:-2.5375218 db:0.3580039\n[epoc:299] (y = 81.749x + 4.195) y_pred:128.438, y:144.33755493164062, loss :333.0141296, dw:-1.1586771 db:0.1943260\n[epoc:349] (y = 82.148x + 4.124) y_pred:128.973, y:144.33755493164062, loss :332.6617126, dw:-0.5298302 db:0.0999891\n[epoc:399] (y = 82.330x + 4.088) y_pred:129.215, y:144.33755493164062, loss :332.5874329, dw:-0.2425606 db:0.0497854\n[epoc:449] (y = 82.414x + 4.070) y_pred:129.324, y:144.33755493164062, loss :332.5717468, dw:-0.1111394 db:0.0242567\n[epoc:499] (y = 82.452x + 4.062) y_pred:129.374, y:144.33755493164062, loss :332.5684509, dw:-0.0509445 db:0.0116392\n[epoc:549] (y = 82.470x + 4.058) y_pred:129.396, y:144.33755493164062, loss :332.5677490, dw:-0.0233506 db:0.0055273\n[epoc:599] (y = 82.478x + 4.056) y_pred:129.407, y:144.33755493164062, loss :332.5675659, dw:-0.0107345 db:0.0026025\n[epoc:649] (y = 82.481x + 4.055) y_pred:129.411, y:144.33755493164062, loss :332.5675659, dw:-0.0049275 db:0.0012180\n[epoc:699] (y = 82.483x + 4.054) y_pred:129.414, y:144.33755493164062, loss :332.5675659, dw:-0.0022745 db:0.0005686\n[epoc:749] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0010500 db:0.0002624\n[epoc:799] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0004619 db:0.0001247\n[epoc:849] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0003704 db:0.0000531\n[epoc:899] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0003721 db:0.0000235\n[epoc:949] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0003721 db:0.0000235\n[epoc:999] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0003721 db:0.0000235\n\n\n\npredicted = model(x).detach().numpy()\n\n\nplt.plot(x_numpy, y_numpy, 'ro')\nplt.plot(x_numpy, predicted, 'b')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom bokeh.io import output_notebook\n\n\noutput_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nfrom bokeh.plotting import figure, show\n\nfrom bokeh.io import curdoc\n# apply theme to current document\ncurdoc().theme = \"dark_minimal\"\n\n\n# create a new plot with a title and axis labels\np = figure(title=\"Real data vs Model\",\n           x_axis_label='x',\n           y_axis_label='y',\n           sizing_mode=\"stretch_width\",\n           max_width=1000,\n           height=500,)\n\n\n# add a line renderer with legend and line thickness to the plot\np.circle(x_numpy.flatten(), y_numpy.flatten(), legend_label=\"Original\", line_width=2, color=\"red\", radius=0.02)\np.line(x_numpy.flatten(), predicted.flatten(), legend_label=\"Predicted\", line_width=2)\n\np.legend.location = \"top_left\"\np.legend.click_policy=\"mute\"\n\n# show the results\nshow(p)",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#logistic-regression",
    "href": "model_training_loop.html#logistic-regression",
    "title": "Model Training Loop",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n\nbc = datasets.load_breast_cancer()\nbc.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n\n\n\nx, y = bc.data, bc.target\nn_samples, n_features = x.shape\nn_samples, n_features\n\n(569, 30)\n\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1234)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape\n\n((455, 30), (114, 30), (455,), (114,))\n\n\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\nx_train = torch.from_numpy(x_train.astype(np.float32))\nx_test = torch.from_numpy(x_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\n\n\ny_train = y_train.view(y_train.shape[0], 1)\n\n\n#1. model\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, n_input):\n        super(LogisticRegression, self).__init__()\n\n        self.linear = nn.Linear(n_input, 1)\n\n    def forward(self, x):\n        y_pred = torch.sigmoid(self.linear(x))\n\n        return y_pred\n\nmodel = LogisticRegression(n_features)\n\n\n#2. loss and optimizer\nlearning_rate = 0.01\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\n\n#3. train loop\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    #forward pass and loss\n    y_predicted = model(x_train)\n    loss=criterion(y_predicted, y_train)\n\n    # backward pass\n    loss.backward()\n\n    #update\n    optimizer.step()\n\n    if (epoch + 1) % 50 == 0:\n        with torch.no_grad():\n            [w,b] = model.parameters()\n            y_predicted = model(x_test)\n            y_predicted_cls = y_predicted.round().flatten()\n            acc = (y_predicted_cls == y_test).float().mean() * 100\n            error = (100 - acc)\n            print(f'[epoc:{epoch + 1}] (y = {w.mean().item():.3f}x + {b.mean().item():.3f}) \\\n            loss:{loss.item():.5f}, accuracy: {acc:.2f}%, error: {error:.2f}%, \\\n            dw:{w.grad.mean().item():.5f} db:{b.grad.mean().item():.5f}')\n\n    optimizer.zero_grad()\n\n[epoc:50] (y = -0.171x + 0.301)             loss:0.11539, accuracy: 93.86%, error: 6.14%,             dw:0.01317 db:-0.03279\n[epoc:100] (y = -0.222x + 0.488)             loss:0.07955, accuracy: 94.74%, error: 5.26%,             dw:0.00604 db:-0.01464\n[epoc:150] (y = -0.257x + 0.602)             loss:0.06487, accuracy: 95.61%, error: 4.39%,             dw:0.00404 db:-0.00820\n[epoc:200] (y = -0.286x + 0.681)             loss:0.05622, accuracy: 96.49%, error: 3.51%,             dw:0.00301 db:-0.00513\n[epoc:250] (y = -0.311x + 0.739)             loss:0.05038, accuracy: 95.61%, error: 4.39%,             dw:0.00238 db:-0.00343\n[epoc:300] (y = -0.334x + 0.783)             loss:0.04613, accuracy: 95.61%, error: 4.39%,             dw:0.00197 db:-0.00240\n[epoc:350] (y = -0.355x + 0.817)             loss:0.04288, accuracy: 95.61%, error: 4.39%,             dw:0.00167 db:-0.00173\n[epoc:400] (y = -0.375x + 0.844)             loss:0.04031, accuracy: 95.61%, error: 4.39%,             dw:0.00144 db:-0.00128\n[epoc:450] (y = -0.394x + 0.866)             loss:0.03821, accuracy: 95.61%, error: 4.39%,             dw:0.00126 db:-0.00096\n[epoc:500] (y = -0.412x + 0.883)             loss:0.03648, accuracy: 95.61%, error: 4.39%,             dw:0.00112 db:-0.00073\n[epoc:550] (y = -0.429x + 0.898)             loss:0.03501, accuracy: 95.61%, error: 4.39%,             dw:0.00101 db:-0.00056\n[epoc:600] (y = -0.445x + 0.909)             loss:0.03374, accuracy: 95.61%, error: 4.39%,             dw:0.00091 db:-0.00042\n[epoc:650] (y = -0.460x + 0.918)             loss:0.03264, accuracy: 95.61%, error: 4.39%,             dw:0.00083 db:-0.00032\n[epoc:700] (y = -0.475x + 0.925)             loss:0.03167, accuracy: 95.61%, error: 4.39%,             dw:0.00076 db:-0.00024\n[epoc:750] (y = -0.489x + 0.931)             loss:0.03080, accuracy: 95.61%, error: 4.39%,             dw:0.00070 db:-0.00017\n[epoc:800] (y = -0.503x + 0.935)             loss:0.03003, accuracy: 95.61%, error: 4.39%,             dw:0.00065 db:-0.00012\n[epoc:850] (y = -0.516x + 0.938)             loss:0.02932, accuracy: 95.61%, error: 4.39%,             dw:0.00060 db:-0.00008\n[epoc:900] (y = -0.529x + 0.940)             loss:0.02868, accuracy: 95.61%, error: 4.39%,             dw:0.00056 db:-0.00004\n[epoc:950] (y = -0.542x + 0.941)             loss:0.02809, accuracy: 95.61%, error: 4.39%,             dw:0.00052 db:-0.00002\n[epoc:1000] (y = -0.554x + 0.942)             loss:0.02754, accuracy: 95.61%, error: 4.39%,             dw:0.00049 db:0.00001\n\n\n\nwith torch.no_grad():\n    [w,b] = model.parameters()\n    y_predicted = model(x_test)\n    y_predicted_cls = y_predicted.round().flatten()\n    acc = (y_predicted_cls == y_test).float().mean() * 100\n    error = (100 - acc)\n    print(f'[epoc:{epoch + 1}] (y = {w.mean().item():.3f}x + {b.mean().item():.3f}) \\\n    loss:{loss.item():.5f}, accuracy: {acc:.2f}%, error: {error:.2f}%')\n\n[epoc:1000] (y = -0.554x + 0.942)     loss:0.02754, accuracy: 95.61%, error: 4.39%",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#softmax-and-cross-entropy",
    "href": "model_training_loop.html#softmax-and-cross-entropy",
    "title": "Model Training Loop",
    "section": "Softmax and Cross-Entropy",
    "text": "Softmax and Cross-Entropy\n\nSoftmax\n\n\\(S(y_i) = \\frac{e^{y_i}}{\\sum e^{y_i}}\\)\n\n\\(Linear = [2.0, 1.0, 0.1]\\)\n\\(Softmax = [0.7, 0.2, 0.1]\\)\nAdds to 1\n\ndef softmax(x):\n    return np.exp(x) / np.sum(np.exp(x), axis = 0)\n\nx = np.array([2.0, 1.0, 0.1])\noutputs = softmax(x)\noutputs\n\narray([0.65900114, 0.24243297, 0.09856589])\n\n\n\nx = torch.from_numpy(x)\nx\n\ntensor([2.0000, 1.0000, 0.1000], dtype=torch.float64)\n\n\n\noutputs = torch.softmax(x, dim = 0)\noutputs\n\ntensor([0.6590, 0.2424, 0.0986], dtype=torch.float64)",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#cross-entropy",
    "href": "model_training_loop.html#cross-entropy",
    "title": "Model Training Loop",
    "section": "Cross Entropy",
    "text": "Cross Entropy\n\n\\(D(\\hat{Y}, Y) = \\dfrac{1}{N} \\cdot \\displaystyle\\sum_{i=1}^{N} Y_i \\cdot \\log{\\hat{Y_i}}\\)\n\n\\(Y = [1, 0, 0]\\)\n\\(\\hat{Y} = [0.7, 0.2, 0.1]  --&gt; D(\\hat{Y}, Y) = 0.35\\)\n\\(Y = [1, 0, 0]\\)\n\\(\\hat{Y} = [0.7, 0.2, 0.1]  --&gt; D(\\hat{Y}, Y) = 2.30\\)\n\ndef cross_entropy(actual, predicted):\n    loss = -np.sum(actual * np.log(predicted))\n    return loss\n\n\nY_actual = np.array([1,0,0])\n\nY_pred_good = np.array([0.7, 0.2, 0.1])\nY_pred_bad = np.array([0.1, 0.3, 0.6])\n\nl1 = cross_entropy(Y_actual, Y_pred_good)\nl2 = cross_entropy(Y_actual, Y_pred_bad)\n\n\nprint(f'good pred:{l1:4f}, bad pred:{l2:.4f}')\n\ngood pred:0.356675, bad pred:2.3026\n\n\n\nnn.CrossEntropyLoss()\n\napplies nn.LogSoftmax + nn.NLLLoss(negative log likelihood loss)\n\n\ny has class labels, not One-Hot!\n\n\nY_pred has raw scores(logits), no softmax\n\n\nloss = nn.CrossEntropyLoss()\n\n\nY = torch.tensor([0])\n\nY_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\nY_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n\nl1 = loss(Y_pred_good, Y)\nl2 = loss(Y_pred_bad, Y)\n\nprint(f'good pred:{l1:4f}, bad pred:{l2:.4f}')\n\ngood pred:0.417030, bad pred:1.8406\n\n\n\n_, predictions1 = torch.max(Y_pred_good, 1)\n_, predictions2 = torch.max(Y_pred_bad, 1)\n\nprint(f'good pred:{predictions1}, bad pred:{predictions2}')\n\ngood pred:tensor([0]), bad pred:tensor([1])\n\n\n\n#Multiclass Problem\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.rely = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        \n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n\n        return out\n\n\nmodel = NeuralNet2(input_size = 28 * 28, hidden_size = 5, num_classes = 3)\ncrioterion = nn.CrossEntropyLoss()",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#activation-functions",
    "href": "model_training_loop.html#activation-functions",
    "title": "Model Training Loop",
    "section": "Activation Functions",
    "text": "Activation Functions\n\nWithout activation functions, our network is basically just a stacked linear regression model\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n#Option 1 (create nn modules)\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n\n        return out\n\n\n#Option 2 (use activation functions directly in forward pass)\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        \n\n    def forward(self, x):\n        out = torch.relu(self.linear1(x))\n        out = torch.sigmoid(self.linear2(out))\n\n        return out",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#mlp-on-mnist",
    "href": "model_training_loop.html#mlp-on-mnist",
    "title": "Model Training Loop",
    "section": "MLP on MNIST",
    "text": "MLP on MNIST\n\nMNIST\nDataLoader, Transformation\nMultilayer Neural Net, activation function\nLoss and Optimizer\nTraining Loop (batch training)\nModel evaluation\nGPU Support\n\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ninput_size = 28 * 28\nhidden_size = 100\nnum_classes = 10\nnum_epochs = 2\nbatch_size = 100\n\n\ntrain_dataset = torchvision.datasets.MNIST(root=\"./data\",\n                                           download=True,\n                                           train=True,\n                                           transform=transforms.ToTensor())\n\ntest_dataset = torchvision.datasets.MNIST(root=\"./data\",\n                                          download=True,\n                                          train=False,\n                                          transform=transforms.ToTensor())\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = batch_size,\n                                           shuffle = True)\n\ntest_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                          batch_size = batch_size,\n                                          shuffle = False)\n\n\nlen(train_dataset), len(test_dataset)\n\n(60000, 10000)\n\n\n\nimage, label = train_dataset[1]\nplt.imshow(transforms.ToPILImage()(image), cmap='gray')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nimages.shape, labels.shape\n\n(torch.Size([100, 1, 28, 28]), torch.Size([100]))\n\n\n\nfor i in range(6):\n    plt.subplot(2, 3, i + 1)\n    plt.imshow(images[i][0], cmap = 'gray')\n    plt.axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# model\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        \n        return out\n\nmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n\n# loss and optimizer\nlearning_rate = 0.001\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\n\n# Train loop\nn_total_steps = len(train_loader)\nrunning_loss = 0.0\nprint_stat = 100\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # 100, 1, 28, 28 --&gt; 100 , 28 * 28\n        images = images.reshape(-1, 28 * 28).to(device)\n        labels = labels.to(device)\n\n        # forward\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        #backwards\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if (i + 1) % print_stat == 0:\n            with torch.no_grad():\n                n_correct = 0\n                n_samples = 0\n                for images, labels in test_loader:\n                    images = images.reshape(-1, 28 * 28).to(device)\n                    labels = labels.to(device)\n                    outputs = model(images)\n\n                    _, predictions = torch.max(outputs, 1)\n                    n_samples += labels.shape[0]\n                    n_correct += (predictions == labels).sum().item()\n            \n                acc = 100.0 * n_correct / n_samples\n            \n            print(f'[epoch:{epoch+1}/{num_epochs}, [step:{i+1}/{n_total_steps}] loss:{(running_loss/print_stat):.4f} accuracy:{acc}')\n            running_loss = 0.0\n\n[epoch:1/2, [step:100/600] loss:0.9543 accuracy:88.51333333333334\n[epoch:1/2, [step:200/600] loss:0.3956 accuracy:90.49166666666666\n[epoch:1/2, [step:300/600] loss:0.3070 accuracy:91.86666666666666\n[epoch:1/2, [step:400/600] loss:0.2970 accuracy:92.62\n[epoch:1/2, [step:500/600] loss:0.2614 accuracy:93.28833333333333\n[epoch:1/2, [step:600/600] loss:0.2401 accuracy:93.665\n[epoch:2/2, [step:100/600] loss:0.2229 accuracy:94.08166666666666\n[epoch:2/2, [step:200/600] loss:0.2201 accuracy:94.43333333333334\n[epoch:2/2, [step:300/600] loss:0.1986 accuracy:94.75833333333334\n[epoch:2/2, [step:400/600] loss:0.1918 accuracy:94.61666666666666\n[epoch:2/2, [step:500/600] loss:0.1879 accuracy:95.115\n[epoch:2/2, [step:600/600] loss:0.1609 accuracy:95.52166666666666\n\n\n\n# Test\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for images, labels in test_loader:\n        images = images.reshape(-1, 28 * 28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n\n        #value, index\n        _, predictions = torch.max(outputs, 1)\n        n_samples += labels.shape[0]\n        n_correct += (predictions == labels).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\n    print(f'accuracy = {acc}')\n\naccuracy = 95.52166666666666",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#cnn-on-cifar-10",
    "href": "model_training_loop.html#cnn-on-cifar-10",
    "title": "Model Training Loop",
    "section": "CNN on Cifar-10",
    "text": "CNN on Cifar-10\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nnum_epochs = 4\nbatch_size = 100\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n\n\ntrain_dataset = torchvision.datasets.CIFAR10(root=\"./data\", \n                                             download=True,\n                                             train=True,\n                                             transform=transform)\n\ntest_dataset = torchvision.datasets.CIFAR10(root=\"./data\",\n                                            download=True,\n                                            train=False,\n                                            transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = batch_size,\n                                           shuffle = True)\n\ntest_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                          batch_size = batch_size,\n                                          shuffle = False)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\nlen(train_dataset), len(test_dataset)\n\n(50000, 10000)\n\n\n\nclasses = train_dataset.class_to_idx\n\n\nclasses = list(train_dataset.class_to_idx)\n\n\nlist(classes)\n\n['airplane',\n 'automobile',\n 'bird',\n 'cat',\n 'deer',\n 'dog',\n 'frog',\n 'horse',\n 'ship',\n 'truck']\n\n\n\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nimages.shape, labels.shape\n\n(torch.Size([100, 3, 32, 32]), torch.Size([100]))\n\n\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        # input size: 3 colour channels\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16*5*5, 120)\n        self.fc2 = nn.Linear(120, 40)\n        self.fc3 = nn.Linear(40, 10)\n        \n\n    def forward(self, x):\n        out = self.pool(F.relu(self.conv1(x)))\n        out = self.pool(F.relu(self.conv2(out)))\n        out = out.view(-1, 16*5*5)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n\n        return out\n        \n\nmodel = ConvNet().to(device)\n\n\nlearning_rate = 0.001\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n\n\n# Image Classifier Neural Network\nclass ImageClassifier(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(64, 64, (3,3)), \n            nn.ReLU(),\n            nn.Flatten(), \n            nn.Linear(64*(28-2)*(28-2), 10)  \n        )\n\n    def forward(self, x): \n        return self.model(x)\n\nmodel = ImageClassifier().to(device)\n\n\n# Train loop\nn_total_steps = len(train_loader)\nrunning_loss = 0.0\nprint_stat = 100\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # 100, 1, 28, 28 --&gt; 100 , 28 * 28\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # forward\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        #backwards\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if (i + 1) % print_stat == 0:\n            with torch.no_grad():\n                n_correct = 0\n                n_samples = 0\n                for images, labels in test_loader:\n                    images = images.to(device)\n                    labels = labels.to(device)\n                    outputs = model(images)\n\n                    _, predictions = torch.max(outputs, 1)\n                    n_samples += labels.shape[0]\n                    n_correct += (predictions == labels).sum().item()\n\n                acc = 100.0 * n_correct / n_samples\n            \n            print(f'[epoch:{epoch+1}/{num_epochs}, [step:{i+1}/{n_total_steps}] loss:{(running_loss/print_stat):.4f} accuracy:{acc}')\n            running_loss = 0.0\n\n[epoch:1/4, [step:100/500] loss:2.3045 accuracy:6.866\n[epoch:1/4, [step:200/500] loss:2.3044 accuracy:6.866\n[epoch:1/4, [step:300/500] loss:2.3047 accuracy:6.866\n[epoch:1/4, [step:400/500] loss:2.3047 accuracy:6.866\n[epoch:1/4, [step:500/500] loss:2.3048 accuracy:6.866\n[epoch:2/4, [step:100/500] loss:2.3046 accuracy:6.866\n[epoch:2/4, [step:200/500] loss:2.3045 accuracy:6.866\n[epoch:2/4, [step:300/500] loss:2.3046 accuracy:6.866\n[epoch:2/4, [step:400/500] loss:2.3046 accuracy:6.866\n[epoch:2/4, [step:500/500] loss:2.3048 accuracy:6.866\n[epoch:3/4, [step:100/500] loss:2.3044 accuracy:6.866\n[epoch:3/4, [step:200/500] loss:2.3045 accuracy:6.866\n[epoch:3/4, [step:300/500] loss:2.3047 accuracy:6.866\n[epoch:3/4, [step:400/500] loss:2.3051 accuracy:6.866\n[epoch:3/4, [step:500/500] loss:2.3044 accuracy:6.866\n[epoch:4/4, [step:100/500] loss:2.3046 accuracy:6.866\n[epoch:4/4, [step:200/500] loss:2.3046 accuracy:6.866\n[epoch:4/4, [step:300/500] loss:2.3048 accuracy:6.866\n[epoch:4/4, [step:400/500] loss:2.3048 accuracy:6.866\n[epoch:4/4, [step:500/500] loss:2.3044 accuracy:6.866",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "fastaiexample.html",
    "href": "fastaiexample.html",
    "title": "FastAI Example",
    "section": "",
    "text": "pip list | grep \"torch\"\n\ntorch                     2.5.1\ntorchvision               0.20.1\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nnvcc --version\n\n\npip list | grep \"fastai\"\n\nfastai                    2.7.18\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip list | grep \"fastbook\"\n\nfastbook                  0.0.29\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "FastAI Example"
    ]
  },
  {
    "objectID": "fastaiexample.html#check-for-all-installs-and-versions",
    "href": "fastaiexample.html#check-for-all-installs-and-versions",
    "title": "FastAI Example",
    "section": "",
    "text": "pip list | grep \"torch\"\n\ntorch                     2.5.1\ntorchvision               0.20.1\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nnvcc --version\n\n\npip list | grep \"fastai\"\n\nfastai                    2.7.18\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip list | grep \"fastbook\"\n\nfastbook                  0.0.29\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "FastAI Example"
    ]
  },
  {
    "objectID": "fastaiexample.html#generate-data-images",
    "href": "fastaiexample.html#generate-data-images",
    "title": "FastAI Example",
    "section": "Generate Data Images",
    "text": "Generate Data Images\n\nfrom fastbook import search_images_ddg\n\n\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\n\ndownload_url(search_images_ddg('elephant', max_images=2)[0], 'Data/elephant.jpg', show_progress=False)\n\nim = Image.open('Data/elephant.jpg')\nim.to_thumb(256,256)\n\n\n\n\n\n\n\n\n\ndownload_url(search_images_ddg('tiger', max_images=1)[0], 'Data/tiger.jpg', show_progress=False)\nImage.open('Data/tiger.jpg').to_thumb(256,256)\n\n\n\n\n\n\n\n\n\ndownload_url(search_images_ddg('panda', max_images=2)[0], 'Data/panda.jpg', show_progress=False)\nImage.open('Data/panda.jpg').to_thumb(256,256)\n\n\n\n\n\n\n\n\n\nimport os\nfrom nbdevAuto import functions\n\n\npip list | grep nbdevAuto\n\nnbdevAuto                 0.3.26\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npath = Path('Data/animal_pics') #Replace with folder path\n\nanimals = ('chimpanzee', 'elephant', 'giraffe',\n                    'kangaroo','lion','panda','rabbit',\n                    'rhino', 'tiger', 'wolf')   \n\nfunctions.create_data_folder(\n    path,\n    animals,\n)\n\nFolder already exists: Data/animal_pics",
    "crumbs": [
      "Blog",
      "FastAI Example"
    ]
  },
  {
    "objectID": "fastaiexample.html#step-2-train-our-model",
    "href": "fastaiexample.html#step-2-train-our-model",
    "title": "FastAI Example",
    "section": "Step 2: Train our model",
    "text": "Step 2: Train our model\n\nfrom fastai.metrics import accuracy, error_rate\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\n\n# Define your metrics\nmetrics = [accuracy, error_rate]\n\n\nlearn11 = vision_learner(dls, resnet18, metrics=metrics)\nlearn11.fine_tune(5)\n\n[W101 22:30:50.769928907 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n1.879884\n0.455274\n0.843575\n0.156425\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nerror_rate\ntime\n\n\n\n\n0\n0.552267\n0.382098\n0.860335\n0.139665\n00:03\n\n\n1\n0.376326\n0.326573\n0.891061\n0.108939\n00:02\n\n\n2\n0.242192\n0.306850\n0.905028\n0.094972\n00:02\n\n\n3\n0.160230\n0.317596\n0.905028\n0.094972\n00:02\n\n\n4\n0.108631\n0.317832\n0.913408\n0.086592\n00:02",
    "crumbs": [
      "Blog",
      "FastAI Example"
    ]
  },
  {
    "objectID": "fastaiexample.html#step-3-display-results-confusion-matrix",
    "href": "fastaiexample.html#step-3-display-results-confusion-matrix",
    "title": "FastAI Example",
    "section": "Step 3: Display Results: Confusion Matrix",
    "text": "Step 3: Display Results: Confusion Matrix\n\ninterp11 = ClassificationInterpretation.from_learner(learn11)\ninterp11.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp11.plot_top_losses(5,nrows = 2, figsize=(17,4))",
    "crumbs": [
      "Blog",
      "FastAI Example"
    ]
  },
  {
    "objectID": "fastaiexample.html#step-4-display-results-t-sne",
    "href": "fastaiexample.html#step-4-display-results-t-sne",
    "title": "FastAI Example",
    "section": "Step 4: Display Results: t-SNE",
    "text": "Step 4: Display Results: t-SNE\n\nfrom fastai.vision.all import *\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n\nfeatures, labels = learn11.get_preds(dl=dls.valid)\n\n# Replace 'val_features' with your extracted features\ntsne = TSNE(n_components=2)\nreduced_features = tsne.fit_transform(features)\n\n# Replace 'labels' with your image labels if available\nplt.scatter(reduced_features[:, 0], reduced_features[:, 1])\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.title('t-SNE Visualization')\nplt.show()",
    "crumbs": [
      "Blog",
      "FastAI Example"
    ]
  },
  {
    "objectID": "fastaiexample.html#step-5-use-our-model",
    "href": "fastaiexample.html#step-5-use-our-model",
    "title": "FastAI Example",
    "section": "Step 5: Use our model",
    "text": "Step 5: Use our model\n\nanimal_list =  ('chimpanzee','elephant','giraffe',\n                'kangaroo', 'lion','panda', 'rabbit',\n                'rhino','tiger','wolf')\nimage= 'Data/panda.jpg'\n\n\nis_real,_,probs = learn11.predict(PILImage.create(image))\nmax_value, max_index = torch.max(probs, dim=0)\nprint(f\"This is a: {animal_list[max_index]} with probability: {max_value}.\")\n\nImage.open(image).to_thumb(256,256)\n\n\n\n\n\n\n\n\nThis is a: panda with probability: 0.9999746084213257.\n\n\n\n\n\n\n\n\n\n\nanimal_list =  ('chimpanzee','elephant','giraffe',\n                'kangaroo', 'lion','panda', 'rabbit',\n                'rhino','tiger','wolf')\nfor index, value in enumerate(animal_list):\n    np.set_printoptions(suppress=True, precision=4)\n    print(f\"Probability of {value} is :{probs[index]:4f}.\")\n\nProbability of chimpanzee is :0.000001.\nProbability of elephant is :0.000000.\nProbability of giraffe is :0.000000.\nProbability of kangaroo is :0.000000.\nProbability of lion is :0.000000.\nProbability of panda is :0.999975.\nProbability of rabbit is :0.000024.\nProbability of rhino is :0.000000.\nProbability of tiger is :0.000000.\nProbability of wolf is :0.000000.",
    "crumbs": [
      "Blog",
      "FastAI Example"
    ]
  },
  {
    "objectID": "evaluation_metrics.html",
    "href": "evaluation_metrics.html",
    "title": "Evaluation Metrics",
    "section": "",
    "text": "Accuracy : is the percentage of correctly predicted examples out of all predictions, formally known as\n\n\\(Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#accuracy",
    "href": "evaluation_metrics.html#accuracy",
    "title": "Evaluation Metrics",
    "section": "",
    "text": "Accuracy : is the percentage of correctly predicted examples out of all predictions, formally known as\n\n\\(Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#precision",
    "href": "evaluation_metrics.html#precision",
    "title": "Evaluation Metrics",
    "section": "Precision",
    "text": "Precision\n\nPrecision is the the probability of the predicted bounding boxes matching actual ground truth boxes, also referred to as the positive predictive value.\n\n\\(Precision = \\frac{TP}{TP + FP} = \\frac{true \\ object \\ detection}{all \\ detected \\ boxes}\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#recall",
    "href": "evaluation_metrics.html#recall",
    "title": "Evaluation Metrics",
    "section": "Recall",
    "text": "Recall\n\nRecall is the true positive rate, also referred to as sensitivity, measures the probability of ground truth objects being correctly detected.\n\n\\(Recall = \\frac{TP}{TP + FN} = \\frac{true \\ object \\ detection}{all \\ ground \\ truth \\ boxes}\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#ap---average-precision",
    "href": "evaluation_metrics.html#ap---average-precision",
    "title": "Evaluation Metrics",
    "section": "AP - Average Precision",
    "text": "AP - Average Precision\n\nit is a single number metric that encapsulates both precision and recall and summarizes the Precision-Recall curve by averaging precision across recall values from 0 to 1\n\n\\(AP = \\dfrac{1}{11} \\sum_{r \\in \\{0,0.1,0.2,...,1\\}} p_{interp}(r)\\)\nwhere\n\\(p_{interp}(r) = \\max p(\\hat{r})\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#map-mean-average-precision",
    "href": "evaluation_metrics.html#map-mean-average-precision",
    "title": "Evaluation Metrics",
    "section": "mAP (Mean Average Precision)",
    "text": "mAP (Mean Average Precision)\n\ncount the accumulated TP and the accumulated FP and compute the precision/recall at each line. Average Precision is computed as the average precision at 11 equally spaced recall levels.",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "Models_Types/Model Design/resnet18_with_scratch.html",
    "href": "Models_Types/Model Design/resnet18_with_scratch.html",
    "title": "Resnet 18 from Scratch",
    "section": "",
    "text": "import torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nfrom fastbook import search_images_ddg\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\nfrom nbdevAuto import functions\nimport os\nimport shutil\n\n\nimport torch.nn as nn\nimport torch\nfrom torch import Tensor\nfrom typing import Type\n\n\nclass BasicBlock(nn.Module):\n    def __init__(\n        self, \n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        expansion: int = 1,\n        downsample: nn.Module = None\n    ) -&gt; None:\n        super(BasicBlock, self).__init__()\n        # Multiplicative factor for the subsequent conv2d layer's output channels.\n        # It is 1 for ResNet18 and ResNet34.\n        self.expansion = expansion\n        self.downsample = downsample\n        self.conv1 = nn.Conv2d(\n            in_channels, \n            out_channels, \n            kernel_size=3, \n            stride=stride, \n            padding=1,\n            bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(\n            out_channels, \n            out_channels*self.expansion, \n            kernel_size=3, \n            padding=1,\n            bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(out_channels*self.expansion)\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return  out\n\n\nclass ResNet(nn.Module):\n    def __init__(\n        self, \n        img_channels: int,\n        num_layers: int,\n        block: Type[BasicBlock],\n        num_classes: int  = 1000\n    ) -&gt; None:\n        super(ResNet, self).__init__()\n        if num_layers == 18:\n            # The following `layers` list defines the number of `BasicBlock` \n            # to use to build the network and how many basic blocks to stack\n            # together.\n            layers = [2, 2, 2, 2]\n            self.expansion = 1\n        \n        self.in_channels = 64\n        # All ResNets (18 to 152) contain a Conv2d =&gt; BN =&gt; ReLU for the first\n        # three layers. Here, kernel size is 7.\n        self.conv1 = nn.Conv2d(\n            in_channels=img_channels,\n            out_channels=self.in_channels,\n            kernel_size=7, \n            stride=2,\n            padding=3,\n            bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512*self.expansion, num_classes)\n    def _make_layer(\n        self, \n        block: Type[BasicBlock],\n        out_channels: int,\n        blocks: int,\n        stride: int = 1\n    ) -&gt; nn.Sequential:\n        downsample = None\n        if stride != 1:\n            \"\"\"\n            This should pass from `layer2` to `layer4` or \n            when building ResNets50 and above. Section 3.3 of the paper\n            Deep Residual Learning for Image Recognition\n            (https://arxiv.org/pdf/1512.03385v1.pdf).\n            \"\"\"\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.in_channels, \n                    out_channels*self.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False \n                ),\n                nn.BatchNorm2d(out_channels * self.expansion),\n            )\n        layers = []\n        layers.append(\n            block(\n                self.in_channels, out_channels, stride, self.expansion, downsample\n            )\n        )\n        self.in_channels = out_channels * self.expansion\n        for i in range(1, blocks):\n            layers.append(block(\n                self.in_channels,\n                out_channels,\n                expansion=self.expansion\n            ))\n        return nn.Sequential(*layers)\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        # The spatial dimension of the final layer's feature \n        # map should be (7, 7) for all ResNets.\n        #print('Dimensions of the last convolutional feature map: ', x.shape)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n\ntensor = torch.rand([1, 3, 224, 224])\nmodel = ResNet(img_channels=3, num_layers=18, block=BasicBlock, num_classes=1000)\nprint(model)\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\noutput = model(tensor)\n\n11,689,512 total parameters.\n11,689,512 training parameters.\n\n\n\nimport matplotlib.pyplot as plt\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nplt.style.use('ggplot')\ndef get_data(batch_size=64):\n    # CIFAR10 training dataset.\n    dataset_train = datasets.CIFAR10(\n        root='data',\n        train=True,\n        download=True,\n        transform=ToTensor(),\n    )\n    # CIFAR10 validation dataset.\n    dataset_valid = datasets.CIFAR10(\n        root='data',\n        train=False,\n        download=True,\n        transform=ToTensor(),\n    )\n    # Create data loaders.\n    train_loader = DataLoader(\n        dataset_train, \n        batch_size=batch_size,\n        shuffle=True\n    )\n    valid_loader = DataLoader(\n        dataset_valid, \n        batch_size=batch_size,\n        shuffle=False\n    )\n    return train_loader, valid_loader\n\n\ndef save_plots(train_acc, valid_acc, train_loss, valid_loss, name=None):\n    \"\"\"\n    Function to save the loss and accuracy plots to disk.\n    \"\"\"\n    # Accuracy plots.\n    plt.figure(figsize=(10, 7))\n    plt.plot(\n        train_acc, color='tab:blue', linestyle='-', \n        label='train accuracy'\n    )\n    plt.plot(\n        valid_acc, color='tab:red', linestyle='-', \n        label='validataion accuracy'\n    )\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n    \n    # Loss plots.\n    plt.figure(figsize=(10, 7))\n    plt.plot(\n        train_loss, color='tab:blue', linestyle='-', \n        label='train loss'\n    )\n    plt.plot(\n        valid_loss, color='tab:red', linestyle='-', \n        label='validataion loss'\n    )\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n\nimport torch\nfrom tqdm import tqdm\n# Training function.\ndef train(model, trainloader, optimizer, criterion, device):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # Forward pass.\n        outputs = model(image)\n        # Calculate the loss.\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # Calculate the accuracy.\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # Backpropagation\n        loss.backward()\n        # Update the weights.\n        optimizer.step()\n    \n    # Loss and accuracy for the complete epoch.\n    epoch_loss = train_running_loss / counter\n    # epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    return epoch_loss, epoch_acc\n\n\n# Validation function.\ndef validate(model, testloader, criterion, device):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # Forward pass.\n            outputs = model(image)\n            # Calculate the loss.\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # Calculate the accuracy.\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n        \n    # Loss and accuracy for the complete epoch.\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n    return epoch_loss, epoch_acc\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport numpy as np\nimport random\n\n# Set seed.\nseed = 42\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True\nnp.random.seed(seed)\nrandom.seed(seed)\n\n\n# Learning and training parameters.\nepochs = 20\nbatch_size = 64\nlearning_rate = 0.01\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_loader, valid_loader = get_data(batch_size=batch_size)\n# Define model based on the argument parser string.\n\nprint('[INFO]: Training ResNet18 built from scratch...')\nmodel = ResNet(img_channels=3, num_layers=18, block=BasicBlock, num_classes=10).to(device)\nplot_name = 'resnet_scratch'\n\n# print(model)\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\n# Optimizer.\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n# Loss function.\ncriterion = nn.CrossEntropyLoss()\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n[INFO]: Training ResNet18 built from scratch...\n11,181,642 total parameters.\n11,181,642 training parameters.\n\n\n\n# Lists to keep track of losses and accuracies.\ntrain_loss, valid_loss = [], []\ntrain_acc, valid_acc = [], []\n# Start the training.\nfor epoch in range(epochs):\n    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss, train_epoch_acc = train(\n        model, \n        train_loader, \n        optimizer, \n        criterion,\n        device\n    )\n    valid_epoch_loss, valid_epoch_acc = validate(\n        model, \n        valid_loader, \n        criterion,\n        device\n    )\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    train_acc.append(train_epoch_acc)\n    valid_acc.append(valid_epoch_acc)\n    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n    print('-'*50)\n    \n# Save the loss and accuracy plots.\nsave_plots(\n    train_acc, \n    valid_acc, \n    train_loss, \n    valid_loss, \n    name=plot_name\n)\nprint('TRAINING COMPLETE')\n\n[INFO]: Epoch 1 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:17&lt;00:00, 44.98it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01&lt;00:00, 83.11it/s]\n\n\nTraining loss: 1.421, training acc: 48.966\nValidation loss: 1.590, validation acc: 45.520\n--------------------------------------------------\n[INFO]: Epoch 2 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:15&lt;00:00, 49.09it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01&lt;00:00, 80.99it/s]\n\n\nTraining loss: 1.021, training acc: 63.716\nValidation loss: 1.145, validation acc: 60.090\n--------------------------------------------------\n[INFO]: Epoch 3 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:14&lt;00:00, 54.44it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01&lt;00:00, 78.70it/s]\n\n\nTraining loss: 0.834, training acc: 70.488\nValidation loss: 1.317, validation acc: 57.260\n--------------------------------------------------\n[INFO]: Epoch 4 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:17&lt;00:00, 44.86it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 73.68it/s]\n\n\nTraining loss: 0.701, training acc: 75.158\nValidation loss: 2.220, validation acc: 46.050\n--------------------------------------------------\n[INFO]: Epoch 5 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:17&lt;00:00, 44.30it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 73.34it/s]\n\n\nTraining loss: 0.595, training acc: 78.946\nValidation loss: 2.410, validation acc: 41.760\n--------------------------------------------------\n[INFO]: Epoch 6 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:15&lt;00:00, 49.72it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 73.03it/s]\n\n\nTraining loss: 0.495, training acc: 82.466\nValidation loss: 1.990, validation acc: 47.290\n--------------------------------------------------\n[INFO]: Epoch 7 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:18&lt;00:00, 42.86it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 77.01it/s]\n\n\nTraining loss: 0.410, training acc: 85.436\nValidation loss: 2.319, validation acc: 45.660\n--------------------------------------------------\n[INFO]: Epoch 8 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:15&lt;00:00, 49.12it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 76.58it/s]\n\n\nTraining loss: 0.341, training acc: 87.844\nValidation loss: 1.169, validation acc: 66.930\n--------------------------------------------------\n[INFO]: Epoch 9 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:14&lt;00:00, 52.23it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 78.26it/s]\n\n\nTraining loss: 0.272, training acc: 90.396\nValidation loss: 2.214, validation acc: 50.400\n--------------------------------------------------\n[INFO]: Epoch 10 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:14&lt;00:00, 53.12it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 76.25it/s]\n\n\nTraining loss: 0.223, training acc: 92.148\nValidation loss: 1.630, validation acc: 61.040\n--------------------------------------------------\n[INFO]: Epoch 11 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:14&lt;00:00, 52.97it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01&lt;00:00, 79.03it/s]\n\n\nTraining loss: 0.189, training acc: 93.398\nValidation loss: 1.593, validation acc: 63.560\n--------------------------------------------------\n[INFO]: Epoch 12 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:19&lt;00:00, 39.58it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 75.93it/s]\n\n\nTraining loss: 0.157, training acc: 94.440\nValidation loss: 1.967, validation acc: 58.260\n--------------------------------------------------\n[INFO]: Epoch 13 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:17&lt;00:00, 43.74it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 72.56it/s]\n\n\nTraining loss: 0.143, training acc: 94.946\nValidation loss: 1.820, validation acc: 61.150\n--------------------------------------------------\n[INFO]: Epoch 14 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:15&lt;00:00, 50.41it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 77.98it/s]\n\n\nTraining loss: 0.112, training acc: 96.124\nValidation loss: 1.780, validation acc: 62.380\n--------------------------------------------------\n[INFO]: Epoch 15 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:15&lt;00:00, 51.22it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01&lt;00:00, 78.64it/s]\n\n\nTraining loss: 0.087, training acc: 97.016\nValidation loss: 2.463, validation acc: 55.470\n--------------------------------------------------\n[INFO]: Epoch 16 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:15&lt;00:00, 51.76it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 72.27it/s]\n\n\nTraining loss: 0.088, training acc: 96.860\nValidation loss: 1.407, validation acc: 70.530\n--------------------------------------------------\n[INFO]: Epoch 17 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:16&lt;00:00, 46.56it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 70.33it/s]\n\n\nTraining loss: 0.068, training acc: 97.678\nValidation loss: 1.432, validation acc: 69.650\n--------------------------------------------------\n[INFO]: Epoch 18 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:17&lt;00:00, 45.68it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 69.50it/s]\n\n\nTraining loss: 0.062, training acc: 97.840\nValidation loss: 2.301, validation acc: 59.010\n--------------------------------------------------\n[INFO]: Epoch 19 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:17&lt;00:00, 44.99it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 67.29it/s]\n\n\nTraining loss: 0.058, training acc: 98.000\nValidation loss: 1.378, validation acc: 72.670\n--------------------------------------------------\n[INFO]: Epoch 20 of 20\nTraining\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:15&lt;00:00, 49.71it/s]\n\n\nValidation\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02&lt;00:00, 73.08it/s]\n\n\nTraining loss: 0.044, training acc: 98.504\nValidation loss: 1.761, validation acc: 68.020\n--------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTRAINING COMPLETE\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "Model Design",
      "Resnet 18 from Scratch"
    ]
  },
  {
    "objectID": "Models_Types/Model Design/simple-mnist-nn-from-scratch-numpy.html",
    "href": "Models_Types/Model Design/simple-mnist-nn-from-scratch-numpy.html",
    "title": "Simple MNIST NN from scratch",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom torchvision import datasets\nfrom tqdm.notebook import tqdm\n\n\n# Import dependencies\nimport torch \nfrom PIL import Image\nfrom torch import nn, save, load\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Get data \ntrain = datasets.MNIST(root=\"data\", download=True, train=True)\n\n\nm_data = train.data.reshape(60000, 28*28)\n\n\nnp_data = np.array(m_data)\ntarget = np.array(train.targets)\n\n\ndata = np.insert(np_data, 0, target, axis=1)\n\n\nnp_data.shape, target.shape, data.shape\n\n((60000, 784), (60000,), (60000, 785))\n\n\n\nm, n = data.shape\nnp.random.shuffle(data) # shuffle before splitting into dev and training sets\n\n\ndata_dev = data[0:1000].T\nY_dev = data_dev[0]\nX_dev = data_dev[1:n]\nX_dev = X_dev / 255.\n\ndata_train = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n]\nX_train = X_train / 255.\n_,m_train = X_train.shape\n\n\nY_train\n\narray([0, 7, 6, ..., 6, 5, 3], dtype=uint8)\n\n\nOur NN will have a simple two-layer architecture. Input layer \\(a^{[0]}\\) will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer \\(a^{[1]}\\) will have 10 units with ReLU activation, and finally our output layer \\(a^{[2]}\\) will have 10 units corresponding to the ten digit classes with softmax activation.\nForward propagation\n\\[Z^{[1]} = W^{[1]} X + b^{[1]}\\] \\[A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))\\] \\[Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\] \\[A^{[2]} = g_{\\text{softmax}}(Z^{[2]})\\]\nBackward propagation\n\\[dZ^{[2]} = A^{[2]} - Y\\] \\[dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}\\] \\[dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}\\] \\[dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})\\] \\[dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}\\] \\[dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}\\]\nParameter updates\n\\[W^{[2]} := W^{[2]} - \\alpha dW^{[2]}\\] \\[b^{[2]} := b^{[2]} - \\alpha db^{[2]}\\] \\[W^{[1]} := W^{[1]} - \\alpha dW^{[1]}\\] \\[b^{[1]} := b^{[1]} - \\alpha db^{[1]}\\]\nVars and shapes\nForward prop\n\n\\(A^{[0]} = X\\): 784 x m\n\\(Z^{[1]} \\sim A^{[1]}\\): 10 x m\n\\(W^{[1]}\\): 10 x 784 (as \\(W^{[1]} A^{[0]} \\sim Z^{[1]}\\))\n\\(B^{[1]}\\): 10 x 1\n\\(Z^{[2]} \\sim A^{[2]}\\): 10 x m\n\\(W^{[1]}\\): 10 x 10 (as \\(W^{[2]} A^{[1]} \\sim Z^{[2]}\\))\n\\(B^{[2]}\\): 10 x 1\n\nBackprop\n\n\\(dZ^{[2]}\\): 10 x m (\\(~A^{[2]}\\))\n\\(dW^{[2]}\\): 10 x 10\n\\(dB^{[2]}\\): 10 x 1\n\\(dZ^{[1]}\\): 10 x m (\\(~A^{[1]}\\))\n\\(dW^{[1]}\\): 10 x 10\n\\(dB^{[1]}\\): 10 x 1\n\n\ndef init_params():\n    W1 = np.random.rand(10, 784) - 0.5\n    b1 = np.random.rand(10, 1) - 0.5\n    W2 = np.random.rand(10, 10) - 0.5\n    b2 = np.random.rand(10, 1) - 0.5\n    return W1, b1, W2, b2\n\ndef ReLU(Z):\n    return np.maximum(Z, 0)\n\ndef softmax(Z):\n    A = np.exp(Z) / sum(np.exp(Z))\n    return A\n    \ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = W1.dot(X) + b1\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef ReLU_deriv(Z):\n    return Z &gt; 0\n\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\ndef backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    one_hot_Y = one_hot(Y)\n    dZ2 = A2 - one_hot_Y\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    db2 = 1 / m * np.sum(dZ2)\n    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    db1 = 1 / m * np.sum(dZ1)\n    return dW1, db1, dW2, db2\n\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    return W1, b1, W2, b2\n\n\ndef get_predictions(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = init_params()\n    for i in tqdm(range(iterations)):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 50 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, b1, W2, b2\n\n\nW1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.20, 500)\n\n\n\n\nIteration:  0\n[0 9 0 ... 0 0 5] [0 7 6 ... 6 5 3]\n0.1644406779661017\nIteration:  50\n[0 7 6 ... 6 0 8] [0 7 6 ... 6 5 3]\n0.6483898305084745\nIteration:  100\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.7637118644067796\nIteration:  150\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.810322033898305\nIteration:  200\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8343559322033899\nIteration:  250\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8489322033898306\nIteration:  300\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8593050847457627\nIteration:  350\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8668135593220339\nIteration:  400\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8729491525423729\nIteration:  450\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8777457627118644\n\n\n~85% accuracy on training set.\n\ndef make_predictions(X, W1, b1, W2, b2):\n    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n    predictions = get_predictions(A2)\n    return predictions\n\ndef test_prediction(index, W1, b1, W2, b2):\n    current_image = X_train[:, index, None]\n    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n    label = Y_train[index]\n    print(\"Prediction: \", prediction)\n    print(\"Label: \", label)\n    \n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()\n    plt.imshow(current_image, interpolation='nearest')\n    plt.show()\n\nLetâ€™s look at a couple of examples:\n\ntest_prediction(0, W1, b1, W2, b2)\ntest_prediction(1, W1, b1, W2, b2)\ntest_prediction(2, W1, b1, W2, b2)\ntest_prediction(3, W1, b1, W2, b2)\n\nPrediction:  [0]\nLabel:  0\n\n\n\n\n\n\n\n\n\nPrediction:  [7]\nLabel:  7\n\n\n\n\n\n\n\n\n\nPrediction:  [6]\nLabel:  6\n\n\n\n\n\n\n\n\n\nPrediction:  [7]\nLabel:  7\n\n\n\n\n\n\n\n\n\nFinally, letâ€™s find the accuracy on the dev set:\n\ndev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\nget_accuracy(dev_predictions, Y_dev)\n\n[1 0 1 9 9 0 3 4 5 3 8 0 3 9 1 4 6 7 0 3 7 5 2 1 0 0 2 5 7 0 9 2 5 0 2 8 8\n 5 2 3 2 3 6 5 6 5 6 5 1 7 1 7 5 7 4 9 7 7 1 3 2 5 9 9 6 4 1 6 7 7 1 5 1 6\n 2 5 0 9 7 2 5 3 4 0 7 7 5 0 7 9 6 4 5 3 1 9 0 2 1 2 0 2 5 5 4 1 8 9 1 5 2\n 5 3 4 2 0 2 5 0 0 4 0 3 0 7 0 0 8 4 1 7 6 3 5 0 4 6 6 1 3 8 9 1 5 6 6 1 7\n 6 2 5 8 9 7 6 9 0 7 6 4 7 1 2 9 9 4 2 0 7 1 4 5 8 0 7 9 2 0 4 6 6 4 2 7 7\n 0 4 7 0 9 4 6 7 6 2 4 5 1 8 1 3 2 0 4 4 7 1 2 8 2 2 3 3 7 7 9 5 1 6 1 0 7\n 4 0 6 0 6 1 9 8 7 0 5 4 7 7 5 0 2 6 6 1 7 8 2 9 7 3 5 1 6 2 8 1 0 4 7 6 8\n 6 6 0 7 7 7 0 0 9 6 0 8 6 6 8 6 6 8 8 0 8 7 1 8 4 5 8 2 2 5 0 1 9 0 7 3 0\n 3 9 2 0 6 9 8 9 5 7 7 5 2 7 7 9 3 3 6 8 3 8 5 3 9 1 8 0 4 5 9 9 6 2 1 4 4\n 0 0 9 9 1 6 3 1 7 3 7 4 6 2 9 1 0 3 2 1 1 4 8 1 2 8 4 8 7 6 4 1 3 7 7 0 3\n 0 6 2 6 6 5 2 9 2 7 8 8 0 0 0 7 0 1 2 2 4 0 4 1 5 4 8 0 8 8 8 8 4 5 4 1 9\n 6 5 6 4 3 8 1 3 6 9 6 7 9 9 6 3 5 1 5 1 4 1 1 9 4 0 6 4 3 8 7 9 5 1 7 0 7\n 7 2 3 5 7 8 8 4 7 6 8 1 4 7 8 9 4 3 6 0 4 5 7 4 4 5 8 4 6 5 8 3 1 2 5 5 5\n 6 1 9 7 6 0 3 7 4 1 8 2 4 8 8 1 4 9 7 3 9 2 1 1 8 2 6 7 2 6 6 1 0 0 1 9 3\n 9 8 3 9 6 7 6 8 4 6 3 9 4 8 4 1 9 8 2 5 4 4 3 5 7 1 3 7 1 6 4 6 1 7 5 7 5\n 9 9 6 3 0 1 6 1 8 0 8 8 0 6 2 8 7 3 1 8 6 9 6 7 1 0 3 3 1 3 7 4 9 0 4 9 0\n 3 3 1 4 7 4 0 7 4 0 1 5 2 2 7 2 7 7 5 4 3 2 8 3 5 7 9 3 5 2 0 8 1 9 4 1 4\n 9 9 1 3 1 3 9 7 0 1 3 4 0 5 2 9 8 3 8 9 0 5 1 7 1 5 5 9 6 9 1 1 8 1 1 6 2\n 0 7 1 5 2 2 1 8 2 7 7 1 4 6 0 2 2 1 4 0 6 2 1 2 0 3 6 7 6 3 6 7 9 0 7 7 9\n 7 5 2 2 8 6 7 8 0 3 0 1 4 5 3 6 1 3 9 1 6 1 8 3 7 3 1 5 3 5 0 1 4 0 9 0 0\n 8 5 9 8 0 2 4 9 1 2 1 8 3 6 4 2 4 1 1 2 3 2 7 5 4 2 5 0 6 3 0 4 4 7 9 3 8\n 0 3 6 1 0 1 8 3 6 5 7 0 5 3 3 4 7 7 2 7 1 3 4 7 0 1 2 3 8 4 3 8 0 0 7 6 8\n 6 1 6 2 3 2 8 2 3 1 0 2 8 3 3 0 3 1 2 0 1 1 6 4 0 1 7 9 4 2 8 6 7 9 1 7 7\n 3 1 8 8 8 9 4 0 7 0 3 1 6 1 1 5 4 8 4 7 5 9 0 3 1 0 1 7 6 1 0 6 4 1 8 2 4\n 2 5 5 6 7 8 4 5 7 6 7 2 1 8 1 5 0 3 8 2 1 0 4 1 9 9 5 6 9 6 1 9 2 3 9 9 7\n 6 9 9 7 7 1 0 9 3 3 2 1 8 3 0 8 9 5 3 1 9 1 5 1 3 6 0 7 4 6 1 2 7 9 5 1 2\n 0 3 5 1 7 4 7 3 8 5 6 4 6 1 4 1 6 5 5 5 1 0 0 2 6 2 8 4 0 6 7 7 0 7 6 1 3\n 0] [1 0 1 9 8 0 3 4 5 3 8 0 3 9 1 4 6 7 0 3 2 3 8 1 5 0 2 5 7 0 9 2 6 0 2 6 8\n 5 2 3 2 3 6 5 6 5 6 5 1 7 1 7 8 7 4 9 3 7 1 3 2 3 8 9 6 9 1 6 7 7 1 5 1 6\n 2 5 0 4 7 2 5 3 4 7 7 7 5 0 7 9 6 4 5 3 1 7 0 6 1 2 0 2 5 5 4 1 8 9 1 5 2\n 5 5 9 2 0 7 8 0 0 4 0 3 0 7 5 0 8 4 1 7 6 3 5 2 4 6 6 1 8 8 4 1 5 6 6 1 7\n 6 2 3 8 9 7 6 9 0 7 6 4 7 1 2 9 9 4 2 0 7 1 4 5 8 0 7 9 2 0 4 4 6 4 2 7 7\n 0 4 7 0 7 4 6 7 6 7 4 1 1 8 1 3 2 0 4 4 7 1 2 8 2 2 3 3 7 7 9 5 1 6 1 0 7\n 4 0 6 0 6 1 9 8 9 0 5 4 1 9 5 0 2 6 2 1 7 8 2 9 7 3 5 1 6 2 8 1 0 8 7 6 8\n 6 6 0 7 7 7 8 0 9 6 0 8 6 6 8 6 6 8 2 0 8 7 1 8 4 5 8 2 2 8 2 1 9 0 7 5 7\n 0 9 2 0 6 7 8 9 3 7 7 8 2 7 9 9 3 8 4 4 3 8 5 3 9 2 8 0 6 5 9 9 6 2 1 4 4\n 0 0 9 9 1 6 3 1 7 3 7 4 6 8 9 7 0 3 2 1 1 2 6 1 2 8 4 8 7 6 6 1 3 7 7 0 3\n 0 6 2 6 6 5 2 9 2 7 8 8 0 0 0 7 0 1 2 2 4 0 6 1 5 4 8 0 8 2 8 8 4 3 0 1 9\n 6 5 4 4 3 8 1 3 6 9 6 7 9 9 6 5 8 1 5 1 4 1 1 9 4 2 5 4 3 8 7 9 3 1 7 0 9\n 7 2 3 5 2 8 8 4 9 6 8 1 4 7 8 9 4 3 6 0 4 5 7 4 4 5 8 4 6 5 3 3 1 2 5 5 5\n 6 1 9 7 6 0 3 7 4 1 8 7 4 8 8 1 7 9 3 3 4 2 8 5 8 7 6 7 1 6 6 1 0 2 1 9 3\n 9 8 3 9 5 7 6 4 4 6 3 9 5 8 4 1 9 8 8 3 4 4 3 5 7 1 3 7 1 6 4 6 1 2 5 7 5\n 5 9 6 3 0 1 6 1 8 0 8 8 0 6 2 5 7 3 8 8 6 9 4 7 1 0 3 1 1 3 7 4 9 0 4 9 0\n 3 3 1 4 7 4 0 7 4 0 1 5 2 2 7 2 7 5 3 4 9 2 8 5 5 7 4 3 5 1 0 8 1 9 4 1 4\n 9 4 1 3 1 3 9 7 0 1 3 4 0 7 8 9 8 3 5 9 0 5 1 7 1 8 5 9 6 9 1 1 8 1 1 6 2\n 0 7 1 5 2 2 1 8 2 2 5 1 4 6 0 2 2 1 4 0 6 2 1 2 0 3 6 7 6 3 6 7 9 0 9 7 9\n 7 5 2 2 8 6 7 8 0 3 0 1 4 5 3 6 1 3 4 1 6 4 8 3 7 3 2 5 3 3 0 1 4 0 9 0 0\n 8 3 9 8 0 2 9 9 1 2 1 8 3 6 4 7 4 1 1 2 3 2 7 5 4 2 5 0 6 3 0 4 4 7 9 3 2\n 0 3 6 1 0 8 8 8 6 5 7 0 5 8 8 4 3 7 0 7 1 7 4 7 0 4 7 3 8 4 3 8 0 0 7 2 2\n 6 1 6 2 3 2 8 5 9 1 9 2 8 3 3 0 3 1 2 0 1 1 6 4 0 1 7 9 4 2 8 6 7 9 8 7 9\n 3 1 8 3 8 9 4 0 7 0 3 1 6 1 1 5 5 8 4 7 5 9 0 3 1 0 1 7 6 1 0 6 4 1 8 2 4\n 2 8 8 6 7 8 4 5 7 6 7 6 1 8 1 5 0 3 8 2 1 0 9 1 5 9 5 6 9 6 1 9 2 3 7 9 2\n 6 9 9 7 7 1 0 9 8 3 2 1 5 3 0 8 7 3 3 1 9 1 4 1 3 6 0 7 4 6 1 2 7 9 5 1 3\n 0 3 5 1 7 4 7 3 8 5 6 4 6 1 4 1 6 5 5 5 1 2 0 2 6 2 8 4 0 6 7 7 0 7 6 1 3\n 7]\n\n\n0.856\n\n\nStill 84% accuracy, so our model generalized from the training data pretty well.\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "Model Design",
      "Simple MNIST NN from scratch"
    ]
  },
  {
    "objectID": "Models_Types/cnn.html#resnet18",
    "href": "Models_Types/cnn.html#resnet18",
    "title": "CNN - Convolutional Neural Networks",
    "section": "Resnet18",
    "text": "Resnet18",
    "crumbs": [
      "Blog",
      "Models Types",
      "CNN - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "Models_Types/gru.html",
    "href": "Models_Types/gru.html",
    "title": "GRU - Gated Recurrent Unit",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "GRU - Gated Recurrent Unit"
    ]
  },
  {
    "objectID": "Models_Types/lstm.html",
    "href": "Models_Types/lstm.html",
    "title": "LSTM - Long Short-Term Memory Networks",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "LSTM - Long Short-Term Memory Networks"
    ]
  },
  {
    "objectID": "Models_Types/rbm.html",
    "href": "Models_Types/rbm.html",
    "title": "RBM - Restricted Boltzmann Machines",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "RBM - Restricted Boltzmann Machines"
    ]
  },
  {
    "objectID": "Models_Types/dbn.html",
    "href": "Models_Types/dbn.html",
    "title": "DBN - Deep Belief Networks",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "DBN - Deep Belief Networks"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DeepLearning",
    "section": "",
    "text": "use of any dataset - cifar, imagenet, kitti, nyu-dataset, Cityscapes\nuse of any model - resnet18, ViT, UNet, midas (depth)\nmodels for detection - faster R-CNN, YOLO, DETR, DINO\ndifferent applications - object classification, object detection, object segmentation\nmeasure change in performance",
    "crumbs": [
      "Blog",
      "DeepLearning"
    ]
  },
  {
    "objectID": "index.html#dimensions",
    "href": "index.html#dimensions",
    "title": "DeepLearning",
    "section": "",
    "text": "use of any dataset - cifar, imagenet, kitti, nyu-dataset, Cityscapes\nuse of any model - resnet18, ViT, UNet, midas (depth)\nmodels for detection - faster R-CNN, YOLO, DETR, DINO\ndifferent applications - object classification, object detection, object segmentation\nmeasure change in performance",
    "crumbs": [
      "Blog",
      "DeepLearning"
    ]
  },
  {
    "objectID": "model_creation.html",
    "href": "model_creation.html",
    "title": "Pytorch Model Creation",
    "section": "",
    "text": "import torch\nimport numpy as np\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#autograd",
    "href": "model_creation.html#autograd",
    "title": "Pytorch Model Creation",
    "section": "Autograd",
    "text": "Autograd\n\nx = torch.tensor([1,2,3], dtype=torch.float, requires_grad = True)\nx\n\ntensor([1., 2., 3.], requires_grad=True)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#containers",
    "href": "model_creation.html#containers",
    "title": "Pytorch Model Creation",
    "section": "Containers",
    "text": "Containers\n\nModule\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\n\n@torch.no_grad()\ndef init_weights(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n\nnet = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\nnew = net.apply(init_weights)\n\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n\n\n\nmodel = Model()\nmodel\n\nModel(\n  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n)\n\n\n\nmodel.__dict__\n\n{'training': True,\n '_parameters': OrderedDict(),\n '_buffers': OrderedDict(),\n '_non_persistent_buffers_set': set(),\n '_backward_pre_hooks': OrderedDict(),\n '_backward_hooks': OrderedDict(),\n '_is_full_backward_hook': None,\n '_forward_hooks': OrderedDict(),\n '_forward_hooks_with_kwargs': OrderedDict(),\n '_forward_hooks_always_called': OrderedDict(),\n '_forward_pre_hooks': OrderedDict(),\n '_forward_pre_hooks_with_kwargs': OrderedDict(),\n '_state_dict_hooks': OrderedDict(),\n '_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_post_hooks': OrderedDict(),\n '_modules': OrderedDict([('conv1',\n               Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))),\n              ('conv2', Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1)))])}\n\n\n\nfor para in model.parameters():\n    print(para.shape)\n\ntorch.Size([20, 1, 5, 5])\ntorch.Size([20])\ntorch.Size([20, 20, 5, 5])\ntorch.Size([20])\n\n\n\n\nSequential\n\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n\nmodel\n\nSequential(\n  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n  (3): ReLU()\n)\n\n\n\n\nModuleList\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n\nmodel = MyModule()\nmodel\n\nMyModule(\n  (linears): ModuleList(\n    (0-9): 10 x Linear(in_features=10, out_features=10, bias=True)\n  )\n)\n\n\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n\nmodel = MyModule()\nmodel\n\nMyModule(\n  (choices): ModuleDict(\n    (conv): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (pool): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n  )\n  (activations): ModuleDict(\n    (lrelu): LeakyReLU(negative_slope=0.01)\n    (prelu): PReLU(num_parameters=1)\n  )\n)\n\n\n\n\nParameterList\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n\nmodel = MyModule()\nmodel\n\nMyModule(\n  (params): ParameterList(\n      (0): Parameter containing: [torch.float32 of size 10x10]\n      (1): Parameter containing: [torch.float32 of size 10x10]\n      (2): Parameter containing: [torch.float32 of size 10x10]\n      (3): Parameter containing: [torch.float32 of size 10x10]\n      (4): Parameter containing: [torch.float32 of size 10x10]\n      (5): Parameter containing: [torch.float32 of size 10x10]\n      (6): Parameter containing: [torch.float32 of size 10x10]\n      (7): Parameter containing: [torch.float32 of size 10x10]\n      (8): Parameter containing: [torch.float32 of size 10x10]\n      (9): Parameter containing: [torch.float32 of size 10x10]\n  )\n)\n\n\n\n\nParameterDict\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n\nmodel = MyModule()\nmodel\n\nMyModule(\n  (params): ParameterDict(\n      (left): Parameter containing: [torch.FloatTensor of size 5x10]\n      (right): Parameter containing: [torch.FloatTensor of size 5x10]\n  )\n)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#convolution-layers",
    "href": "model_creation.html#convolution-layers",
    "title": "Pytorch Model Creation",
    "section": "Convolution Layers",
    "text": "Convolution Layers\n\nnn.Conv1d\n\ninput1 = torch.torch.tensor([[[[ 1.,  2.,  3., 4., 5.],\n                                  [ 6.,  7.,  8., 9., 10.],\n                                  [11., 12., 13., 14., 15.],\n                                  [16., 17., 18., 19., 20.]]]])\n\n\nnew = input1.reshape(4,5)\nnew\n\ntensor([[ 1.,  2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9., 10.],\n        [11., 12., 13., 14., 15.],\n        [16., 17., 18., 19., 20.]])\n\n\n\nm = nn.Conv1d(4, 2, 3, stride=2)\ntype(m)\n\ntorch.nn.modules.conv.Conv1d\n\n\n\nfor para in m.parameters():\n    print(para.shape)\n\ntorch.Size([2, 4, 3])\ntorch.Size([2])\n\n\n\n# input = torch.randn(20, 16, 50)\noutput = m(new)\n\n\noutput.shape\n\ntorch.Size([2, 2])\n\n\n\noutput\n\ntensor([[-1.3600, -1.1102],\n        [ 6.1355,  6.0188]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n\nnn.Conv2d\n\nnew = input1.reshape(4, 5,1)\nnew\n\ntensor([[[ 1.],\n         [ 2.],\n         [ 3.],\n         [ 4.],\n         [ 5.]],\n\n        [[ 6.],\n         [ 7.],\n         [ 8.],\n         [ 9.],\n         [10.]],\n\n        [[11.],\n         [12.],\n         [13.],\n         [14.],\n         [15.]],\n\n        [[16.],\n         [17.],\n         [18.],\n         [19.],\n         [20.]]])\n\n\n\n# With square kernels and equal stride\nm = nn.Conv2d(4, 2, 3, stride=2)\n# non-square kernels and unequal stride and with padding\nm = nn.Conv2d(4, 2, (3, 5), stride=(2, 1), padding=(4, 2))\n# non-square kernels and unequal stride and with padding and dilation\nm = nn.Conv2d(4, 2, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n\noutput = m(new)\noutput.shape\n\ntorch.Size([2, 4, 1])\n\n\n\noutput\n\ntensor([[[-1.3839],\n         [-1.8465],\n         [-0.8563],\n         [-0.0644]],\n\n        [[ 1.2715],\n         [ 1.5135],\n         [-2.8098],\n         [-3.2210]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n\nnn.Conv3d\n\n# With square kernels and equal stride\nm = nn.Conv3d(16, 33, 3, stride=2)\n# non-square kernels and unequal} stride and with padding\nm = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\ninput = torch.randn(20, 16, 10, 50, 100)\noutput = m(input)\n\n\noutput.shape\n\ntorch.Size([20, 33, 8, 50, 99])\n\n\n\n\nnn.ConvTranspose2d\n\nThis module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation as it does not compute a true inverse of convolution).\n\n\n# With square kernels and equal stride\nm = nn.ConvTranspose2d(16, 33, 3, stride=2)\n# non-square kernels and unequal stride and with padding\nm = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\ninput = torch.randn(20, 16, 50, 100)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 33, 93, 100])\n\n\n\n# exact output size can be also specified as an argument\ninput = torch.randn(1, 16, 12, 12)\ndownsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\nupsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\nh = downsample(input)\nh.size()\n\ntorch.Size([1, 16, 6, 6])\n\n\n\noutput = upsample(h, output_size=input.size())\noutput.size()\n\ntorch.Size([1, 16, 12, 12])",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#pooling-layers",
    "href": "model_creation.html#pooling-layers",
    "title": "Pytorch Model Creation",
    "section": "Pooling Layers",
    "text": "Pooling Layers\n\nnn.MaxPool2d\n\nApplies a 2D max pooling over an input signal composed of several input planes.\n\n\nnew = input1.reshape(1, 4,5)\nnew\n\ntensor([[[ 1.,  2.,  3.,  4.,  5.],\n         [ 6.,  7.,  8.,  9., 10.],\n         [11., 12., 13., 14., 15.],\n         [16., 17., 18., 19., 20.]]])\n\n\n\n# pool of square window of size=3, stride=2\nm = nn.MaxPool2d(3, stride=2)\n# pool of non-square window\nm = nn.MaxPool2d((2, 2), stride=(2, 1))\ninput = torch.randn(20, 16, 50, 32)\noutput = m(new)\noutput\n\ntensor([[[ 7.,  8.,  9., 10.],\n         [17., 18., 19., 20.]]])\n\n\n\n\nnn.MaxUnpool2d\n\nComputes a partial inverse of MaxPool2d.\n\n\ninput = torch.tensor([[[[ 1.,  2.,  3.,  4.],\n                            [ 5.,  6.,  7.,  8.],\n                            [ 9., 10., 11., 12.],\n                            [13., 14., 15., 16.]]]])\n\n\npool = nn.MaxPool2d(2, stride=2, return_indices=True)\nunpool = nn.MaxUnpool2d(2, stride=2)\n\noutput, indices = pool(input)\nunpool(output, indices)\n\ntensor([[[[ 0.,  0.,  0.,  0.],\n          [ 0.,  6.,  0.,  8.],\n          [ 0.,  0.,  0.,  0.],\n          [ 0., 14.,  0., 16.]]]])\n\n\n\ninput1\n\ntensor([[[[ 1.,  2.,  3.,  4.,  5.],\n          [ 6.,  7.,  8.,  9., 10.],\n          [11., 12., 13., 14., 15.],\n          [16., 17., 18., 19., 20.]]]])\n\n\n\noutput, indices = pool(input1)\n# This call will not work without specifying output_size\nunpool(output, indices, output_size=input1.size())\n\ntensor([[[[ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  7.,  0.,  9.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [ 0., 17.,  0., 19.,  0.]]]])\n\n\n\n\nnn.AvgPool2d\n\nApplies a 2D average pooling over an input signal composed of several input planes.\n\n\ninput1\n\ntensor([[[[ 1.,  2.,  3.,  4.,  5.],\n          [ 6.,  7.,  8.,  9., 10.],\n          [11., 12., 13., 14., 15.],\n          [16., 17., 18., 19., 20.]]]])\n\n\n\n# pool of square window of size=3, stride=2\nm = nn.AvgPool2d(3, stride=2)\n# pool of non-square window\nm = nn.AvgPool2d((2, 2), stride=(1, 1))\n# input = torch.randn(20, 16, 50, 32)\noutput = m(input1)\noutput\n\ntensor([[[[ 4.,  5.,  6.,  7.],\n          [ 9., 10., 11., 12.],\n          [14., 15., 16., 17.]]]])\n\n\n\n\nnn.FractionalMaxPool2d\n\nApplies a 2D fractional max pooling over an input signal composed of several input planes.\n\n\n# pool of square window of size=3, and target output size 13x12\nm = nn.FractionalMaxPool2d(3, output_size=(13, 12))\n# pool of square window and target output size being half of input image size\nm = nn.FractionalMaxPool2d(2, output_ratio=(0.7, 0.7))\noutput = m(input1)\noutput\n\ntensor([[[[ 7.,  8., 10.],\n          [17., 18., 20.]]]])\n\n\n\n\nnn.LPPool2d\n\nApplies a 2D power-average pooling over an input signal composed of several input planes.\n\n\n# power-2 pool of square window of size=3, stride=2\nm = nn.LPPool2d(2, 3, stride=2)\n# pool of non-square window of power 1.2\nm = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))\n\noutput = m(input)\noutput\n\ntensor([[[[25.4396, 29.7206, 34.0561]]]])\n\n\n\n\nnn.AdaptiveMaxPool2d\n\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\n\n\n# target output size of 5x7\nm = nn.AdaptiveMaxPool2d((5, 7))\noutput = m(input1)\noutput\n\ntensor([[[[ 1.,  2.,  3.,  3.,  4.,  5.,  5.],\n          [ 6.,  7.,  8.,  8.,  9., 10., 10.],\n          [11., 12., 13., 13., 14., 15., 15.],\n          [16., 17., 18., 18., 19., 20., 20.],\n          [16., 17., 18., 18., 19., 20., 20.]]]])\n\n\n\n# target output size of 7x7 (square)\nm = nn.AdaptiveMaxPool2d(7)\noutput = m(input1)\noutput\n\ntensor([[[[ 1.,  2.,  3.,  3.,  4.,  5.,  5.],\n          [ 6.,  7.,  8.,  8.,  9., 10., 10.],\n          [ 6.,  7.,  8.,  8.,  9., 10., 10.],\n          [11., 12., 13., 13., 14., 15., 15.],\n          [11., 12., 13., 13., 14., 15., 15.],\n          [16., 17., 18., 18., 19., 20., 20.],\n          [16., 17., 18., 18., 19., 20., 20.]]]])\n\n\n\n# target output size of 10x7\nm = nn.AdaptiveMaxPool2d((None, 7))\noutput = m(input1)\noutput\n\ntensor([[[[ 1.,  2.,  3.,  3.,  4.,  5.,  5.],\n          [ 6.,  7.,  8.,  8.,  9., 10., 10.],\n          [11., 12., 13., 13., 14., 15., 15.],\n          [16., 17., 18., 18., 19., 20., 20.]]]])\n\n\n\n\nnn.AdaptiveAvgPool2d\n\nApplies a 2D adaptive average pooling over an input signal composed of several input planes.\n\n\n# target output size of 5x7\nm = nn.AdaptiveAvgPool2d((5, 7))\nm(input1)\n\ntensor([[[[ 1.0000,  1.5000,  2.5000,  3.0000,  3.5000,  4.5000,  5.0000],\n          [ 3.5000,  4.0000,  5.0000,  5.5000,  6.0000,  7.0000,  7.5000],\n          [ 8.5000,  9.0000, 10.0000, 10.5000, 11.0000, 12.0000, 12.5000],\n          [13.5000, 14.0000, 15.0000, 15.5000, 16.0000, 17.0000, 17.5000],\n          [16.0000, 16.5000, 17.5000, 18.0000, 18.5000, 19.5000, 20.0000]]]])\n\n\n\n# target output size of 7x7 (square)\nm = nn.AdaptiveAvgPool2d(7)\nm(input1)\n\ntensor([[[[ 1.0000,  1.5000,  2.5000,  3.0000,  3.5000,  4.5000,  5.0000],\n          [ 3.5000,  4.0000,  5.0000,  5.5000,  6.0000,  7.0000,  7.5000],\n          [ 6.0000,  6.5000,  7.5000,  8.0000,  8.5000,  9.5000, 10.0000],\n          [ 8.5000,  9.0000, 10.0000, 10.5000, 11.0000, 12.0000, 12.5000],\n          [11.0000, 11.5000, 12.5000, 13.0000, 13.5000, 14.5000, 15.0000],\n          [13.5000, 14.0000, 15.0000, 15.5000, 16.0000, 17.0000, 17.5000],\n          [16.0000, 16.5000, 17.5000, 18.0000, 18.5000, 19.5000, 20.0000]]]])\n\n\n\n# target output size of 10x7\nm = nn.AdaptiveAvgPool2d((None, 7))\nm(input1)\n\ntensor([[[[ 1.0000,  1.5000,  2.5000,  3.0000,  3.5000,  4.5000,  5.0000],\n          [ 6.0000,  6.5000,  7.5000,  8.0000,  8.5000,  9.5000, 10.0000],\n          [11.0000, 11.5000, 12.5000, 13.0000, 13.5000, 14.5000, 15.0000],\n          [16.0000, 16.5000, 17.5000, 18.0000, 18.5000, 19.5000, 20.0000]]]])",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#padding-layers",
    "href": "model_creation.html#padding-layers",
    "title": "Pytorch Model Creation",
    "section": "Padding Layers",
    "text": "Padding Layers\n\nnn.ReflectionPad2d\n\nPads the input tensor using the reflection of the input boundary.\n\n\nm = nn.ReflectionPad2d(2)\ninput = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\ninput\n\ntensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]]]])\n\n\n\nm(input)\n\ntensor([[[[8., 7., 6., 7., 8., 7., 6.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [2., 1., 0., 1., 2., 1., 0.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [8., 7., 6., 7., 8., 7., 6.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [2., 1., 0., 1., 2., 1., 0.]]]])\n\n\n\n# using different paddings for different sides\nm = nn.ReflectionPad2d((1, 1, 2, 0))\nm(input)\n\ntensor([[[[7., 6., 7., 8., 7.],\n          [4., 3., 4., 5., 4.],\n          [1., 0., 1., 2., 1.],\n          [4., 3., 4., 5., 4.],\n          [7., 6., 7., 8., 7.]]]])\n\n\n\n\nnn.ReplicationPad2d\n\nPads the input tensor using replication of the input boundary.\n\n\nm = nn.ReplicationPad2d(2)\n\nm(input)\n\ntensor([[[[0., 0., 0., 1., 2., 2., 2.],\n          [0., 0., 0., 1., 2., 2., 2.],\n          [0., 0., 0., 1., 2., 2., 2.],\n          [3., 3., 3., 4., 5., 5., 5.],\n          [6., 6., 6., 7., 8., 8., 8.],\n          [6., 6., 6., 7., 8., 8., 8.],\n          [6., 6., 6., 7., 8., 8., 8.]]]])\n\n\n\n# using different paddings for different sides\nm = nn.ReplicationPad2d((1, 1, 2, 0))\nm(input)\n\ntensor([[[[0., 0., 1., 2., 2.],\n          [0., 0., 1., 2., 2.],\n          [0., 0., 1., 2., 2.],\n          [3., 3., 4., 5., 5.],\n          [6., 6., 7., 8., 8.]]]])\n\n\n\n\nnn.ZeroPad2d\n\nPads the input tensor boundaries with zero.\n\n\nm = nn.ZeroPad2d(2)\nm(input)\n\ntensor([[[[0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 1., 2., 0., 0.],\n          [0., 0., 3., 4., 5., 0., 0.],\n          [0., 0., 6., 7., 8., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0.]]]])\n\n\n\n# using different paddings for different sides\nm = nn.ZeroPad2d((1, 1, 2, 0))\nm(input)\n\ntensor([[[[0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 1., 2., 0.],\n          [0., 3., 4., 5., 0.],\n          [0., 6., 7., 8., 0.]]]])\n\n\n\n\nnn.ConstantPad2d\n\nPads the input tensor boundaries with a constant value.\n\n\nm = nn.ConstantPad2d(2, 11)\n\nm(input)\n\ntensor([[[[11., 11., 11., 11., 11., 11., 11.],\n          [11., 11., 11., 11., 11., 11., 11.],\n          [11., 11.,  0.,  1.,  2., 11., 11.],\n          [11., 11.,  3.,  4.,  5., 11., 11.],\n          [11., 11.,  6.,  7.,  8., 11., 11.],\n          [11., 11., 11., 11., 11., 11., 11.],\n          [11., 11., 11., 11., 11., 11., 11.]]]])\n\n\n\n# using different paddings for different sides\nm = nn.ConstantPad2d((3, 0, 2, 1), 11)\nm(input)\n\ntensor([[[[11., 11., 11., 11., 11., 11.],\n          [11., 11., 11., 11., 11., 11.],\n          [11., 11., 11.,  0.,  1.,  2.],\n          [11., 11., 11.,  3.,  4.,  5.],\n          [11., 11., 11.,  6.,  7.,  8.],\n          [11., 11., 11., 11., 11., 11.]]]])",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#non-linear-activations-weighted-sum-nonlinearity",
    "href": "model_creation.html#non-linear-activations-weighted-sum-nonlinearity",
    "title": "Pytorch Model Creation",
    "section": "Non-linear Activations (weighted sum, nonlinearity)",
    "text": "Non-linear Activations (weighted sum, nonlinearity)\n\ninput = torch.linspace(-5,5,100)\n\n\nimport matplotlib.pyplot as plt \ndef plot_show(input, output):\n    plt.plot(input, input, color='green', linestyle='dashed',\n         linewidth=1, label = 'input')\n    plt.plot(input, output, color='red',\n             linewidth=1, label = 'output')\n    plt.legend()\n    plt.show()\n\n\nnn.LogSigmoid()\n\nm = nn.LogSigmoid()\noutput = m(input)\n\n\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.ReLU()\n\nm = nn.ReLU()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.LeakyReLU(0.5)\n\nm = nn.LeakyReLU(0.5)\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\nm = nn.SELU()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.Sigmoid()\n\nm = nn.Sigmoid()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.Softplus()\n\nm = nn.Softplus()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.Tanh()\n\nm = nn.Tanh()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.Threshold\n\nm = nn.Threshold(0, -5)\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.SELU\n\n\nNon-linear Activations (other)\n\ninput = torch.linspace(-1,1,10)\ninput = input.reshape(2,5)\ninput\n\ntensor([[-1.0000, -0.7778, -0.5556, -0.3333, -0.1111],\n        [ 0.1111,  0.3333,  0.5556,  0.7778,  1.0000]])\n\n\n\nm = nn.Softmin(dim=1)\n\n\noutput = m(input)\n\n\noutput\n\ntensor([[0.2970, 0.2379, 0.1905, 0.1525, 0.1221],\n        [0.2970, 0.2379, 0.1905, 0.1525, 0.1221]])\n\n\n\noutput.sum()\n\ntensor(2.0000)\n\n\n\nplot_show(input.flatten(), output.flatten())\n\n\n\n\n\n\n\n\n\n\nNormalization Layers\n\n\nnn.BatchNorm2d\n\ninput = torch.arange(27, dtype=torch.float).reshape(1,3, 3, 3)\ninput\n\ntensor([[[[ 0.,  1.,  2.],\n          [ 3.,  4.,  5.],\n          [ 6.,  7.,  8.]],\n\n         [[ 9., 10., 11.],\n          [12., 13., 14.],\n          [15., 16., 17.]],\n\n         [[18., 19., 20.],\n          [21., 22., 23.],\n          [24., 25., 26.]]]])\n\n\n\n# With Learnable Parameters\nm = nn.BatchNorm2d(3)\n# Without Learnable Parameters\nm = nn.BatchNorm2d(3, affine=False)\n\noutput = m(input)\noutput\n\ntensor([[[[-1.5492e+00, -1.1619e+00, -7.7460e-01],\n          [-3.8730e-01,  0.0000e+00,  3.8730e-01],\n          [ 7.7460e-01,  1.1619e+00,  1.5492e+00]],\n\n         [[-1.5492e+00, -1.1619e+00, -7.7460e-01],\n          [-3.8730e-01,  1.7881e-07,  3.8730e-01],\n          [ 7.7460e-01,  1.1619e+00,  1.5492e+00]],\n\n         [[-1.5492e+00, -1.1619e+00, -7.7460e-01],\n          [-3.8730e-01, -3.5763e-07,  3.8730e-01],\n          [ 7.7460e-01,  1.1619e+00,  1.5492e+00]]]])\n\n\n\n\nnn.GroupNorm\n\n# Separate 6 channels into 3 groups\nm = nn.GroupNorm(1, 3)\noutput = m(input)\noutput\n\ntensor([[[[-1.6690e+00, -1.5407e+00, -1.4123e+00],\n          [-1.2839e+00, -1.1555e+00, -1.0271e+00],\n          [-8.9872e-01, -7.7033e-01, -6.4194e-01]],\n\n         [[-5.1355e-01, -3.8516e-01, -2.5678e-01],\n          [-1.2839e-01, -2.9802e-08,  1.2839e-01],\n          [ 2.5678e-01,  3.8516e-01,  5.1355e-01]],\n\n         [[ 6.4194e-01,  7.7033e-01,  8.9872e-01],\n          [ 1.0271e+00,  1.1555e+00,  1.2839e+00],\n          [ 1.4123e+00,  1.5407e+00,  1.6690e+00]]]],\n       grad_fn=&lt;NativeGroupNormBackward0&gt;)\n\n\n\ninput_t = torch.randn(20, 6, 10, 10)\n# Separate 6 channels into 6 groups (equivalent with InstanceNorm)\nm = nn.GroupNorm(6, 6)\noutput = m(input_t)\n\n\n# Put all 6 channels into a single group (equivalent with LayerNorm)\nm = nn.GroupNorm(1, 6)\n# Activating the module\noutput = m(input_t)\n\n\n\nnn.LayerNorm\n\n# NLP Example\nbatch, sentence_length, embedding_dim = 20, 5, 10\nembedding = torch.randn(batch, sentence_length, embedding_dim)\nlayer_norm = nn.LayerNorm(embedding_dim)\n# Activate module\noutput = layer_norm(embedding)\n\n\nembedding[7,:,:].std()\n\ntensor(1.0733)\n\n\n\noutput[7,:,:].std()\n\ntensor(1.0101, grad_fn=&lt;StdBackward0&gt;)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#recurrent-layers",
    "href": "model_creation.html#recurrent-layers",
    "title": "Pytorch Model Creation",
    "section": "Recurrent Layers",
    "text": "Recurrent Layers\n\nRNNBase - Base class for RNN modules (RNN, LSTM, GRU).",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#nn.rnn",
    "href": "model_creation.html#nn.rnn",
    "title": "Pytorch Model Creation",
    "section": "nn.RNN",
    "text": "nn.RNN\n\nrnn = nn.RNN(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\noutput, hn = rnn(input, h0)\n\n\noutput.shape\n\ntorch.Size([5, 3, 20])\n\n\n\nnn.LSTM\n\nlstm = nn.LSTM(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\noutput, (hn, cn) = lstm(input, (h0, c0))\noutput.shape\n\ntorch.Size([5, 3, 20])\n\n\n\n\nnn.GRU\n\ngru = nn.GRU(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\noutput, hn = gru(input, h0)\noutput.shape\n\ntorch.Size([5, 3, 20])\n\n\n\n\nnn.RNNCell\n\nrnn = nn.RNNCell(10, 20)\ninput = torch.randn(6, 3, 10)\nhx = torch.randn(3, 20)\noutput = []\nfor i in range(6):\n    hx = rnn(input[i], hx)\n    output.append(hx)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#transformer-layers",
    "href": "model_creation.html#transformer-layers",
    "title": "Pytorch Model Creation",
    "section": "Transformer Layers",
    "text": "Transformer Layers\n\nnn.Transformer\n\ntransformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\nsrc = torch.rand((10, 32, 512))\ntgt = torch.rand((20, 32, 512))\nout = transformer_model(src, tgt)\n\n\n\nnn.TransformerEncoderLayer\n\nencoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\nsrc = torch.rand(10, 32, 512)\nout = transformer_encoder(src)\n\n\n\nnn.TransformerDecoderLayer\n\ndecoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\ntransformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\nmemory = torch.rand(10, 32, 512)\ntgt = torch.rand(20, 32, 512)\nout = transformer_decoder(tgt, memory)\n\n\n\nnn.TransformerEncoderLayer\n\nencoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\nsrc = torch.rand(32, 10, 512)\nout = encoder_layer(src)\n\n\n\nnn.TransformerDecoderLayer\n\ndecoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\nmemory = torch.rand(32, 10, 512)\ntgt = torch.rand(32, 20, 512)\nout = decoder_layer(tgt, memory)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#linear-layers",
    "href": "model_creation.html#linear-layers",
    "title": "Pytorch Model Creation",
    "section": "Linear Layers",
    "text": "Linear Layers\n\nnn.Identity\n\nA placeholder identity operator that is argument-insensitive.\n\n\nm = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\ninput = torch.randn(128, 20)\noutput = m(input)\nprint(output.size())\n\ntorch.Size([128, 20])\n\n\n\n\nnn.Linear\n\nm = nn.Linear(20, 30)\ninput = torch.randn(128, 20)\noutput = m(input)\nprint(output.size())\n\ntorch.Size([128, 30])\n\n\n\nfor para in m.parameters():\n    print(para.shape)\n\ntorch.Size([30, 20])\ntorch.Size([30])\n\n\n\n\nDropout Layers\n\nm = nn.Dropout(p=0.2)\ninput = torch.randn(20, 16)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 16])\n\n\n\nm = nn.Dropout2d(p=0.2)\ninput = torch.randn(20, 16, 32, 32)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 16, 32, 32])\n\n\n\nm = nn.AlphaDropout(p=0.2)\ninput = torch.randn(20, 16)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 16])\n\n\n\nm = nn.FeatureAlphaDropout(p=0.2)\ninput = torch.randn(20, 16, 4, 32, 32)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 16, 4, 32, 32])\n\n\n\n\nLoss Functions\n\n\nnn.L1Loss\n\ninput = torch.linspace(1,10,10, requires_grad=True)\ntarget = torch.linspace(1.1, 10.1, 10, requires_grad=True)\ninput, target\n\n(tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], requires_grad=True),\n tensor([ 1.1000,  2.1000,  3.1000,  4.1000,  5.1000,  6.1000,  7.1000,  8.1000,\n          9.1000, 10.1000], requires_grad=True))\n\n\n\nloss = nn.L1Loss()\noutput = loss(input, target)\noutput\n\ntensor(0.1000, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\n\nnn.MSELoss\n\nloss = nn.MSELoss()\noutput = loss(input, target)\noutput\n\ntensor(0.0100, grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\n\nnn.CrossEntropyLoss\n\n# Example of target with class indices\nloss = nn.CrossEntropyLoss()\noutput = loss(input, target)\noutput\n\ntensor(195.1833, grad_fn=&lt;DivBackward1&gt;)\n\n\n\nloss = nn.GaussianNLLLoss()\n\nvar = torch.ones(10, requires_grad=True)  # heteroscedastic\noutput = loss(input, target, var)\noutput\n\ntensor(0.0050, grad_fn=&lt;MeanBackward0&gt;)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#vision-layers",
    "href": "model_creation.html#vision-layers",
    "title": "Pytorch Model Creation",
    "section": "Vision Layers",
    "text": "Vision Layers\nRearrange elements in a tensor according to an upscaling factor.\n\npixel_shuffle = nn.PixelShuffle(3)\ninput = torch.randn(1, 9, 4, 4)\noutput = pixel_shuffle(input)\nprint(output.size())\n\ntorch.Size([1, 1, 12, 12])\n\n\n\npixel_unshuffle = nn.PixelUnshuffle(3)\ninput = torch.randn(1, 1, 12, 12)\noutput = pixel_unshuffle(input)\nprint(output.size())\n\ntorch.Size([1, 9, 4, 4])",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "Models_Types/fnn.html",
    "href": "Models_Types/fnn.html",
    "title": "FNN - Feedforward Neural Networks",
    "section": "",
    "text": "read through each famous models papers\nimplement it in code\n\nModels: - resnet18 - Unet - Yolo - Faster_RCNN - RNN - Gans - LSTM\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "FNN - Feedforward Neural Networks"
    ]
  },
  {
    "objectID": "Models_Types/gan.html",
    "href": "Models_Types/gan.html",
    "title": "GAN - Generative Adversarial Networks",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "GAN - Generative Adversarial Networks"
    ]
  },
  {
    "objectID": "Models_Types/autoencoders.html",
    "href": "Models_Types/autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "Autoencoders"
    ]
  },
  {
    "objectID": "Models_Types/rnn.html",
    "href": "Models_Types/rnn.html",
    "title": "RNN - Recurrent Neural Networks",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "RNN - Recurrent Neural Networks"
    ]
  },
  {
    "objectID": "Models_Types/transformers.html",
    "href": "Models_Types/transformers.html",
    "title": "Transformer Networks",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "Transformer Networks"
    ]
  },
  {
    "objectID": "Models_Types/Model Design/resnet18_with_fastai.html",
    "href": "Models_Types/Model Design/resnet18_with_fastai.html",
    "title": "Resnet 18 from FastAI",
    "section": "",
    "text": "import torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nfrom fastbook import *\nfrom fastai.vision.all import *\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\n\n\nfrom fastai.vision.all import *\n\n# Load CIFAR-10 dataset\npath = untar_data(URLs.CIFAR)\n\n# Define DataBlock\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   splitter=GrandparentSplitter(train_name='train', valid_name='test'),\n                   item_tfms=Resize(32))\n\n# Create DataLoaders\ndls = dblock.dataloaders(path, num_workers=4)\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\n\n# Create Learner\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.684787\n1.488868\n0.521500\n00:38\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.971654\n0.850866\n0.293200\n00:39\n\n\n1\n0.714961\n0.687398\n0.235600\n00:40\n\n\n2\n0.522026\n0.672022\n0.228400\n00:40\n\n\n\n\n\n\n# Create Learner\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.629048\n1.444631\n0.497000\n00:41\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.899234\n0.774175\n0.266900\n00:54\n\n\n1\n0.652826\n0.635258\n0.216300\n00:57\n\n\n2\n0.414785\n0.603115\n0.205500\n00:55\n\n\n\n\n\n\nlearn.model\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=10, bias=False)\n  )\n)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "Model Design",
      "Resnet 18 from FastAI"
    ]
  },
  {
    "objectID": "Models_Types/Model Design/simple_cnn_in_pytorch.html",
    "href": "Models_Types/Model Design/simple_cnn_in_pytorch.html",
    "title": "CNN",
    "section": "",
    "text": "import torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\n\n# Import dependencies\nimport torch \nfrom PIL import Image\nfrom torch import nn, save, load\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Get data \ntrain = datasets.MNIST(root=\"data\", download=True, train=True, transform=ToTensor())\ndataset = DataLoader(train, 32)\n#1,28,28 - classes 0-9\n\n# Image Classifier Neural Network\nclass ImageClassifier(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 32, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(64, 64, (3,3)), \n            nn.ReLU(),\n            nn.Flatten(), \n            nn.Linear(64*(28-6)*(28-6), 10)  \n        )\n\n    def forward(self, x): \n        return self.model(x)\n\n# Instance of the neural network, loss, optimizer \nclf = ImageClassifier().to('cuda')\nopt = Adam(clf.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n\n# Training flow \nfor epoch in range(10): # train for 10 epochs\n    for batch in tqdm(dataset): \n        X,y = batch \n        X, y = X.to('cuda'), y.to('cuda') \n        yhat = clf(X) \n        loss = loss_fn(yhat, y) \n\n        # Apply backprop \n        opt.zero_grad()\n        loss.backward() \n        opt.step() \n\n    print(f\"Epoch:{epoch} loss is {loss.item()}\")\n\nwith open('data/model_state.pt', 'wb') as f: \n    save(clf.state_dict(), f)\n\n\n\n\nEpoch:0 loss is 0.02930162288248539\n\n\n\n\n\nEpoch:1 loss is 0.015122702345252037\n\n\n\n\n\nEpoch:2 loss is 0.0010034267324954271\n\n\n\n\n\nEpoch:3 loss is 0.0003837654658127576\n\n\n\n\n\nEpoch:4 loss is 4.9363912694389e-05\n\n\n\n\n\nEpoch:5 loss is 0.00015089042426552624\n\n\n\n\n\nEpoch:6 loss is 3.805131927947514e-05\n\n\n\n\n\nEpoch:7 loss is 3.0979390430729836e-05\n\n\n\n\n\nEpoch:8 loss is 7.227040441648569e-07\n\n\n\n\n\nEpoch:9 loss is 8.898725354811177e-06\ntensor(9, device='cuda:0')\n\n\n\nwith open('data/model_state.pt', 'rb') as f: \n    clf.load_state_dict(load(f))  \n\nimg = Image.open('data/img_3.jpg') \nplt.imshow(img, cmap = 'gray')\nimg_tensor = ToTensor()(img).unsqueeze(0).to('cuda')\n\nprint(torch.argmax(clf(img_tensor)))\n\ntensor(9, device='cuda:0')\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Models Types",
      "Model Design",
      "CNN"
    ]
  },
  {
    "objectID": "tensorflow.html",
    "href": "tensorflow.html",
    "title": "TensorFlow",
    "section": "",
    "text": "# Load the TensorBoard notebook extension\n\n\n\n\nReusing TensorBoard on port 6006 (pid 431396), started 0:35:57 ago. (Use '!kill 431396' to kill it.)\n\n\n\n      \n      \n      \n    \n\n\n\nfrom tensorboard import notebook\nnotebook.list() # View open TensorBoard instances\n\nNo known TensorBoard instances running.\n\n\n\nnotebook.display(port=6006, height=1000)\n\n\n      \n      \n      \n    \n\n\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n############## TENSORBOARD ########################\nimport sys\nfrom torch.utils.tensorboard import SummaryWriter\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter()\n###################################################\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \ninput_size = 784 # 28x28\nhidden_size = 500 \nnum_classes = 10\nnum_epochs = 1\nbatch_size = 64\nlearning_rate = 0.001\n\n# MNIST dataset \ntrain_dataset = torchvision.datasets.MNIST(root='./Data', \n                                           train=True, \n                                           transform=transforms.ToTensor(),  \n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='./Data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\nexamples = iter(test_loader)\nexample_data, example_targets = next(examples)\n\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    plt.imshow(example_data[i][0], cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n############## TENSORBOARD ########################\nimg_grid = torchvision.utils.make_grid(example_data)\nimg_grid\n\nwriter.add_image('mnist_images', img_grid)\nwriter.flush()\n#sys.exit()\n###################################################\n\n\n# Fully connected neural network with one hidden layer\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.input_size = input_size\n        self.l1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        # no activation and no softmax at the end\n        return out\n\n\nimport timm\n\n\n# Load ResNet model without the final classification layer\nmodel = timm.create_model('resnet18', pretrained=True, num_classes=10)\n# Modify the first convolution layer to accept single-channel images\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\nmodel = model.to(device)\n\n# model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n############## TENSORBOARD ########################\n# writer.add_graph(model, example_data.reshape(-1, 28*28).to(device))\nwriter.add_graph(model, example_data.to(device))\nwriter.flush()\n#sys.exit()\n###################################################\n\n\n# Train the model\nrunning_loss = 0.0\nrunning_correct = 0\nn_total_steps = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # origin shape: [100, 1, 28, 28]\n        # resized: [100, 784]\n        # images = images.reshape(-1, 28*28).to(device)\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n\n        _, predicted = torch.max(outputs.data, 1)\n        running_correct += (predicted == labels).sum().item()\n        if (i+1) % 100 == 0:\n            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n            ############## TENSORBOARD ########################\n            writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n            running_accuracy = running_correct / 100 / predicted.size(0)\n            writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n            running_correct = 0\n            running_loss = 0.0\n            writer.flush()\n            ###################################################\n\nEpoch [1/1], Step [100/938], Loss: 0.4240\nEpoch [1/1], Step [200/938], Loss: 0.2560\nEpoch [1/1], Step [300/938], Loss: 0.1293\nEpoch [1/1], Step [400/938], Loss: 0.1558\nEpoch [1/1], Step [500/938], Loss: 0.1048\nEpoch [1/1], Step [600/938], Loss: 0.0294\nEpoch [1/1], Step [700/938], Loss: 0.1048\nEpoch [1/1], Step [800/938], Loss: 0.1394\nEpoch [1/1], Step [900/938], Loss: 0.0257\n\n\n\n# Test the model\n# In test phase, we don't need to compute gradients (for memory efficiency)\nclass_labels = []\nclass_preds = []\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for images, labels in test_loader:\n        # images = images.reshape(-1, 28*28).to(device)\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        # max returns (value ,index)\n        values, predicted = torch.max(outputs.data, 1)\n        n_samples += labels.size(0)\n        n_correct += (predicted == labels).sum().item()\n\n        class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n\n        class_preds.append(class_probs_batch)\n        class_labels.append(labels)\n\n    # 10000, 10, and 10000, 1\n    # stack concatenates tensors along a new dimension\n    # cat concatenates tensors in the given dimension\n    class_preds = torch.cat([torch.stack(batch) for batch in class_preds])\n    class_labels = torch.cat(class_labels)\n\n    acc = 100.0 * n_correct / n_samples\n    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n\n    ############## TENSORBOARD ########################\n    classes = range(10)\n    for i in classes:\n        labels_i = class_labels == i\n        preds_i = class_preds[:, i]\n        writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0)\n        writer.flush()\n    ###################################################\n\nAccuracy of the network on the 10000 test images: 97.53 %\n\n\n\nwriter.close()\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "TensorFlow"
    ]
  },
  {
    "objectID": "timm_models_torch.html",
    "href": "timm_models_torch.html",
    "title": "Timm - Loading Models",
    "section": "",
    "text": "!pip list | grep timm\n\ntimm                      1.0.3\n\n\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ntimm??\n\n\nType:        module\nString form: &lt;module 'timm' from '/home/ben/miniconda3/envs/pfast/lib/python3.12/site-packages/timm/__init__.py'&gt;\nFile:        ~/miniconda3/envs/pfast/lib/python3.12/site-packages/timm/__init__.py\nSource:     \nfrom .version import __version__\nfrom .layers import is_scriptable, is_exportable, set_scriptable, set_exportable\nfrom .models import create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n    is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#using-timm",
    "href": "timm_models_torch.html#using-timm",
    "title": "Timm - Loading Models",
    "section": "",
    "text": "!pip list | grep timm\n\ntimm                      1.0.3\n\n\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ntimm??\n\n\nType:        module\nString form: &lt;module 'timm' from '/home/ben/miniconda3/envs/pfast/lib/python3.12/site-packages/timm/__init__.py'&gt;\nFile:        ~/miniconda3/envs/pfast/lib/python3.12/site-packages/timm/__init__.py\nSource:     \nfrom .version import __version__\nfrom .layers import is_scriptable, is_exportable, set_scriptable, set_exportable\nfrom .models import create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n    is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#list-models",
    "href": "timm_models_torch.html#list-models",
    "title": "Timm - Loading Models",
    "section": "List Models",
    "text": "List Models\n\nprint(timm.list_models(\"resnet*\"))\n\n['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34', 'resnet34d', 'resnet50', 'resnet50_gn', 'resnet50c', 'resnet50d', 'resnet50s', 'resnet50t', 'resnet51q', 'resnet61q', 'resnet101', 'resnet101c', 'resnet101d', 'resnet101s', 'resnet152', 'resnet152c', 'resnet152d', 'resnet152s', 'resnet200', 'resnet200d', 'resnetaa34d', 'resnetaa50', 'resnetaa50d', 'resnetaa101d', 'resnetblur18', 'resnetblur50', 'resnetblur50d', 'resnetblur101d', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_50', 'resnetv2_50d', 'resnetv2_50d_evos', 'resnetv2_50d_frn', 'resnetv2_50d_gn', 'resnetv2_50t', 'resnetv2_50x1_bit', 'resnetv2_50x3_bit', 'resnetv2_101', 'resnetv2_101d', 'resnetv2_101x1_bit', 'resnetv2_101x3_bit', 'resnetv2_152', 'resnetv2_152d', 'resnetv2_152x2_bit', 'resnetv2_152x4_bit']\n\n\n\nlen(timm.list_models())\n\n1063",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#create-model",
    "href": "timm_models_torch.html#create-model",
    "title": "Timm - Loading Models",
    "section": "Create Model",
    "text": "Create Model\n\nmodel = timm.create_model('resnet18', pretrained=True)",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#list-pretrained",
    "href": "timm_models_torch.html#list-pretrained",
    "title": "Timm - Loading Models",
    "section": "List Pretrained",
    "text": "List Pretrained\n\nprint(timm.list_pretrained(\"resnet*\"))\n\n['resnet10t.c3_in1k', 'resnet14t.c3_in1k', 'resnet18.a1_in1k', 'resnet18.a2_in1k', 'resnet18.a3_in1k', 'resnet18.fb_ssl_yfcc100m_ft_in1k', 'resnet18.fb_swsl_ig1b_ft_in1k', 'resnet18.gluon_in1k', 'resnet18.tv_in1k', 'resnet18d.ra2_in1k', 'resnet26.bt_in1k', 'resnet26d.bt_in1k', 'resnet26t.ra2_in1k', 'resnet32ts.ra2_in1k', 'resnet33ts.ra2_in1k', 'resnet34.a1_in1k', 'resnet34.a2_in1k', 'resnet34.a3_in1k', 'resnet34.bt_in1k', 'resnet34.gluon_in1k', 'resnet34.tv_in1k', 'resnet34d.ra2_in1k', 'resnet50.a1_in1k', 'resnet50.a1h_in1k', 'resnet50.a2_in1k', 'resnet50.a3_in1k', 'resnet50.am_in1k', 'resnet50.b1k_in1k', 'resnet50.b2k_in1k', 'resnet50.bt_in1k', 'resnet50.c1_in1k', 'resnet50.c2_in1k', 'resnet50.d_in1k', 'resnet50.fb_ssl_yfcc100m_ft_in1k', 'resnet50.fb_swsl_ig1b_ft_in1k', 'resnet50.gluon_in1k', 'resnet50.ra_in1k', 'resnet50.ram_in1k', 'resnet50.tv2_in1k', 'resnet50.tv_in1k', 'resnet50_gn.a1h_in1k', 'resnet50c.gluon_in1k', 'resnet50d.a1_in1k', 'resnet50d.a2_in1k', 'resnet50d.a3_in1k', 'resnet50d.gluon_in1k', 'resnet50d.ra2_in1k', 'resnet50s.gluon_in1k', 'resnet51q.ra2_in1k', 'resnet61q.ra2_in1k', 'resnet101.a1_in1k', 'resnet101.a1h_in1k', 'resnet101.a2_in1k', 'resnet101.a3_in1k', 'resnet101.gluon_in1k', 'resnet101.tv2_in1k', 'resnet101.tv_in1k', 'resnet101c.gluon_in1k', 'resnet101d.gluon_in1k', 'resnet101d.ra2_in1k', 'resnet101s.gluon_in1k', 'resnet152.a1_in1k', 'resnet152.a1h_in1k', 'resnet152.a2_in1k', 'resnet152.a3_in1k', 'resnet152.gluon_in1k', 'resnet152.tv2_in1k', 'resnet152.tv_in1k', 'resnet152c.gluon_in1k', 'resnet152d.gluon_in1k', 'resnet152d.ra2_in1k', 'resnet152s.gluon_in1k', 'resnet200d.ra2_in1k', 'resnetaa50.a1h_in1k', 'resnetaa50d.d_in12k', 'resnetaa50d.sw_in12k', 'resnetaa50d.sw_in12k_ft_in1k', 'resnetaa101d.sw_in12k', 'resnetaa101d.sw_in12k_ft_in1k', 'resnetblur50.bt_in1k', 'resnetrs50.tf_in1k', 'resnetrs101.tf_in1k', 'resnetrs152.tf_in1k', 'resnetrs200.tf_in1k', 'resnetrs270.tf_in1k', 'resnetrs350.tf_in1k', 'resnetrs420.tf_in1k', 'resnetv2_50.a1h_in1k', 'resnetv2_50d_evos.ah_in1k', 'resnetv2_50d_gn.ah_in1k', 'resnetv2_50x1_bit.goog_distilled_in1k', 'resnetv2_50x1_bit.goog_in21k', 'resnetv2_50x1_bit.goog_in21k_ft_in1k', 'resnetv2_50x3_bit.goog_in21k', 'resnetv2_50x3_bit.goog_in21k_ft_in1k', 'resnetv2_101.a1h_in1k', 'resnetv2_101x1_bit.goog_in21k', 'resnetv2_101x1_bit.goog_in21k_ft_in1k', 'resnetv2_101x3_bit.goog_in21k', 'resnetv2_101x3_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_in21k', 'resnetv2_152x2_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384', 'resnetv2_152x4_bit.goog_in21k', 'resnetv2_152x4_bit.goog_in21k_ft_in1k']",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#using-torchvision-models",
    "href": "timm_models_torch.html#using-torchvision-models",
    "title": "Timm - Loading Models",
    "section": "Using torchvision models",
    "text": "Using torchvision models\n\nimport torchvision.models\n\n\nprint(torchvision.models.list_models())\n\n['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'deeplabv3_mobilenet_v3_large', 'deeplabv3_resnet101', 'deeplabv3_resnet50', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'fasterrcnn_mobilenet_v3_large_320_fpn', 'fasterrcnn_mobilenet_v3_large_fpn', 'fasterrcnn_resnet50_fpn', 'fasterrcnn_resnet50_fpn_v2', 'fcn_resnet101', 'fcn_resnet50', 'fcos_resnet50_fpn', 'googlenet', 'inception_v3', 'keypointrcnn_resnet50_fpn', 'lraspp_mobilenet_v3_large', 'maskrcnn_resnet50_fpn', 'maskrcnn_resnet50_fpn_v2', 'maxvit_t', 'mc3_18', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'mvit_v1_b', 'mvit_v2_s', 'quantized_googlenet', 'quantized_inception_v3', 'quantized_mobilenet_v2', 'quantized_mobilenet_v3_large', 'quantized_resnet18', 'quantized_resnet50', 'quantized_resnext101_32x8d', 'quantized_resnext101_64x4d', 'quantized_shufflenet_v2_x0_5', 'quantized_shufflenet_v2_x1_0', 'quantized_shufflenet_v2_x1_5', 'quantized_shufflenet_v2_x2_0', 'r2plus1d_18', 'r3d_18', 'raft_large', 'raft_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'retinanet_resnet50_fpn', 'retinanet_resnet50_fpn_v2', 's3d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'ssd300_vgg16', 'ssdlite320_mobilenet_v3_large', 'swin3d_b', 'swin3d_s', 'swin3d_t', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#model-summary",
    "href": "timm_models_torch.html#model-summary",
    "title": "Timm - Loading Models",
    "section": "Model Summary",
    "text": "Model Summary\n\nfrom torchinfo import summary\nimport numpy as np\n\n\ntimm.create_model?\n\n\nmodel = timm.create_model('resnet50', pretrained=True, num_classes=10)\nrandom_input = np.random.rand(1, 3, 100, 100)\n\n\n\n\n\nmodel\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Linear(in_features=2048, out_features=10, bias=True)\n)\n\n\n\nsummary(model, input_size=random_input.shape, verbose = 0, depth  = 5, col_names = (\n                \"input_size\",\n                \"output_size\",\n                \"num_params\",\n                \"params_percent\",\n                \"kernel_size\",\n                \"mult_adds\",\n                \"trainable\",\n            ))\n\n=======================================================================================================================================================================================================================\nLayer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n=======================================================================================================================================================================================================================\nResNet                                   [1, 3, 100, 100]          [1, 10]                   --                             --                   --                        --                        True\nâ”œâ”€Conv2d: 1-1                            [1, 3, 100, 100]          [1, 64, 50, 50]           9,408                       0.04%                   [7, 7]                    23,520,000                True\nâ”œâ”€BatchNorm2d: 1-2                       [1, 64, 50, 50]           [1, 64, 50, 50]           128                         0.00%                   --                        128                       True\nâ”œâ”€ReLU: 1-3                              [1, 64, 50, 50]           [1, 64, 50, 50]           --                             --                   --                        --                        --\nâ”œâ”€MaxPool2d: 1-4                         [1, 64, 50, 50]           [1, 64, 25, 25]           --                             --                   3                         --                        --\nâ”œâ”€Sequential: 1-5                        [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â””â”€Bottleneck: 2-1                   [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-1                  [1, 64, 25, 25]           [1, 64, 25, 25]           4,096                       0.02%                   [1, 1]                    2,560,000                 True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-2             [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\nâ”‚    â”‚    â””â”€ReLU: 3-3                    [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-4                  [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-5             [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\nâ”‚    â”‚    â””â”€Identity: 3-6                [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-7                    [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-8                [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-9                  [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-10            [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€Sequential: 3-11             [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â””â”€Conv2d: 4-1             [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â”‚    â””â”€BatchNorm2d: 4-2        [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-12                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-2                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-13                 [1, 256, 25, 25]          [1, 64, 25, 25]           16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-14            [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\nâ”‚    â”‚    â””â”€ReLU: 3-15                   [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-16                 [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-17            [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\nâ”‚    â”‚    â””â”€Identity: 3-18               [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-19                   [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-20               [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-21                 [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-22            [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-23                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-3                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-24                 [1, 256, 25, 25]          [1, 64, 25, 25]           16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-25            [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\nâ”‚    â”‚    â””â”€ReLU: 3-26                   [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-27                 [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-28            [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\nâ”‚    â”‚    â””â”€Identity: 3-29               [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-30                   [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-31               [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-32                 [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-33            [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-34                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\nâ”œâ”€Sequential: 1-6                        [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â””â”€Bottleneck: 2-4                   [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-35                 [1, 256, 25, 25]          [1, 128, 25, 25]          32,768                      0.14%                   [1, 1]                    20,480,000                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-36            [1, 128, 25, 25]          [1, 128, 25, 25]          256                         0.00%                   --                        256                       True\nâ”‚    â”‚    â””â”€ReLU: 3-37                   [1, 128, 25, 25]          [1, 128, 25, 25]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-38                 [1, 128, 25, 25]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-39            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\nâ”‚    â”‚    â””â”€Identity: 3-40               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-41                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-42               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-43                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-44            [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€Sequential: 3-45             [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â””â”€Conv2d: 4-3             [1, 256, 25, 25]          [1, 512, 13, 13]          131,072                     0.56%                   [1, 1]                    22,151,168                True\nâ”‚    â”‚    â”‚    â””â”€BatchNorm2d: 4-4        [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€ReLU: 3-46                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-5                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-47                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-48            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\nâ”‚    â”‚    â””â”€ReLU: 3-49                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-50                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-51            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\nâ”‚    â”‚    â””â”€Identity: 3-52               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-53                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-54               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-55                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-56            [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€ReLU: 3-57                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-6                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-58                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-59            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\nâ”‚    â”‚    â””â”€ReLU: 3-60                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-61                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-62            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\nâ”‚    â”‚    â””â”€Identity: 3-63               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-64                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-65               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-66                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-67            [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€ReLU: 3-68                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-7                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-69                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-70            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\nâ”‚    â”‚    â””â”€ReLU: 3-71                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-72                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-73            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\nâ”‚    â”‚    â””â”€Identity: 3-74               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-75                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-76               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-77                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-78            [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€ReLU: 3-79                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\nâ”œâ”€Sequential: 1-7                        [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â””â”€Bottleneck: 2-8                   [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-80                 [1, 512, 13, 13]          [1, 256, 13, 13]          131,072                     0.56%                   [1, 1]                    22,151,168                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-81            [1, 256, 13, 13]          [1, 256, 13, 13]          512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-82                   [1, 256, 13, 13]          [1, 256, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-83                 [1, 256, 13, 13]          [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-84            [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€Identity: 3-85               [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-86                   [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-87               [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-88                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-89            [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\nâ”‚    â”‚    â””â”€Sequential: 3-90             [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â””â”€Conv2d: 4-5             [1, 512, 13, 13]          [1, 1024, 7, 7]           524,288                     2.23%                   [1, 1]                    25,690,112                True\nâ”‚    â”‚    â”‚    â””â”€BatchNorm2d: 4-6        [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\nâ”‚    â”‚    â””â”€ReLU: 3-91                   [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-9                   [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-92                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-93            [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-94                   [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-95                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-96            [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€Identity: 3-97               [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-98                   [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-99               [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-100                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-101           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\nâ”‚    â”‚    â””â”€ReLU: 3-102                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-10                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-103                [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-104           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-105                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-106                [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-107           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€Identity: 3-108              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-109                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-110              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-111                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-112           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\nâ”‚    â”‚    â””â”€ReLU: 3-113                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-11                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-114                [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-115           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-116                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-117                [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-118           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€Identity: 3-119              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-120                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-121              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-122                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-123           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\nâ”‚    â”‚    â””â”€ReLU: 3-124                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-12                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-125                [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-126           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-127                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-128                [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-129           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€Identity: 3-130              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-131                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-132              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-133                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-134           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\nâ”‚    â”‚    â””â”€ReLU: 3-135                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-13                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-136                [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-137           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€ReLU: 3-138                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-139                [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-140           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\nâ”‚    â”‚    â””â”€Identity: 3-141              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-142                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-143              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-144                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-145           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\nâ”‚    â”‚    â””â”€ReLU: 3-146                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”œâ”€Sequential: 1-8                        [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â””â”€Bottleneck: 2-14                  [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-147                [1, 1024, 7, 7]           [1, 512, 7, 7]            524,288                     2.23%                   [1, 1]                    25,690,112                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-148           [1, 512, 7, 7]            [1, 512, 7, 7]            1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€ReLU: 3-149                  [1, 512, 7, 7]            [1, 512, 7, 7]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-150                [1, 512, 7, 7]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-151           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€Identity: 3-152              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-153                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-154              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-155                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-156           [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        4,096                     True\nâ”‚    â”‚    â””â”€Sequential: 3-157            [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â””â”€Conv2d: 4-7             [1, 1024, 7, 7]           [1, 2048, 4, 4]           2,097,152                   8.91%                   [1, 1]                    33,554,432                True\nâ”‚    â”‚    â”‚    â””â”€BatchNorm2d: 4-8        [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        4,096                     True\nâ”‚    â”‚    â””â”€ReLU: 3-158                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-15                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-159                [1, 2048, 4, 4]           [1, 512, 4, 4]            1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-160           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€ReLU: 3-161                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-162                [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-163           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€Identity: 3-164              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-165                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-166              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-167                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-168           [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        4,096                     True\nâ”‚    â”‚    â””â”€ReLU: 3-169                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\nâ”‚    â””â”€Bottleneck: 2-16                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Conv2d: 3-170                [1, 2048, 4, 4]           [1, 512, 4, 4]            1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-171           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€ReLU: 3-172                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-173                [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-174           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\nâ”‚    â”‚    â””â”€Identity: 3-175              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€ReLU: 3-176                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-177              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Conv2d: 3-178                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â””â”€BatchNorm2d: 3-179           [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        4,096                     True\nâ”‚    â”‚    â””â”€ReLU: 3-180                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\nâ”œâ”€SelectAdaptivePool2d: 1-9              [1, 2048, 4, 4]           [1, 2048]                 --                             --                   --                        --                        --\nâ”‚    â””â”€AdaptiveAvgPool2d: 2-17           [1, 2048, 4, 4]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\nâ”‚    â””â”€Flatten: 2-18                     [1, 2048, 1, 1]           [1, 2048]                 --                             --                   --                        --                        --\nâ”œâ”€Linear: 1-10                           [1, 2048]                 [1, 10]                   20,490                      0.09%                   --                        20,490                    True\n=======================================================================================================================================================================================================================\nTotal params: 23,528,522\nTrainable params: 23,528,522\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 995.48\n=======================================================================================================================================================================================================================\nInput size (MB): 0.12\nForward/backward pass size (MB): 38.94\nParams size (MB): 94.11\nEstimated Total Size (MB): 133.17\n=======================================================================================================================================================================================================================\n\n\n\nmodel = timm.create_model('resnetv2_50', pretrained=True, num_classes=10)\nrandom_input = np.random.rand(1, 3, 100, 100)\n\nsummary(model, input_size=random_input.shape, verbose = 0, depth  = 5, col_names = (\n                \"input_size\",\n                \"output_size\",\n                \"num_params\",\n                \"params_percent\",\n                \"kernel_size\",\n                \"mult_adds\",\n                \"trainable\",\n            ))\n\n\n\n\n=================================================================================================================================================================================================================================\nLayer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n=================================================================================================================================================================================================================================\nResNetV2                                           [1, 3, 100, 100]          [1, 10]                   --                             --                   --                        --                        True\nâ”œâ”€Sequential: 1-1                                  [1, 3, 100, 100]          [1, 64, 25, 25]           --                             --                   --                        --                        True\nâ”‚    â””â”€Conv2d: 2-1                                 [1, 3, 100, 100]          [1, 64, 50, 50]           9,408                       0.04%                   [7, 7]                    23,520,000                True\nâ”‚    â””â”€MaxPool2d: 2-2                              [1, 64, 50, 50]           [1, 64, 25, 25]           --                             --                   3                         --                        --\nâ”œâ”€Sequential: 1-2                                  [1, 64, 25, 25]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â””â”€ResNetStage: 2-3                            [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Sequential: 3-1                        [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-1             [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-1          [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€DownsampleConv: 5-2          [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   --                        10,240,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-3                  [1, 64, 25, 25]           [1, 64, 25, 25]           4,096                       0.02%                   [1, 1]                    2,560,000                 True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-4          [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-5                  [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-6          [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-7                  [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-8                [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-2             [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-9          [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-10                 [1, 256, 25, 25]          [1, 64, 25, 25]           16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-11         [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-12                 [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-13         [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-14                 [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-15               [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-3             [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-16         [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-17                 [1, 256, 25, 25]          [1, 64, 25, 25]           16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-18         [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-19                 [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-20         [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-21                 [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-22               [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\nâ”‚    â””â”€ResNetStage: 2-4                            [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Sequential: 3-2                        [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-4             [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-23         [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€DownsampleConv: 5-24         [1, 256, 25, 25]          [1, 512, 13, 13]          131,072                     0.56%                   --                        22,151,168                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-25                 [1, 256, 25, 25]          [1, 128, 25, 25]          32,768                      0.14%                   [1, 1]                    20,480,000                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-26         [1, 128, 25, 25]          [1, 128, 25, 25]          256                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-27                 [1, 128, 25, 25]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-28         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-29                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-30               [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-5             [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-31         [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-32                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-33         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-34                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-35         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-36                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-37               [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-6             [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-38         [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-39                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-40         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-41                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-42         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-43                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-44               [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-7             [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-45         [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-46                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-47         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-48                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-49         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-50                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-51               [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\nâ”‚    â””â”€ResNetStage: 2-5                            [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Sequential: 3-3                        [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-8             [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-52         [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€DownsampleConv: 5-53         [1, 512, 13, 13]          [1, 1024, 7, 7]           524,288                     2.23%                   --                        25,690,112                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-54                 [1, 512, 13, 13]          [1, 256, 13, 13]          131,072                     0.56%                   [1, 1]                    22,151,168                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-55         [1, 256, 13, 13]          [1, 256, 13, 13]          512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-56                 [1, 256, 13, 13]          [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-57         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-58                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-59               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-9             [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-60         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-61                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-62         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-63                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-64         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-65                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-66               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-10            [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-67         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-68                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-69         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-70                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-71         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-72                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-73               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-11            [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-74         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-75                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-76         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-77                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-78         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-79                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-80               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-12            [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-81         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-82                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-83         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-84                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-85         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-86                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-87               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-13            [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-88         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-89                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-90         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-91                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-92         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-93                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-94               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\nâ”‚    â””â”€ResNetStage: 2-6                            [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â””â”€Sequential: 3-4                        [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-14            [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-95         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€DownsampleConv: 5-96         [1, 1024, 7, 7]           [1, 2048, 4, 4]           2,097,152                   8.92%                   --                        33,554,432                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-97                 [1, 1024, 7, 7]           [1, 512, 7, 7]            524,288                     2.23%                   [1, 1]                    25,690,112                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-98         [1, 512, 7, 7]            [1, 512, 7, 7]            1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-99                 [1, 512, 7, 7]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-100        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-101                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-102              [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-15            [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-103        [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-104                [1, 2048, 4, 4]           [1, 512, 4, 4]            1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-105        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-106                [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-107        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-108                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-109              [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\nâ”‚    â”‚    â”‚    â””â”€PreActBottleneck: 4-16            [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-110        [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-111                [1, 2048, 4, 4]           [1, 512, 4, 4]            1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-112        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-113                [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€BatchNormAct2d: 5-114        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Conv2d: 5-115                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\nâ”‚    â”‚    â”‚    â”‚    â””â”€Identity: 5-116              [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\nâ”œâ”€BatchNormAct2d: 1-3                              [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        --                        True\nâ”‚    â””â”€Identity: 2-7                               [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\nâ”‚    â””â”€ReLU: 2-8                                   [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\nâ”œâ”€ClassifierHead: 1-4                              [1, 2048, 4, 4]           [1, 10]                   --                             --                   --                        --                        True\nâ”‚    â””â”€SelectAdaptivePool2d: 2-9                   [1, 2048, 4, 4]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€AdaptiveAvgPool2d: 3-5                 [1, 2048, 4, 4]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\nâ”‚    â”‚    â””â”€Identity: 3-6                          [1, 2048, 1, 1]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\nâ”‚    â””â”€Dropout: 2-10                               [1, 2048, 1, 1]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\nâ”‚    â””â”€Conv2d: 2-11                                [1, 2048, 1, 1]           [1, 10, 1, 1]             20,490                      0.09%                   [1, 1]                    20,490                    True\nâ”‚    â””â”€Flatten: 2-12                               [1, 10, 1, 1]             [1, 10]                   --                             --                   --                        --                        --\n=================================================================================================================================================================================================================================\nTotal params: 23,520,842\nTrainable params: 23,520,842\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 995.42\n=================================================================================================================================================================================================================================\nInput size (MB): 0.12\nForward/backward pass size (MB): 19.47\nParams size (MB): 93.90\nEstimated Total Size (MB): 113.49\n=================================================================================================================================================================================================================================",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#detection-models",
    "href": "timm_models_torch.html#detection-models",
    "title": "Timm - Loading Models",
    "section": "Detection models",
    "text": "Detection models\n\nfrom torchvision.models import detection \nimport timm\n\n\n############## TENSORBOARD ########################\nimport sys\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter()\n###################################################\n\n\n# Load ResNet model without the final classification layer\nmodel = timm.create_model('resnetv2_50', pretrained=True, num_classes=10)\n\n# weights = detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n# model = detection.fasterrcnn_resnet50_fpn_v2(weights=weights,\n#                                     box_score_thresh=0.9).train()\n\n\nimport torch\n\n# Generate random input tensor\ninput_tensor = torch.randn(1, 3, 224, 224)",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#onnx",
    "href": "timm_models_torch.html#onnx",
    "title": "Timm - Loading Models",
    "section": "ONNX",
    "text": "ONNX\n\ntorch.onnx.export(model, input_tensor, 'Data/resnet50_v2.onnx', input_names=[\"features\"], output_names=[\"logits\"])\n\n\n############## TENSORBOARD ########################\n# writer.add_graph(model, example_data.reshape(-1, 28*28).to(device))\n\nwriter.add_graph(model, input_tensor)\nwriter.flush()\nwriter.close()\n###################################################\n\n\nlist(model.children())[:]\n\n[Sequential(\n   (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n   (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n ),\n Sequential(\n   (0): ResNetStage(\n     (blocks): Sequential(\n       (0): PreActBottleneck(\n         (downsample): DownsampleConv(\n           (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n           (norm): Identity()\n         )\n         (norm1): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (1): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (2): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n     )\n   )\n   (1): ResNetStage(\n     (blocks): Sequential(\n       (0): PreActBottleneck(\n         (downsample): DownsampleConv(\n           (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n           (norm): Identity()\n         )\n         (norm1): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (1): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (2): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (3): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n     )\n   )\n   (2): ResNetStage(\n     (blocks): Sequential(\n       (0): PreActBottleneck(\n         (downsample): DownsampleConv(\n           (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n           (norm): Identity()\n         )\n         (norm1): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (1): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (2): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (3): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (4): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (5): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n     )\n   )\n   (3): ResNetStage(\n     (blocks): Sequential(\n       (0): PreActBottleneck(\n         (downsample): DownsampleConv(\n           (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n           (norm): Identity()\n         )\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (1): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (2): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n     )\n   )\n ),\n BatchNormAct2d(\n   2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n   (drop): Identity()\n   (act): ReLU(inplace=True)\n ),\n ClassifierHead(\n   (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n   (drop): Dropout(p=0.0, inplace=False)\n   (fc): Conv2d(2048, 10, kernel_size=(1, 1), stride=(1, 1))\n   (flatten): Flatten(start_dim=1, end_dim=-1)\n )]",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "Transforms",
    "section": "",
    "text": "import torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass WineDataset(Dataset):\n    def __init__(self):\n        #data loading\n        xy = np.loadtxt('Data/wine.csv', delimiter=\",\", dtype=np.float32, skiprows = 1)\n        self.xy = xy\n        self.x = torch.from_numpy(xy[:,1:])\n        self.y = torch.from_numpy(xy[:,[0]])\n        self.n_samples = xy.shape[0]\n\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        return self.n_samples\ndataset = WineDataset()\nfirst_data = dataset[0]\nfirst_data\n\n(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n         1.0650e+03]),\n tensor([1.]))\nfeatures, labels = dataset[0]\nfeatures, labels\n\n(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n         1.0650e+03]),\n tensor([1.]))\ndataloader = DataLoader(dataset=dataset, batch_size = 4, shuffle = True, num_workers = 2)\ndataiter = iter(dataloader)\nnext(dataiter)\n\n[tensor([[1.2250e+01, 1.7300e+00, 2.1200e+00, 1.9000e+01, 8.0000e+01, 1.6500e+00,\n          2.0300e+00, 3.7000e-01, 1.6300e+00, 3.4000e+00, 1.0000e+00, 3.1700e+00,\n          5.1000e+02],\n         [1.2420e+01, 1.6100e+00, 2.1900e+00, 2.2500e+01, 1.0800e+02, 2.0000e+00,\n          2.0900e+00, 3.4000e-01, 1.6100e+00, 2.0600e+00, 1.0600e+00, 2.9600e+00,\n          3.4500e+02],\n         [1.2370e+01, 1.1300e+00, 2.1600e+00, 1.9000e+01, 8.7000e+01, 3.5000e+00,\n          3.1000e+00, 1.9000e-01, 1.8700e+00, 4.4500e+00, 1.2200e+00, 2.8700e+00,\n          4.2000e+02],\n         [1.3830e+01, 1.6500e+00, 2.6000e+00, 1.7200e+01, 9.4000e+01, 2.4500e+00,\n          2.9900e+00, 2.2000e-01, 2.2900e+00, 5.6000e+00, 1.2400e+00, 3.3700e+00,\n          1.2650e+03]]),\n tensor([[2.],\n         [2.],\n         [2.],\n         [1.]])]\ndataiter = iter(dataloader)\ndata = next(dataiter)\nfeatures, labels = data\nfeatures, labels\n\n(tensor([[1.3720e+01, 1.4300e+00, 2.5000e+00, 1.6700e+01, 1.0800e+02, 3.4000e+00,\n          3.6700e+00, 1.9000e-01, 2.0400e+00, 6.8000e+00, 8.9000e-01, 2.8700e+00,\n          1.2850e+03],\n         [1.1840e+01, 2.8900e+00, 2.2300e+00, 1.8000e+01, 1.1200e+02, 1.7200e+00,\n          1.3200e+00, 4.3000e-01, 9.5000e-01, 2.6500e+00, 9.6000e-01, 2.5200e+00,\n          5.0000e+02],\n         [1.3860e+01, 1.5100e+00, 2.6700e+00, 2.5000e+01, 8.6000e+01, 2.9500e+00,\n          2.8600e+00, 2.1000e-01, 1.8700e+00, 3.3800e+00, 1.3600e+00, 3.1600e+00,\n          4.1000e+02],\n         [1.4120e+01, 1.4800e+00, 2.3200e+00, 1.6800e+01, 9.5000e+01, 2.2000e+00,\n          2.4300e+00, 2.6000e-01, 1.5700e+00, 5.0000e+00, 1.1700e+00, 2.8200e+00,\n          1.2800e+03]]),\n tensor([[1.],\n         [2.],\n         [2.],\n         [1.]]))\nnum_epochs = 2\ntotal_samples = len(dataset)\nn_iterations = int(np.ceil(total_samples/4))\n\ntotal_samples, n_iterations\n\n(178, 45)\nfor epoch in range(num_epochs):\n    for i, (inputs, labels) in enumerate(dataloader):\n        if (i + 1) % 5 == 0:\n            print(f'epoch {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs:{inputs[0][:5]} labels:{labels[0]}')\n\nepoch 1/2, step 5/45, inputs:tensor([13.3600,  2.5600,  2.3500, 20.0000, 89.0000]) labels:tensor([3.])\nepoch 1/2, step 10/45, inputs:tensor([ 13.2900,   1.9700,   2.6800,  16.8000, 102.0000]) labels:tensor([1.])\nepoch 1/2, step 15/45, inputs:tensor([14.1600,  2.5100,  2.4800, 20.0000, 91.0000]) labels:tensor([3.])\nepoch 1/2, step 20/45, inputs:tensor([ 13.9400,   1.7300,   2.2700,  17.4000, 108.0000]) labels:tensor([1.])\nepoch 1/2, step 25/45, inputs:tensor([12.6000,  1.3400,  1.9000, 18.5000, 88.0000]) labels:tensor([2.])\nepoch 1/2, step 30/45, inputs:tensor([ 13.2400,   2.5900,   2.8700,  21.0000, 118.0000]) labels:tensor([1.])\nepoch 1/2, step 35/45, inputs:tensor([11.0300,  1.5100,  2.2000, 21.5000, 85.0000]) labels:tensor([2.])\nepoch 1/2, step 40/45, inputs:tensor([ 13.4800,   1.8100,   2.4100,  20.5000, 100.0000]) labels:tensor([1.])\nepoch 1/2, step 45/45, inputs:tensor([ 12.6400,   1.3600,   2.0200,  16.8000, 100.0000]) labels:tensor([2.])\nepoch 2/2, step 5/45, inputs:tensor([14.7500,  1.7300,  2.3900, 11.4000, 91.0000]) labels:tensor([1.])\nepoch 2/2, step 10/45, inputs:tensor([12.3700,  1.6300,  2.3000, 24.5000, 88.0000]) labels:tensor([2.])\nepoch 2/2, step 15/45, inputs:tensor([ 13.8300,   1.5700,   2.6200,  20.0000, 115.0000]) labels:tensor([1.])\nepoch 2/2, step 20/45, inputs:tensor([12.6900,  1.5300,  2.2600, 20.7000, 80.0000]) labels:tensor([2.])\nepoch 2/2, step 25/45, inputs:tensor([11.4100,  0.7400,  2.5000, 21.0000, 88.0000]) labels:tensor([2.])\nepoch 2/2, step 30/45, inputs:tensor([12.2500,  1.7300,  2.1200, 19.0000, 80.0000]) labels:tensor([2.])\nepoch 2/2, step 35/45, inputs:tensor([ 11.5600,   2.0500,   3.2300,  28.5000, 119.0000]) labels:tensor([2.])\nepoch 2/2, step 40/45, inputs:tensor([ 14.1000,   2.0200,   2.4000,  18.8000, 103.0000]) labels:tensor([1.])\nepoch 2/2, step 45/45, inputs:tensor([ 14.2200,   1.7000,   2.3000,  16.3000, 118.0000]) labels:tensor([1.])",
    "crumbs": [
      "Blog",
      "Transforms"
    ]
  },
  {
    "objectID": "transforms.html#dataset-transform",
    "href": "transforms.html#dataset-transform",
    "title": "Transforms",
    "section": "Dataset Transform",
    "text": "Dataset Transform\n\nTypes of Transform:\n\nOn Images:\n\nCenterCrop, Grayscale, Pad, RandomAffine RandomCrop, RandomHorizontalFlip, RandomRotation Resize, Scale\n\n\n\nOn Tensors:\n\nLinearTransformation, Normalize, RandomErasing\n\n\n\nConversion:\n\nToPILImage: from tensor or ndarray\n\n\nToTensor: from numpy.ndarray or PIL Image\n\n\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass WineDataset(Dataset):\n    def __init__(self, transform = None):\n        #data loading\n        xy = np.loadtxt('Data/wine.csv', delimiter=\",\", dtype=np.float32, skiprows = 1)\n        self.xy = xy\n        self.x = xy[:,1:]\n        self.y = xy[:,[0]]\n        self.n_samples = xy.shape[0]\n        self.transform = transform\n\n    def __getitem__(self, index):\n        sample = self.x[index], self.y[index]\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def __len__(self):\n        return self.n_samples\n\n\nclass ToTensor():\n    def __call__(self, sample):\n        inputs, targets  = sample\n        return torch.from_numpy(inputs), torch.from_numpy(targets)\n\nclass MulTransform:\n    def __init__(self, factor):\n        self.factor = factor\n\n    def __call__(self, sample):\n        inputs, target = sample\n        inputs *= self.factor\n        return inputs, target\n\n\ncomposed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n\n\ndataset = WineDataset(transform = composed)\n\n\nfirst_data = dataset[0]\nfirst_data\n\n(tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n         6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n         2.1300e+03]),\n tensor([1.]))\n\n\n\nfeatures, labels = dataset[0]\nfeatures, labels\n\n(tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n         1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n         4.2600e+03]),\n tensor([1.]))\n\n\n\ndataloader = DataLoader(dataset=dataset, batch_size = 4, shuffle = True, num_workers = 2)\n\n\ndataiter = iter(dataloader)\n\n\nnext(dataiter)\n\n[tensor([[2.7460e+01, 3.0000e+00, 5.4000e+00, 4.5000e+01, 2.0200e+02, 6.0000e+00,\n          6.5000e+00, 5.8000e-01, 4.7600e+00, 1.1400e+01, 2.3800e+00, 5.4200e+00,\n          2.5700e+03],\n         [2.5440e+01, 3.6200e+00, 4.4000e+00, 3.7600e+01, 1.7200e+02, 4.4000e+00,\n          5.0600e+00, 5.2000e-01, 3.5400e+00, 7.8000e+00, 2.3200e+00, 6.2800e+00,\n          1.4280e+03],\n         [2.8200e+01, 4.0400e+00, 4.8000e+00, 3.7600e+01, 2.0600e+02, 5.5000e+00,\n          5.8400e+00, 6.4000e-01, 4.7600e+00, 1.2400e+01, 2.1400e+00, 5.5000e+00,\n          2.1200e+03],\n         [2.6100e+01, 3.5400e+00, 4.2000e+00, 3.4000e+01, 2.1400e+02, 6.0000e+00,\n          6.0000e+00, 5.6000e-01, 4.0600e+00, 1.0080e+01, 1.7600e+00, 6.7000e+00,\n          1.7700e+03]]),\n tensor([[1.],\n         [2.],\n         [1.],\n         [1.]])]\n\n\n\ndataiter = iter(dataloader)\n\n\ndata = next(dataiter)\nfeatures, labels = data\n\n\nfeatures, labels\n\n(tensor([[2.5200e+01, 2.6800e+00, 3.8000e+00, 3.7000e+01, 1.7600e+02, 2.9000e+00,\n          2.7200e+00, 5.8000e-01, 2.7000e+00, 4.9000e+00, 2.0800e+00, 5.5400e+00,\n          1.1240e+03],\n         [2.6460e+01, 6.6000e+00, 4.5600e+00, 3.7000e+01, 1.9600e+02, 3.6000e+00,\n          1.6600e+00, 1.2200e+00, 3.7400e+00, 2.1040e+01, 1.1200e+00, 3.0200e+00,\n          1.3500e+03],\n         [2.4000e+01, 3.0200e+00, 4.8400e+00, 4.4000e+01, 1.7200e+02, 2.9000e+00,\n          2.5000e+00, 1.0000e+00, 3.2600e+00, 7.2000e+00, 2.1000e+00, 5.3000e+00,\n          9.0000e+02],\n         [2.2820e+01, 1.4800e+00, 5.0000e+00, 4.2000e+01, 1.7600e+02, 4.9600e+00,\n          4.0200e+00, 8.4000e-01, 2.8800e+00, 6.1600e+00, 2.2000e+00, 4.6200e+00,\n          8.6800e+02]]),\n tensor([[2.],\n         [3.],\n         [2.],\n         [2.]]))\n\n\n\nnum_epochs = 2\ntotal_samples = len(dataset)\nn_iterations = int(np.ceil(total_samples/4))\n\ntotal_samples, n_iterations\n\n(178, 45)\n\n\n\nfor epoch in range(num_epochs):\n    for i, (inputs, labels) in enumerate(dataloader):\n        if (i + 1) % 5 == 0:\n            print(f'epoch {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs:{inputs[0][:5]} labels:{labels[0]}')\n\nepoch 1/2, step 5/45, inputs:tensor([ 24.0000,   6.8600,   4.0000,  38.0000, 174.0000]) labels:tensor([2.])\nepoch 1/2, step 10/45, inputs:tensor([ 26.9800,   3.3200,   4.4800,  48.0000, 174.0000]) labels:tensor([2.])\nepoch 1/2, step 15/45, inputs:tensor([ 25.4000,   7.1000,   4.7200,  43.0000, 212.0000]) labels:tensor([3.])\nepoch 1/2, step 20/45, inputs:tensor([ 22.9200,   7.4800,   3.6400,  39.0000, 214.0000]) labels:tensor([2.])\nepoch 1/2, step 25/45, inputs:tensor([ 23.2800,   4.1200,   4.9200,  43.2000, 168.0000]) labels:tensor([2.])\nepoch 1/2, step 30/45, inputs:tensor([ 27.4400,   2.8600,   5.0000,  33.4000, 216.0000]) labels:tensor([1.])\nepoch 1/2, step 35/45, inputs:tensor([ 23.3000,   3.3400,   5.2400,  52.0000, 176.0000]) labels:tensor([2.])\nepoch 1/2, step 40/45, inputs:tensor([ 26.3400,   5.1800,   4.7400,  40.0000, 240.0000]) labels:tensor([3.])\nepoch 1/2, step 45/45, inputs:tensor([ 29.5000,   3.4600,   4.7800,  22.8000, 182.0000]) labels:tensor([1.])\nepoch 2/2, step 5/45, inputs:tensor([ 27.1600,   5.1600,   5.3800,  49.0000, 210.0000]) labels:tensor([3.])\nepoch 2/2, step 10/45, inputs:tensor([ 26.1000,   3.5400,   4.2000,  34.0000, 214.0000]) labels:tensor([1.])\nepoch 2/2, step 15/45, inputs:tensor([ 24.1400,   4.3200,   4.3400,  42.0000, 170.0000]) labels:tensor([2.])\nepoch 2/2, step 20/45, inputs:tensor([ 24.7400,   3.2600,   4.6000,  49.0000, 176.0000]) labels:tensor([2.])\nepoch 2/2, step 25/45, inputs:tensor([ 24.5800,   2.8200,   3.9600,  32.0000, 170.0000]) labels:tensor([2.])\nepoch 2/2, step 30/45, inputs:tensor([ 24.7400,   2.2600,   4.3200,  38.0000, 174.0000]) labels:tensor([2.])\nepoch 2/2, step 35/45, inputs:tensor([ 26.5600,   3.2800,   5.6800,  31.0000, 220.0000]) labels:tensor([1.])\nepoch 2/2, step 40/45, inputs:tensor([ 24.7400,   1.8800,   2.7200,  21.2000, 176.0000]) labels:tensor([2.])\nepoch 2/2, step 45/45, inputs:tensor([ 26.3400,  10.3800,   4.6400,  44.0000, 186.0000]) labels:tensor([3.])",
    "crumbs": [
      "Blog",
      "Transforms"
    ]
  },
  {
    "objectID": "transforms.html#images",
    "href": "transforms.html#images",
    "title": "Transforms",
    "section": "Images",
    "text": "Images\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets, utils\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import DataLoader\n\nfrom datetime import datetime\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport timm\nimport numpy as np\n\n\ntransform_default = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet mean and std\n])\n\n\n# Download and load the Imagenette dataset\ntrain_dataset = datasets.Imagenette(root='Data',\n                                    split='train',\n                                    # download=True,\n                                    transform=transform_default,\n                                    )\n\n\n# Download and load the Imagenette dataset\ntest_dataset = datasets.Imagenette(root='Data',\n                                  split='val',\n                                  # download=True,\n                                  transform=transform_default,\n                                 )\n\n\n# Define a function to display images\ndef show_image(dataset):\n    # Access an image and its label from the dataset\n    image, label = dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image[:3].numpy().transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_np.clip(0,1))\n    plt.axis('off')\n    plt.title(f' {train_dataset.classes[label][0]}')\n    plt.show()\n    \n   \n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        img_np = image[:3].numpy().transpose((1, 2, 0))\n        \n        # Display image\n        ax.imshow(img_np.clip(0,1))\n        ax.axis('off')\n        ax.set_title(f' {train_dataset.classes[label][0]}')\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n        \n    plt.show()\n\n\nimage, label = train_dataset[2]\n\n\ntype(image)\n\ntorch.Tensor\n\n\n\nshow_image(train_dataset[2])",
    "crumbs": [
      "Blog",
      "Transforms"
    ]
  },
  {
    "objectID": "transforms.html#test-transform",
    "href": "transforms.html#test-transform",
    "title": "Transforms",
    "section": "Test Transform",
    "text": "Test Transform\n\nimport torch\nimport torchvision.transforms.functional as TF\n\nclass AddGrayscaleChannel(object):\n    def __init__(self):\n        super().__init__()\n    \n    def __call__(self, img):\n        # Convert the image to grayscale\n        gray_img = TF.rgb_to_grayscale(img)\n        \n        # Concatenate the grayscale image with the original image along the fourth dimension\n        img_with_gray_channel = torch.cat((img, gray_img), dim=0)\n        \n        return img_with_gray_channel\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'",
    "crumbs": [
      "Blog",
      "Transforms"
    ]
  },
  {
    "objectID": "transforms.html#fft-transform",
    "href": "transforms.html#fft-transform",
    "title": "Transforms",
    "section": "FFT Transform",
    "text": "FFT Transform\n\nclass ComputeFFT(object):\n    def __init__(self):\n        super().__init__()\n    \n    def __call__(self, image):\n        # Convert the color image to grayscale\n        grayscale_image = TF.rgb_to_grayscale(image).squeeze()\n        \n        # Convert the grayscale image to tensor and apply FFT\n        fft_result = torch.fft.fft2(grayscale_image)\n        \n        # Compute magnitude spectrum\n        magnitude_spectrum = torch.log(torch.abs(fft_result) + 1)\n        \n        # Compute phase spectrum\n        phase_spectrum = torch.angle(fft_result)\n        \n        combined_image = torch.cat((image, magnitude_spectrum.unsqueeze(0), phase_spectrum.unsqueeze(0)), dim=0)\n\n        return combined_image\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\ntransform = ComputeFFT()\nnew_image = transform(image)\ntransposed_image = torch.transpose(new_image[:3], 0, 2).transpose(0, 1)\n\nplt.imshow(transposed_image, cmap='gray')\nplt.title('image')\nplt.colorbar()\nplt.show()\n\nplt.imshow(new_image[3], cmap='gray')\nplt.title('Magnitude Spectrum')\nplt.colorbar()\nplt.show()\n\n# Visualize phase spectrum\nplt.imshow(new_image[4], cmap='gray')\nplt.title('Phase Spectrum')\nplt.colorbar()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\nmodel_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n\nmidas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmidas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n\nif model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n    transform = midas_transforms.dpt_transform\nelse:\n    transform = midas_transforms.small_transform\n\nUsing cache found in /home/ben/.cache/torch/hub/intel-isl_MiDaS_master\n\n\nLoading weights:  None\n\n\nUsing cache found in /home/ben/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\nUsing cache found in /home/ben/.cache/torch/hub/intel-isl_MiDaS_master\n\n\n\nclass ComputeDepth(object):\n    def __init__(self, model, transform, device = \"cuda\"):\n        super().__init__()\n        self.device = device\n        self.model = model\n        self.model.to(self.device)\n        self.model.eval()\n        self.transform = transform\n    \n    def __call__(self, image):\n        input_image = image.to('cpu').squeeze().numpy().transpose(1, 2, 0)\n        # input_image = np.array(image)\n        input_batch = transform(input_image).to(self.device)\n        \n        with torch.no_grad():\n            prediction = self.model(input_batch)\n            \n            prediction = torch.nn.functional.interpolate(\n                prediction.unsqueeze(1),\n                size=input_image.shape[:2],\n                mode=\"bicubic\",\n                align_corners=False,\n            ).squeeze(0)\n            prediction = prediction.to('cpu')\n\n        prediction_mean = torch.mean(prediction)\n        prediction_std = torch.std(prediction)\n        \n        # Calculate the scaling factors for normalization\n        scale_factor = 0.225 / prediction_std\n        bias = 0.45 - prediction_mean * scale_factor\n        \n        # Normalize the tensor to the desired mean and standard deviation\n        prediction = prediction * scale_factor + bias\n        \n        combined_image = torch.cat((image, prediction), dim=0)\n\n        return combined_image\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nimage, label = train_dataset[2]\n\n\ntype(image)\n\ntorch.Tensor\n\n\n\ntransform_depth = ComputeDepth(midas, transform = transform, device = \"cpu\")\nnew_image = transform_depth(image)\ntransposed_image = torch.transpose(new_image[:3], 0, 2).transpose(0, 1)\n\n\nplt.imshow(transposed_image, cmap='gray')\nplt.title('image')\nplt.colorbar()\nplt.show()\n\nplt.imshow(new_image[3], cmap='gray')\nplt.title('Magnitude Spectrum')\nplt.colorbar()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).",
    "crumbs": [
      "Blog",
      "Transforms"
    ]
  },
  {
    "objectID": "Car_detection/car_detection.html",
    "href": "Car_detection/car_detection.html",
    "title": "Car detection",
    "section": "",
    "text": "from ultralytics import YOLO\nimport cv2\n\n# Load the pre-trained YOLOv8 model\nmodel = YOLO('yolov8n.pt')  # Replace 'yolov8s.pt' with 'yolov8n.pt' for a lighter model if necessary\n\n# Dictionary to filter and label specific objects\ntarget_classes = model.names\n\n# Reference the camera directly by device path\ncap = cv2.VideoCapture('/dev/video0')  # Replace '/dev/video0' with your device path if different\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        print(\"Error: Could not read from camera\")\n        break\n\n    # Run YOLOv8 inference on the frame\n    results = model.predict(frame, conf=0.5, imgsz=128)\n\n    # Loop through results and only show detections for specified classes\n    for result in results:\n        boxes = result.boxes  # Bounding boxes\n        for box in boxes:\n            class_id = int(box.cls)\n            if class_id in target_classes:\n                label_name = target_classes[class_id]  # Get the label from the dictionary\n                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Coordinates\n                confidence = box.conf[0]  # Confidence score\n                label = f\"{label_name} {confidence:.2f}\"\n\n                # Draw the bounding box and label on the frame\n                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Display the frame\n    cv2.imshow('YOLOv8 Targeted Detection', frame)\n\n    # Press 'q' to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.3ms preprocess, 8.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.6ms\nSpeed: 0.5ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.2ms\nSpeed: 0.4ms preprocess, 7.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 6.7ms\nSpeed: 0.4ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.4ms\nSpeed: 0.5ms preprocess, 9.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 6.7ms\nSpeed: 0.4ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 1.4ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.2ms\nSpeed: 0.5ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.1ms\nSpeed: 0.5ms preprocess, 11.1ms inference, 1.0ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.1ms\nSpeed: 0.5ms preprocess, 11.1ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.1ms\nSpeed: 0.4ms preprocess, 11.1ms inference, 0.9ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.6ms\nSpeed: 0.4ms preprocess, 8.6ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.4ms\nSpeed: 0.6ms preprocess, 8.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.5ms\nSpeed: 0.4ms preprocess, 7.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.6ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.6ms preprocess, 7.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.1ms\nSpeed: 0.5ms preprocess, 8.1ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.6ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.8ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.6ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.6ms preprocess, 10.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.7ms\nSpeed: 0.5ms preprocess, 8.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.8ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.5ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.9ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.8ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.5ms preprocess, 10.8ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.1ms\nSpeed: 0.6ms preprocess, 12.1ms inference, 0.9ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.8ms\nSpeed: 0.5ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.4ms\nSpeed: 0.5ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.6ms preprocess, 8.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.9ms\nSpeed: 0.6ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.0ms\nSpeed: 0.4ms preprocess, 9.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.7ms\nSpeed: 0.6ms preprocess, 9.7ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.7ms\nSpeed: 0.5ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.5ms preprocess, 10.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.5ms preprocess, 11.0ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.5ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.6ms\nSpeed: 0.6ms preprocess, 11.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.5ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.6ms\nSpeed: 0.5ms preprocess, 8.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.4ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.6ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.6ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.8ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.2ms\nSpeed: 0.6ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.5ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.9ms\nSpeed: 0.5ms preprocess, 12.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.9ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.2ms\nSpeed: 0.8ms preprocess, 12.2ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.6ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.7ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.6ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.8ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.7ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.9ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.1ms\nSpeed: 0.5ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.5ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.2ms\nSpeed: 0.6ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.1ms\nSpeed: 0.4ms preprocess, 7.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 6.9ms\nSpeed: 0.5ms preprocess, 6.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.7ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.2ms\nSpeed: 0.7ms preprocess, 7.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.2ms\nSpeed: 0.4ms preprocess, 7.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.7ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.3ms\nSpeed: 0.8ms preprocess, 12.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.6ms\nSpeed: 0.7ms preprocess, 12.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.1ms\nSpeed: 0.6ms preprocess, 12.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.6ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.7ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.7ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.6ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.6ms\nSpeed: 0.6ms preprocess, 9.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 6.6ms\nSpeed: 0.4ms preprocess, 6.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.6ms\nSpeed: 0.6ms preprocess, 9.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.6ms preprocess, 10.8ms inference, 1.0ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.6ms\nSpeed: 0.6ms preprocess, 7.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.5ms\nSpeed: 0.4ms preprocess, 7.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.9ms\nSpeed: 0.8ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.7ms\nSpeed: 0.6ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.7ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.7ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.6ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.4ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.4ms\nSpeed: 0.7ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.3ms\nSpeed: 0.5ms preprocess, 7.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.2ms\nSpeed: 0.6ms preprocess, 7.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.8ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.6ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.6ms\nSpeed: 0.4ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.9ms\nSpeed: 0.6ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.9ms\nSpeed: 0.5ms preprocess, 9.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.1ms\nSpeed: 0.7ms preprocess, 7.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 6.8ms\nSpeed: 0.6ms preprocess, 6.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.1ms\nSpeed: 0.5ms preprocess, 7.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.0ms\nSpeed: 0.7ms preprocess, 7.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.7ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.2ms\nSpeed: 0.8ms preprocess, 8.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.0ms\nSpeed: 0.4ms preprocess, 7.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.7ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.1ms\nSpeed: 0.7ms preprocess, 8.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.5ms preprocess, 7.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.8ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.3ms\nSpeed: 0.8ms preprocess, 11.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.7ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.6ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.5ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.5ms\nSpeed: 0.6ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.9ms\nSpeed: 0.4ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.8ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.7ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.8ms\nSpeed: 0.6ms preprocess, 8.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.2ms\nSpeed: 0.6ms preprocess, 8.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.6ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.9ms\nSpeed: 0.6ms preprocess, 8.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.6ms\nSpeed: 0.6ms preprocess, 7.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.6ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.9ms\nSpeed: 0.6ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.6ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.4ms\nSpeed: 0.6ms preprocess, 9.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.6ms\nSpeed: 0.6ms preprocess, 7.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.5ms\nSpeed: 0.7ms preprocess, 7.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.1ms\nSpeed: 0.6ms preprocess, 8.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.6ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.5ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.7ms preprocess, 8.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.6ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.9ms\nSpeed: 0.6ms preprocess, 11.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.6ms\nSpeed: 0.6ms preprocess, 8.6ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.4ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.5ms preprocess, 10.8ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.6ms preprocess, 11.0ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.2ms\nSpeed: 0.6ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.7ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.2ms\nSpeed: 0.8ms preprocess, 11.2ms inference, 1.0ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.5ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.6ms\nSpeed: 0.5ms preprocess, 11.6ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.3ms\nSpeed: 0.7ms preprocess, 7.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.5ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.5ms\nSpeed: 0.5ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.3ms\nSpeed: 0.6ms preprocess, 9.3ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.5ms\nSpeed: 0.5ms preprocess, 7.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.6ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.9ms\nSpeed: 0.7ms preprocess, 7.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.7ms preprocess, 8.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.6ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.8ms\nSpeed: 0.5ms preprocess, 8.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.5ms preprocess, 8.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.6ms preprocess, 8.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.7ms\nSpeed: 0.8ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.3ms\nSpeed: 0.6ms preprocess, 9.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.5ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.8ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.6ms\nSpeed: 0.6ms preprocess, 8.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.2ms\nSpeed: 0.5ms preprocess, 8.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.5ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.3ms\nSpeed: 0.8ms preprocess, 8.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.9ms\nSpeed: 0.6ms preprocess, 9.9ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.8ms\nSpeed: 0.6ms preprocess, 8.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.5ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.5ms preprocess, 10.8ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.5ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.6ms preprocess, 10.8ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.6ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.6ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.0ms\nSpeed: 0.5ms preprocess, 9.0ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.1ms\nSpeed: 0.5ms preprocess, 11.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.6ms\nSpeed: 0.5ms preprocess, 8.6ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.4ms\nSpeed: 0.6ms preprocess, 8.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.7ms\nSpeed: 0.7ms preprocess, 8.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.4ms\nSpeed: 0.5ms preprocess, 8.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.3ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.8ms\nSpeed: 0.5ms preprocess, 9.8ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.7ms\nSpeed: 0.4ms preprocess, 7.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.6ms\nSpeed: 0.6ms preprocess, 8.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.5ms preprocess, 10.9ms inference, 0.9ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.7ms preprocess, 11.0ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.8ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 6.9ms\nSpeed: 0.5ms preprocess, 6.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.7ms\nSpeed: 0.6ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.7ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.7ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.7ms preprocess, 10.8ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.3ms\nSpeed: 0.3ms preprocess, 7.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 13.5ms\nSpeed: 0.7ms preprocess, 13.5ms inference, 0.9ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.3ms\nSpeed: 0.7ms preprocess, 11.3ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.1ms\nSpeed: 0.4ms preprocess, 11.1ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.6ms\nSpeed: 0.8ms preprocess, 8.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.6ms preprocess, 7.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.7ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.6ms\nSpeed: 0.6ms preprocess, 11.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.6ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 13.4ms\nSpeed: 0.8ms preprocess, 13.4ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 9.9ms\nSpeed: 0.9ms preprocess, 9.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.4ms\nSpeed: 0.5ms preprocess, 10.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.4ms\nSpeed: 0.8ms preprocess, 12.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.5ms\nSpeed: 0.6ms preprocess, 12.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 7.9ms\nSpeed: 0.4ms preprocess, 7.9ms inference, 0.3ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 7.1ms\nSpeed: 0.6ms preprocess, 7.1ms inference, 0.2ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.6ms\nSpeed: 0.9ms preprocess, 10.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.0ms\nSpeed: 0.6ms preprocess, 10.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 13.1ms\nSpeed: 0.9ms preprocess, 13.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 13.1ms\nSpeed: 0.5ms preprocess, 13.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 7.2ms\nSpeed: 0.4ms preprocess, 7.2ms inference, 0.2ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.6ms\nSpeed: 0.8ms preprocess, 10.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.5ms\nSpeed: 0.8ms preprocess, 10.5ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.5ms\nSpeed: 0.9ms preprocess, 12.5ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.6ms\nSpeed: 0.7ms preprocess, 11.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 14.9ms\nSpeed: 0.8ms preprocess, 14.9ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 16.0ms\nSpeed: 0.8ms preprocess, 16.0ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 15.2ms\nSpeed: 0.7ms preprocess, 15.2ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.3ms\nSpeed: 0.8ms preprocess, 11.3ms inference, 0.3ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.9ms\nSpeed: 0.8ms preprocess, 10.9ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.6ms\nSpeed: 0.9ms preprocess, 10.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.4ms\nSpeed: 1.0ms preprocess, 10.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.6ms\nSpeed: 0.8ms preprocess, 10.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.5ms\nSpeed: 0.9ms preprocess, 11.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.0ms\nSpeed: 0.6ms preprocess, 12.0ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 7.5ms\nSpeed: 0.4ms preprocess, 7.5ms inference, 0.3ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.3ms\nSpeed: 0.4ms preprocess, 10.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 9.5ms\nSpeed: 0.7ms preprocess, 9.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.4ms\nSpeed: 0.7ms preprocess, 12.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.0ms\nSpeed: 1.0ms preprocess, 11.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 13.1ms\nSpeed: 0.7ms preprocess, 13.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.7ms preprocess, 10.7ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.7ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.7ms preprocess, 10.4ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.9ms\nSpeed: 0.6ms preprocess, 11.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.5ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 9.3ms\nSpeed: 0.6ms preprocess, 9.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 13.1ms\nSpeed: 0.9ms preprocess, 13.1ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.9ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.8ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.5ms\nSpeed: 0.6ms preprocess, 7.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.9ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.7ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.7ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.7ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.8ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.7ms preprocess, 10.8ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.7ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.8ms\nSpeed: 0.9ms preprocess, 11.8ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.5ms\nSpeed: 0.8ms preprocess, 11.5ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.6ms\nSpeed: 0.7ms preprocess, 11.6ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.1ms\nSpeed: 0.7ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.9ms\nSpeed: 0.6ms preprocess, 11.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.8ms\nSpeed: 0.8ms preprocess, 11.8ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.7ms\nSpeed: 0.8ms preprocess, 11.7ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.3ms\nSpeed: 0.5ms preprocess, 8.3ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.8ms\nSpeed: 0.7ms preprocess, 9.8ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.9ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.7ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.8ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 14.3ms\nSpeed: 0.6ms preprocess, 14.3ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.3ms\nSpeed: 0.4ms preprocess, 7.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.8ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.7ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.7ms preprocess, 10.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.5ms\nSpeed: 0.9ms preprocess, 8.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.8ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.9ms\nSpeed: 0.7ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.3ms\nSpeed: 0.5ms preprocess, 8.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.6ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.7ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.7ms preprocess, 10.8ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.8ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.7ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.8ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.8ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.1ms\nSpeed: 0.8ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.5ms\nSpeed: 0.5ms preprocess, 11.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.5ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.8ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.8ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.8ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.3ms\nSpeed: 0.7ms preprocess, 12.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.6ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.9ms\nSpeed: 0.7ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.8ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.9ms\nSpeed: 0.8ms preprocess, 11.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.8ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.9ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.7ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.9ms\nSpeed: 0.9ms preprocess, 11.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.3ms\nSpeed: 0.6ms preprocess, 11.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.8ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.7ms\nSpeed: 0.7ms preprocess, 9.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.4ms\nSpeed: 0.5ms preprocess, 9.4ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.5ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.9ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.5ms preprocess, 7.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.8ms preprocess, 10.9ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.4ms\nSpeed: 0.8ms preprocess, 11.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.6ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.2ms\nSpeed: 1.0ms preprocess, 9.2ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.6ms\nSpeed: 0.8ms preprocess, 12.6ms inference, 0.9ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.6ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.1ms\nSpeed: 0.5ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.8ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.9ms\nSpeed: 0.6ms preprocess, 9.9ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.1ms\nSpeed: 0.8ms preprocess, 9.1ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.5ms\nSpeed: 0.9ms preprocess, 12.5ms inference, 1.0ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.3ms\nSpeed: 0.6ms preprocess, 12.3ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.5ms\nSpeed: 0.5ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.8ms preprocess, 10.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.5ms\nSpeed: 0.6ms preprocess, 10.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.7ms\nSpeed: 0.9ms preprocess, 10.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.9ms\nSpeed: 0.8ms preprocess, 10.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.6ms\nSpeed: 0.8ms preprocess, 10.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.8ms\nSpeed: 0.6ms preprocess, 12.8ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.9ms\nSpeed: 0.8ms preprocess, 10.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.7ms\nSpeed: 0.5ms preprocess, 11.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.5ms\nSpeed: 0.6ms preprocess, 10.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.3ms\nSpeed: 0.7ms preprocess, 10.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.7ms\nSpeed: 0.7ms preprocess, 11.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.9ms\nSpeed: 0.6ms preprocess, 10.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.8ms\nSpeed: 0.7ms preprocess, 10.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.3ms\nSpeed: 0.8ms preprocess, 10.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 7.4ms\nSpeed: 0.4ms preprocess, 7.4ms inference, 0.2ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 8.6ms\nSpeed: 0.4ms preprocess, 8.6ms inference, 0.2ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.4ms\nSpeed: 0.6ms preprocess, 10.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 8.7ms\nSpeed: 0.6ms preprocess, 8.7ms inference, 0.3ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 9.1ms\nSpeed: 0.6ms preprocess, 9.1ms inference, 0.3ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.0ms\nSpeed: 0.8ms preprocess, 12.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.8ms\nSpeed: 0.6ms preprocess, 11.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.1ms\nSpeed: 0.9ms preprocess, 11.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.2ms\nSpeed: 0.5ms preprocess, 11.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.8ms\nSpeed: 0.7ms preprocess, 10.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.8ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.0ms\nSpeed: 0.7ms preprocess, 10.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.4ms\nSpeed: 0.7ms preprocess, 10.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.1ms\nSpeed: 0.7ms preprocess, 12.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.1ms\nSpeed: 0.8ms preprocess, 12.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 7.3ms\nSpeed: 0.5ms preprocess, 7.3ms inference, 0.2ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.8ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.7ms preprocess, 10.7ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.7ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.9ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.7ms\nSpeed: 0.7ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 13.2ms\nSpeed: 0.8ms preprocess, 13.2ms inference, 0.9ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.1ms\nSpeed: 0.8ms preprocess, 10.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 8.9ms\nSpeed: 0.7ms preprocess, 8.9ms inference, 0.2ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 1.0ms preprocess, 10.6ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.8ms preprocess, 10.8ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.7ms\nSpeed: 0.8ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.0ms\nSpeed: 0.8ms preprocess, 12.0ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.6ms\nSpeed: 0.5ms preprocess, 11.6ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.6ms\nSpeed: 0.7ms preprocess, 11.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.4ms preprocess, 7.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.2ms\nSpeed: 0.8ms preprocess, 10.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.3ms\nSpeed: 0.7ms preprocess, 10.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.4ms\nSpeed: 0.9ms preprocess, 10.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.1ms\nSpeed: 0.8ms preprocess, 11.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.7ms\nSpeed: 0.7ms preprocess, 10.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.3ms\nSpeed: 0.9ms preprocess, 12.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.2ms\nSpeed: 0.7ms preprocess, 11.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.1ms\nSpeed: 0.9ms preprocess, 11.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.5ms\nSpeed: 0.8ms preprocess, 11.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.0ms\nSpeed: 0.7ms preprocess, 12.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.6ms\nSpeed: 0.9ms preprocess, 11.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.6ms\nSpeed: 1.0ms preprocess, 11.6ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.9ms\nSpeed: 0.6ms preprocess, 10.9ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.3ms\nSpeed: 0.9ms preprocess, 11.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 6.9ms\nSpeed: 0.6ms preprocess, 6.9ms inference, 0.2ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.1ms\nSpeed: 0.8ms preprocess, 11.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.3ms\nSpeed: 0.8ms preprocess, 11.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.4ms\nSpeed: 0.7ms preprocess, 12.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.0ms\nSpeed: 0.5ms preprocess, 11.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.0ms\nSpeed: 0.8ms preprocess, 11.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.5ms\nSpeed: 0.7ms preprocess, 11.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 9.7ms\nSpeed: 0.6ms preprocess, 9.7ms inference, 0.3ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 9.2ms\nSpeed: 0.6ms preprocess, 9.2ms inference, 0.3ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 7.8ms\nSpeed: 0.5ms preprocess, 7.8ms inference, 0.3ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.6ms preprocess, 10.9ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.9ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.7ms preprocess, 11.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.9ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.6ms preprocess, 11.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.1ms\nSpeed: 0.6ms preprocess, 7.1ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.9ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.7ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.5ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.5ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.9ms\nSpeed: 0.6ms preprocess, 11.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.3ms\nSpeed: 1.0ms preprocess, 12.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.1ms\nSpeed: 0.6ms preprocess, 12.1ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.9ms\nSpeed: 0.6ms preprocess, 11.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.2ms\nSpeed: 0.7ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.3ms\nSpeed: 0.5ms preprocess, 11.3ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.9ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.8ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.6ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.8ms\nSpeed: 0.7ms preprocess, 9.8ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.6ms\nSpeed: 0.4ms preprocess, 7.6ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.3ms\nSpeed: 0.5ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.3ms\nSpeed: 0.8ms preprocess, 12.3ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.3ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.2ms\nSpeed: 0.7ms preprocess, 7.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.9ms\nSpeed: 0.5ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.4ms\nSpeed: 0.7ms preprocess, 11.4ms inference, 0.9ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.8ms preprocess, 10.9ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.6ms preprocess, 10.7ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 15.4ms\nSpeed: 0.6ms preprocess, 15.4ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.4ms\nSpeed: 0.5ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.5ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.6ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.4ms\nSpeed: 0.3ms preprocess, 7.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.2ms\nSpeed: 0.7ms preprocess, 11.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.2ms\nSpeed: 0.4ms preprocess, 7.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.8ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.8ms preprocess, 10.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.6ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.5ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.7ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.7ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.6ms\nSpeed: 0.6ms preprocess, 10.6ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.8ms\nSpeed: 0.5ms preprocess, 9.8ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.9ms\nSpeed: 0.7ms preprocess, 9.9ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.8ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.9ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.9ms\nSpeed: 0.8ms preprocess, 11.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.8ms\nSpeed: 0.5ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.2ms\nSpeed: 0.8ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.9ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.9ms preprocess, 10.9ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.7ms preprocess, 10.4ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.2ms\nSpeed: 0.8ms preprocess, 10.2ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 8.0ms\nSpeed: 0.5ms preprocess, 8.0ms inference, 0.2ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.0ms\nSpeed: 0.7ms preprocess, 11.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.3ms\nSpeed: 0.8ms preprocess, 11.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 11.5ms\nSpeed: 0.7ms preprocess, 11.5ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 12.2ms\nSpeed: 0.9ms preprocess, 12.2ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 (no detections), 10.7ms\nSpeed: 0.6ms preprocess, 10.7ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.4ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.7ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.7ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 2 persons, 8.0ms\nSpeed: 0.7ms preprocess, 8.0ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.0ms\nSpeed: 0.8ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.8ms preprocess, 10.7ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.6ms preprocess, 11.0ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.5ms\nSpeed: 0.9ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.8ms preprocess, 10.8ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.5ms\nSpeed: 0.4ms preprocess, 9.5ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.3ms\nSpeed: 0.6ms preprocess, 10.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.7ms\nSpeed: 0.4ms preprocess, 9.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.4ms\nSpeed: 0.5ms preprocess, 8.4ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.8ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.3ms\nSpeed: 0.8ms preprocess, 11.3ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.3ms\nSpeed: 0.6ms preprocess, 9.3ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.7ms\nSpeed: 0.6ms preprocess, 9.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.8ms\nSpeed: 0.8ms preprocess, 10.8ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.0ms\nSpeed: 0.6ms preprocess, 8.0ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.4ms\nSpeed: 0.6ms preprocess, 8.4ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.3ms\nSpeed: 0.6ms preprocess, 7.3ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.7ms\nSpeed: 0.5ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.7ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 9.3ms\nSpeed: 0.7ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.4ms\nSpeed: 0.6ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.1ms\nSpeed: 0.9ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.7ms preprocess, 10.9ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.1ms\nSpeed: 0.7ms preprocess, 12.1ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.7ms\nSpeed: 0.5ms preprocess, 10.7ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 12.0ms\nSpeed: 0.7ms preprocess, 12.0ms inference, 0.8ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.0ms\nSpeed: 0.6ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.8ms\nSpeed: 0.3ms preprocess, 8.8ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.8ms\nSpeed: 0.6ms preprocess, 8.8ms inference, 0.4ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 7.5ms\nSpeed: 0.4ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 8.4ms\nSpeed: 0.5ms preprocess, 8.4ms inference, 0.5ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.1ms\nSpeed: 0.7ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 10.9ms\nSpeed: 0.8ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n0: 96x128 1 person, 11.7ms\nSpeed: 0.7ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 96, 128)\n\n\n\nfrom ultralytics import YOLO\nimport cv2\n\n# Load the pre-trained YOLOv8 model\nmodel = YOLO('yolov8n.pt')  # Replace 'yolov8s.pt' with 'yolov8n.pt' for a lighter model if necessary\n\n# Print the full dictionary of class labels\nprint(\"Class ID to Label Mapping:\")\nfor class_id, label in model.names.items():\n    print(f\"{class_id}: {label}\")\n\nClass ID to Label Mapping:\n0: person\n1: bicycle\n2: car\n3: motorcycle\n4: airplane\n5: bus\n6: train\n7: truck\n8: boat\n9: traffic light\n10: fire hydrant\n11: stop sign\n12: parking meter\n13: bench\n14: bird\n15: cat\n16: dog\n17: horse\n18: sheep\n19: cow\n20: elephant\n21: bear\n22: zebra\n23: giraffe\n24: backpack\n25: umbrella\n26: handbag\n27: tie\n28: suitcase\n29: frisbee\n30: skis\n31: snowboard\n32: sports ball\n33: kite\n34: baseball bat\n35: baseball glove\n36: skateboard\n37: surfboard\n38: tennis racket\n39: bottle\n40: wine glass\n41: cup\n42: fork\n43: knife\n44: spoon\n45: bowl\n46: banana\n47: apple\n48: sandwich\n49: orange\n50: broccoli\n51: carrot\n52: hot dog\n53: pizza\n54: donut\n55: cake\n56: chair\n57: couch\n58: potted plant\n59: bed\n60: dining table\n61: toilet\n62: tv\n63: laptop\n64: mouse\n65: remote\n66: keyboard\n67: cell phone\n68: microwave\n69: oven\n70: toaster\n71: sink\n72: refrigerator\n73: book\n74: clock\n75: vase\n76: scissors\n77: teddy bear\n78: hair drier\n79: toothbrush\n\n\n\n\n\n{0: 'person',\n 1: 'bicycle',\n 2: 'car',\n 3: 'motorcycle',\n 4: 'airplane',\n 5: 'bus',\n 6: 'train',\n 7: 'truck',\n 8: 'boat',\n 9: 'traffic light',\n 10: 'fire hydrant',\n 11: 'stop sign',\n 12: 'parking meter',\n 13: 'bench',\n 14: 'bird',\n 15: 'cat',\n 16: 'dog',\n 17: 'horse',\n 18: 'sheep',\n 19: 'cow',\n 20: 'elephant',\n 21: 'bear',\n 22: 'zebra',\n 23: 'giraffe',\n 24: 'backpack',\n 25: 'umbrella',\n 26: 'handbag',\n 27: 'tie',\n 28: 'suitcase',\n 29: 'frisbee',\n 30: 'skis',\n 31: 'snowboard',\n 32: 'sports ball',\n 33: 'kite',\n 34: 'baseball bat',\n 35: 'baseball glove',\n 36: 'skateboard',\n 37: 'surfboard',\n 38: 'tennis racket',\n 39: 'bottle',\n 40: 'wine glass',\n 41: 'cup',\n 42: 'fork',\n 43: 'knife',\n 44: 'spoon',\n 45: 'bowl',\n 46: 'banana',\n 47: 'apple',\n 48: 'sandwich',\n 49: 'orange',\n 50: 'broccoli',\n 51: 'carrot',\n 52: 'hot dog',\n 53: 'pizza',\n 54: 'donut',\n 55: 'cake',\n 56: 'chair',\n 57: 'couch',\n 58: 'potted plant',\n 59: 'bed',\n 60: 'dining table',\n 61: 'toilet',\n 62: 'tv',\n 63: 'laptop',\n 64: 'mouse',\n 65: 'remote',\n 66: 'keyboard',\n 67: 'cell phone',\n 68: 'microwave',\n 69: 'oven',\n 70: 'toaster',\n 71: 'sink',\n 72: 'refrigerator',\n 73: 'book',\n 74: 'clock',\n 75: 'vase',\n 76: 'scissors',\n 77: 'teddy bear',\n 78: 'hair drier',\n 79: 'toothbrush'}\n\n\n\na = [1,2,3]\n\nb = [2,3,4]\n\nc = None\n\nresult = min([i for i in [a,b,c] if i])\nprint(result)\n\n[1, 2, 3]\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Car Detection",
      "Car detection"
    ]
  },
  {
    "objectID": "fastai_setup.html",
    "href": "fastai_setup.html",
    "title": "FastAI Setup",
    "section": "",
    "text": "pip install fastai\n\n!pip list | grep \"fastai\"\n\n!mamba install -c nvidia fastai\nconda install -c nvidia fastai anaconda\n\n\n\npip install fastbook\n\n!pip list | grep \"fastbook\"\n\n\nimport torch\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 import torch\n\nModuleNotFoundError: No module named 'torch'\n\n\n\n\nif torch.cuda.is_available():\n    print(\"GPU is available.\")\n    num_gpu = torch.cuda.device_count()\n    for i in range(num_gpu):\n        gpu_name = torch.cuda.get_device_name(i)\n        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n        print(f\"GPU {i}: {gpu_name}, Memory: {gpu_memory:.2f} GB\")\nelse:\n    print(\"GPU is not available.\")",
    "crumbs": [
      "Blog",
      "FastAI Setup"
    ]
  },
  {
    "objectID": "fastai_setup.html#installs",
    "href": "fastai_setup.html#installs",
    "title": "FastAI Setup",
    "section": "",
    "text": "pip install fastai\n\n!pip list | grep \"fastai\"\n\n!mamba install -c nvidia fastai\nconda install -c nvidia fastai anaconda\n\n\n\npip install fastbook\n\n!pip list | grep \"fastbook\"\n\n\nimport torch\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 import torch\n\nModuleNotFoundError: No module named 'torch'\n\n\n\n\nif torch.cuda.is_available():\n    print(\"GPU is available.\")\n    num_gpu = torch.cuda.device_count()\n    for i in range(num_gpu):\n        gpu_name = torch.cuda.get_device_name(i)\n        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n        print(f\"GPU {i}: {gpu_name}, Memory: {gpu_memory:.2f} GB\")\nelse:\n    print(\"GPU is not available.\")",
    "crumbs": [
      "Blog",
      "FastAI Setup"
    ]
  },
  {
    "objectID": "Computer_Vision/image_multi_classification.html",
    "href": "Computer_Vision/image_multi_classification.html",
    "title": "Image Multi Classification",
    "section": "",
    "text": "CIFAR-10\n\n\nDescription: 60,000 32x32 color images in 10 classes, with 6,000 images per class.\nURL: CIFAR-10\n\n\nCIFAR-100\n\n\nDescription: 60,000 32x32 color images in 100 classes, with 600 images per class.\nURL: CIFAR-100\n\n\nImageNet\n\n\nDescription: Over 14 million images categorized into over 20,000 classes.\nURL: ImageNet\n\n\nMNIST\n\n\nDescription: 70,000 28x28 grayscale images of handwritten digits in 10 classes.\nURL: MNIST\n\n\nFashion-MNIST\n\n\nDescription: 70,000 28x28 grayscale images of 10 fashion categories.\nURL: Fashion-MNIST",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image Multi Classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_multi_classification.html#popular-datasets",
    "href": "Computer_Vision/image_multi_classification.html#popular-datasets",
    "title": "Image Multi Classification",
    "section": "",
    "text": "CIFAR-10\n\n\nDescription: 60,000 32x32 color images in 10 classes, with 6,000 images per class.\nURL: CIFAR-10\n\n\nCIFAR-100\n\n\nDescription: 60,000 32x32 color images in 100 classes, with 600 images per class.\nURL: CIFAR-100\n\n\nImageNet\n\n\nDescription: Over 14 million images categorized into over 20,000 classes.\nURL: ImageNet\n\n\nMNIST\n\n\nDescription: 70,000 28x28 grayscale images of handwritten digits in 10 classes.\nURL: MNIST\n\n\nFashion-MNIST\n\n\nDescription: 70,000 28x28 grayscale images of 10 fashion categories.\nURL: Fashion-MNIST",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image Multi Classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_multi_classification.html#popular-models",
    "href": "Computer_Vision/image_multi_classification.html#popular-models",
    "title": "Image Multi Classification",
    "section": "Popular Models",
    "text": "Popular Models\n\nConvolutional Neural Networks (CNNs)\n\n\nExamples: LeNet, AlexNet, VGG, ResNet, DenseNet\n\n\nResidual Networks (ResNet)\n\n\nDescription: Introduces skip connections to prevent vanishing gradients.\nURL: ResNet\n\n\nDenseNet\n\n\nDescription: Connects each layer to every other layer in a feed-forward fashion.\nURL: DenseNet\n\n\nInception Networks (GoogLeNet)\n\n\nDescription: Uses multiple types of convolutions in parallel to capture different features.\nURL: Inception\n\n\nEfficientNet\n\n\nDescription: Scales up model size while balancing network depth, width, and resolution.\nURL: EfficientNet",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image Multi Classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_multi_classification.html#hyperparameters",
    "href": "Computer_Vision/image_multi_classification.html#hyperparameters",
    "title": "Image Multi Classification",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nLearning Rate\n\n\nDescription: Controls how much to change the model in response to the estimated error each time the model weights are updated.\n\n\nBatch Size\n\n\nDescription: The number of training examples utilized in one iteration.\n\n\nNumber of Epochs\n\n\nDescription: The number of complete passes through the training dataset.\n\n\nOptimizer\n\n\nExamples: SGD, Adam, RMSprop\n\n\nWeight Initialization\n\n\nExamples: Xavier, He initialization\n\n\nDropout Rate\n\n\nDescription: Fraction of the input units to drop.\n\n\nLearning Rate Decay\n\n\nDescription: Reduces the learning rate as training progresses.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image Multi Classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_multi_classification.html#popular-loss-functions",
    "href": "Computer_Vision/image_multi_classification.html#popular-loss-functions",
    "title": "Image Multi Classification",
    "section": "Popular Loss Functions",
    "text": "Popular Loss Functions\n\nCross-Entropy Loss\n\n\nDescription: Measures the performance of a classification model whose output is a probability value between 0 and 1.\n\n\nHinge Loss\n\n\nDescription: Used for â€œmaximum-marginâ€ classification, mostly for SVMs.\n\n\nKullback-Leibler Divergence Loss\n\n\nDescription: Measures how one probability distribution diverges from a second, expected probability distribution.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image Multi Classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_multi_classification.html#popular-evaluation-metrics",
    "href": "Computer_Vision/image_multi_classification.html#popular-evaluation-metrics",
    "title": "Image Multi Classification",
    "section": "Popular Evaluation Metrics",
    "text": "Popular Evaluation Metrics\n\nAccuracy\n\n\nDescription: The ratio of correctly predicted observation to the total observations.\n\n\nPrecision\n\n\nDescription: The ratio of correctly predicted positive observations to the total predicted positives.\n\n\nRecall (Sensitivity)\n\n\nDescription: The ratio of correctly predicted positive observations to the all observations in actual class.\n\n\nF1 Score\n\n\nDescription: The weighted average of Precision and Recall.\n\n\nConfusion Matrix\n\n\nDescription: A table used to describe the performance of a classification model.\n\n\nROC-AUC\n\n\nDescription: A graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image Multi Classification"
    ]
  },
  {
    "objectID": "Computer_Vision/image_multi_classification.html#other-important-topics",
    "href": "Computer_Vision/image_multi_classification.html#other-important-topics",
    "title": "Image Multi Classification",
    "section": "Other Important Topics",
    "text": "Other Important Topics\n\nData Augmentation\n\n\nDescription: Techniques used to increase the diversity of your training data without actually collecting new data.\nExamples: Rotation, Translation, Scaling, Flipping, Adding Noise\n\n\nTransfer Learning\n\n\nDescription: Reusing a pre-trained model on a new problem.\nExample: Fine-tuning a model pre-trained on ImageNet for a new classification task.\n\n\nRegularization Techniques\n\n\nExamples: L1/L2 regularization, Dropout, Early Stopping\n\n\nEnsemble Methods\n\n\nDescription: Combining the predictions of multiple models to improve generalizability and robustness.\nExamples: Bagging, Boosting, Stacking\n\n\nFine-Tuning\n\n\nDescription: Adjusting the parameters of an existing model to better fit the new data.\n\n\nHyperparameter Tuning\n\n\nTechniques: Grid Search, Random Search, Bayesian Optimization\n\n\nModel Interpretability\n\n\nTechniques: SHAP values, LIME",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Image Multi Classification"
    ]
  },
  {
    "objectID": "Computer_Vision/semantic_segmentation.html",
    "href": "Computer_Vision/semantic_segmentation.html",
    "title": "Semantic Segmentation",
    "section": "",
    "text": "A wide variety of datasets are available for training and evaluating semantic segmentation models. Each dataset has unique characteristics depending on the application domain, such as autonomous driving, general object segmentation, or medical imaging.\n\n\n\nCityscapes\n\nDescription: Large-scale dataset for urban scene understanding. Includes street-level imagery from various German cities.\nClasses: 30 classes (e.g., road, person, car, building).\nSize: 5,000 fine-annotated images (2975 train, 500 validation, 1525 test), 20,000 coarse annotations.\nResolution: 2048Ã—1024 pixels.\nLink: Cityscapes Dataset\n\nMapillary Vistas\n\nDescription: A diverse dataset collected from various countries, designed for street-level segmentation tasks.\nClasses: 66 object categories.\nSize: 25,000 annotated high-resolution images.\nResolution: Varies, high resolution.\nLink: Mapillary Vistas\n\nCamVid\n\nDescription: One of the earlier datasets for autonomous driving, with video sequences.\nClasses: 32 classes.\nSize: 701 labeled frames.\nResolution: 960Ã—720 pixels.\nLink: CamVid Dataset\n\n\n\n\n\n\nPASCAL VOC 2012\n\nDescription: Widely used dataset for image classification, detection, and segmentation tasks.\nClasses: 21 object categories.\nSize: 1,464 images (train), 1,449 (val), 1,456 (test).\nResolution: Varies.\nLink: PASCAL VOC\n\nMS COCO (Common Objects in Context)\n\nDescription: Large-scale object detection, segmentation, and captioning dataset.\nClasses: 80 object categories.\nSize: 123,287 images with pixel-wise annotations.\nResolution: Varies.\nLink: MS COCO\n\n\n\n\n\n\nLung Nodule Analysis (LUNA)\n\nDescription: A dataset for detecting and segmenting lung nodules in CT scans.\nClasses: Lung nodules.\nSize: 888 CT scans.\nResolution: Voxel-wise 3D data.\nLink: LUNA Dataset\n\nBraTS (Brain Tumor Segmentation)\n\nDescription: A dataset for segmenting brain tumors using MRI scans.\nClasses: Tumor, normal tissue.\nSize: 500+ 3D MRI images.\nResolution: Voxel-wise 3D data.\nLink: BraTS Dataset",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Semantic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/semantic_segmentation.html#popular-datasets-for-semantic-segmentation",
    "href": "Computer_Vision/semantic_segmentation.html#popular-datasets-for-semantic-segmentation",
    "title": "Semantic Segmentation",
    "section": "",
    "text": "A wide variety of datasets are available for training and evaluating semantic segmentation models. Each dataset has unique characteristics depending on the application domain, such as autonomous driving, general object segmentation, or medical imaging.\n\n\n\nCityscapes\n\nDescription: Large-scale dataset for urban scene understanding. Includes street-level imagery from various German cities.\nClasses: 30 classes (e.g., road, person, car, building).\nSize: 5,000 fine-annotated images (2975 train, 500 validation, 1525 test), 20,000 coarse annotations.\nResolution: 2048Ã—1024 pixels.\nLink: Cityscapes Dataset\n\nMapillary Vistas\n\nDescription: A diverse dataset collected from various countries, designed for street-level segmentation tasks.\nClasses: 66 object categories.\nSize: 25,000 annotated high-resolution images.\nResolution: Varies, high resolution.\nLink: Mapillary Vistas\n\nCamVid\n\nDescription: One of the earlier datasets for autonomous driving, with video sequences.\nClasses: 32 classes.\nSize: 701 labeled frames.\nResolution: 960Ã—720 pixels.\nLink: CamVid Dataset\n\n\n\n\n\n\nPASCAL VOC 2012\n\nDescription: Widely used dataset for image classification, detection, and segmentation tasks.\nClasses: 21 object categories.\nSize: 1,464 images (train), 1,449 (val), 1,456 (test).\nResolution: Varies.\nLink: PASCAL VOC\n\nMS COCO (Common Objects in Context)\n\nDescription: Large-scale object detection, segmentation, and captioning dataset.\nClasses: 80 object categories.\nSize: 123,287 images with pixel-wise annotations.\nResolution: Varies.\nLink: MS COCO\n\n\n\n\n\n\nLung Nodule Analysis (LUNA)\n\nDescription: A dataset for detecting and segmenting lung nodules in CT scans.\nClasses: Lung nodules.\nSize: 888 CT scans.\nResolution: Voxel-wise 3D data.\nLink: LUNA Dataset\n\nBraTS (Brain Tumor Segmentation)\n\nDescription: A dataset for segmenting brain tumors using MRI scans.\nClasses: Tumor, normal tissue.\nSize: 500+ 3D MRI images.\nResolution: Voxel-wise 3D data.\nLink: BraTS Dataset",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Semantic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/semantic_segmentation.html#popular-models-for-semantic-segmentation",
    "href": "Computer_Vision/semantic_segmentation.html#popular-models-for-semantic-segmentation",
    "title": "Semantic Segmentation",
    "section": "2. Popular Models for Semantic Segmentation",
    "text": "2. Popular Models for Semantic Segmentation\n\na. Fully Convolutional Networks (FCN)\n\nArchitecture: The first major deep learning-based approach for semantic segmentation. FCNs use convolutional layers for both feature extraction and pixel-wise classification.\nKey Idea: Replace fully connected layers with convolutional layers for dense pixel prediction.\nReferences: Long et al., 2015, FCN Paper\n\n\n\nb. U-Net\n\nArchitecture: Symmetrical â€œUâ€-shaped network with encoder-decoder architecture.\nKey Idea: Encoder extracts features, and the decoder upsamples to full resolution using skip connections to recover spatial information.\nApplications: Extremely popular in medical imaging.\nReferences: Ronneberger et al., 2015, U-Net Paper\n\n\n\nc.Â DeepLab (v1, v2, v3, v3+)\n\nArchitecture: Encoder-decoder architecture with Atrous (dilated) convolutions to capture multi-scale context.\nKey Idea: Atrous convolutions and ASPP (Atrous Spatial Pyramid Pooling) enable capturing objects at multiple scales.\nDeepLab v3+: Combines spatial pyramid pooling with a decoder module for better object boundary detection.\nReferences: Chen et al., 2017, DeepLab Paper\n\n\n\nd.Â SegNet\n\nArchitecture: Encoder-decoder architecture that recovers resolution using max-pooling indices from the encoder.\nKey Idea: Efficient upsampling using indices from max-pooling in the encoder, which reduces computational cost.\nReferences: Badrinarayanan et al., 2015, SegNet Paper\n\n\n\ne. PSPNet (Pyramid Scene Parsing Network)\n\nArchitecture: Uses a pyramid pooling module to capture multi-scale global context.\nKey Idea: Captures global scene-level context using pyramid pooling before the final pixel-wise prediction.\nReferences: Zhao et al., 2017, PSPNet Paper\n\n\n\nf.Â HRNet (High-Resolution Network)\n\nArchitecture: Maintains high-resolution feature maps throughout the network while using multi-scale fusion.\nKey Idea: Avoids the downsampling-heavy nature of many segmentation networks, preserving spatial detail.\nReferences: Wang et al., 2020, HRNet Paper",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Semantic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/semantic_segmentation.html#important-hyperparameters",
    "href": "Computer_Vision/semantic_segmentation.html#important-hyperparameters",
    "title": "Semantic Segmentation",
    "section": "3. Important Hyperparameters",
    "text": "3. Important Hyperparameters\nTuning hyperparameters is crucial to improving the performance of segmentation models. Below are the most important ones:\n\na. Learning Rate\n\nDescription: Controls the step size during optimization.\nTypical Range: 0.0001 â€“ 0.01 (with learning rate schedules like cosine annealing, or warm restarts).\n\n\n\nb. Batch Size\n\nDescription: The number of samples processed before the model is updated.\nTypical Range: 2 â€“ 16 (for large images due to memory constraints).\n\n\n\nc.Â Number of Filters / Feature Maps\n\nDescription: Number of filters in convolutional layers, which controls model capacity.\nTypical Range: 32 â€“ 512 per layer, depending on model depth and complexity.\n\n\n\nd.Â Optimizer\n\nPopular Choices:\n\nAdam (adaptive learning rate).\nSGD with momentum (common in large-scale datasets).\n\n\n\n\ne. Weight Decay / L2 Regularization\n\nDescription: Helps prevent overfitting by penalizing large weights.\nTypical Range: 0.0001 â€“ 0.001.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Semantic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/semantic_segmentation.html#popular-loss-functions",
    "href": "Computer_Vision/semantic_segmentation.html#popular-loss-functions",
    "title": "Semantic Segmentation",
    "section": "4. Popular Loss Functions",
    "text": "4. Popular Loss Functions\nIn semantic segmentation, the loss function plays a critical role in training the network by minimizing pixel-wise errors. Common loss functions include:\n\na. Cross-Entropy Loss\n\nDescription: The most common loss for multi-class pixel-wise classification tasks.\nFormula: \\(L = - \\sum_{i} y_i \\log(\\hat{y}_i)\\) where $ y_i $ is the true label and $ _i $ is the predicted probability.\n\n\n\nb. Dice Loss\n\nDescription: Measures the overlap between predicted and ground truth segmentation.\nFormula: \\(L = 1 - \\frac{2 |Y \\cap \\hat{Y}|}{|Y| + |\\hat{Y}|}\\) where $ Y $ is the ground truth mask, and $ $ is the predicted mask.\n\n\n\nc.Â Intersection over Union (IoU) Loss\n\nDescription: Measures the overlap between the predicted and ground truth areas.\nFormula: \\(\\text{IoU} = \\frac{|Y \\cap \\hat{Y}|}{|Y \\cup \\hat{Y}|}\\) where $ |Y| $ is the area of ground truth and $ || $ is the area of prediction.\n\n\n\nd.Â Tversky Loss\n\nDescription: A generalization of Dice Loss, controlling false positives and false negatives.\nFormula: $ L = 1 - $",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Semantic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/semantic_segmentation.html#other-important-topics",
    "href": "Computer_Vision/semantic_segmentation.html#other-important-topics",
    "title": "Semantic Segmentation",
    "section": "5. Other Important Topics",
    "text": "5. Other Important Topics\n\na. Data Augmentation\n\nTechniques: Random crop, horizontal/vertical flipping, color jittering, and elastic deformation.\nPurpose: Prevent overfitting and improve generalization.\n\n\n\nb. Post-Processing Techniques\n\nCRF (Conditional Random Field): Often used as a post-processing step to refine segmentation boundaries by enforcing spatial consistency.\n\n\n\nc.Â Evaluation Metrics\n\nMean Intersection over Union (mIoU): The most widely used evaluation metric for segmentation tasks.\nPixel Accuracy: The ratio of correctly predicted pixels to total pixels.",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Semantic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/semantic_segmentation.html#references",
    "href": "Computer_Vision/semantic_segmentation.html#references",
    "title": "Semantic Segmentation",
    "section": "6. References",
    "text": "6. References\n\nLong, J., Shelhamer, E., & Darrell, T. (2015). â€œFully Convolutional Networks for Semantic Segmentation.â€ CVPR. Link\nRonneberger, O., Fischer, P., & Brox, T. (2015). â€œU-Net: Convolutional Networks for Biomedical Image Segmentation.â€ MICCAI. Link\nChen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2017). â€œDeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.â€ PAMI. Link\nZhao, H., Shi, J., Qi, X., Wang, X., & Jia, J. (2017). â€œPyramid Scene Parsing Network.â€ CVPR. Link\nWang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., & Liu, W. (2020). â€œDeep High-Resolution Representation Learning for Visual Recognition.â€ PAMI. Link",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Semantic Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/instance_segmentation.html#models",
    "href": "Computer_Vision/instance_segmentation.html#models",
    "title": "Instance Segmentation",
    "section": "Models",
    "text": "Models\nMask R-CNN (extends Faster R-CNN by adding a branch for predicting segmentation masks for each region of interest).",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Instance Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/instance_segmentation.html#hyperparameters",
    "href": "Computer_Vision/instance_segmentation.html#hyperparameters",
    "title": "Instance Segmentation",
    "section": "Hyperparameters",
    "text": "Hyperparameters",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Instance Segmentation"
    ]
  },
  {
    "objectID": "Computer_Vision/instance_segmentation.html#loss-functions",
    "href": "Computer_Vision/instance_segmentation.html#loss-functions",
    "title": "Instance Segmentation",
    "section": "Loss Functions",
    "text": "Loss Functions",
    "crumbs": [
      "Blog",
      "Computer Vision",
      "Instance Segmentation"
    ]
  },
  {
    "objectID": "model_diagrams.html",
    "href": "model_diagrams.html",
    "title": "Model Diagrams",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n#Option 1 (create nn modules)\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n\n        return out\n\n\n# Image Classifier Neural Network\nclass ImageClassifier(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(64, 64, (3,3)), \n            nn.ReLU(),\n            nn.Flatten(), \n            nn.Linear(64*(28-2)*(28-2), 10)  \n        )\n\n    def forward(self, x): \n        return self.model(x)\n\n\nmodel = ImageClassifier()\nmodel\n\nImageClassifier(\n  (model): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=43264, out_features=10, bias=True)\n  )\n)\n\n\n\nfor item in model.state_dict():\n    print(f'{item}        size: {model.state_dict()[item].shape}')\n\nmodel.0.weight        size: torch.Size([32, 3, 3, 3])\nmodel.0.bias        size: torch.Size([32])\nmodel.2.weight        size: torch.Size([64, 32, 3, 3])\nmodel.2.bias        size: torch.Size([64])\nmodel.4.weight        size: torch.Size([64, 64, 3, 3])\nmodel.4.bias        size: torch.Size([64])\nmodel.7.weight        size: torch.Size([10, 43264])\nmodel.7.bias        size: torch.Size([10])\n\n\n\nfor param in model.parameters():\n    print(param.shape)\n\ntorch.Size([32, 3, 3, 3])\ntorch.Size([32])\ntorch.Size([64, 32, 3, 3])\ntorch.Size([64])\ntorch.Size([64, 64, 3, 3])\ntorch.Size([64])\ntorch.Size([10, 43264])\ntorch.Size([10])\n\n\n\nfor child in model.children():\n    print(child)\n\nSequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (3): ReLU()\n  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (5): ReLU()\n  (6): Flatten(start_dim=1, end_dim=-1)\n  (7): Linear(in_features=43264, out_features=10, bias=True)\n)\n\n\n\na = 'sadf(asdf(123(123'\na.split('(', 1)\n\n['sadf', 'asdf(123(123']\n\n\n\n# Iterate through each layer in the model\nfor name, layer in model.named_modules():\n    layer_name = str(layer).split('(', 1)[0]\n    print(f'[name:{name}] layer:{layer_name}')\n\n[name:] layer:ImageClassifier\n[name:model] layer:Sequential\n[name:model.0] layer:Conv2d\n[name:model.1] layer:ReLU\n[name:model.2] layer:Conv2d\n[name:model.3] layer:ReLU\n[name:model.4] layer:Conv2d\n[name:model.5] layer:ReLU\n[name:model.6] layer:Flatten\n[name:model.7] layer:Linear\n\n\n\ndef create_model_diagram(model):\n    from nbdevAuto.functions import graph\n    from graphviz import Digraph\n    dot = graph()\n    dot.attr(rankdir='TB')\n\n    for name, layer in model.named_modules():\n        layer_name = str(layer).split('(', 1)[0]\n        node = f'{name}\\n{layer_name}'\n        dot.node(node, node)\n\n    for index, (name, layer) in enumerate(model.named_modules()):\n        layer_name = str(layer).split('(', 1)[0]\n        node = f'{name}\\n{layer_name}'\n        if index == 0:\n            previous = node\n            continue    \n        \n        dot.edge(previous, node)\n        previous = node\n\n    # Render the graph\n    return dot\n\n\ncreate_model_diagram(model)\n\n\n\n\n\n\n\n\n\nfrom torchviz import make_dot\n\n\n# Define a dummy input tensor\ndummy_input = torch.randn(1, 3, 32, 32)\n\n# Perform a forward pass\noutput = model(dummy_input)\n\n# Visualize the model's forward structure\n# This will create a graph representing the forward pass\ngraph = make_dot(output, params=dict(model.named_parameters()))\ngraph\n\n\n\n\n\n\n\n\n\ngraph.render(\"forward_structure\", format=\"png\", cleanup=True)\n\n'forward_structure.png'",
    "crumbs": [
      "Blog",
      "Model Diagrams"
    ]
  },
  {
    "objectID": "model_diagrams.html#neutral-network",
    "href": "model_diagrams.html#neutral-network",
    "title": "Model Diagrams",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n#Option 1 (create nn modules)\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n\n        return out\n\n\n# Image Classifier Neural Network\nclass ImageClassifier(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(64, 64, (3,3)), \n            nn.ReLU(),\n            nn.Flatten(), \n            nn.Linear(64*(28-2)*(28-2), 10)  \n        )\n\n    def forward(self, x): \n        return self.model(x)\n\n\nmodel = ImageClassifier()\nmodel\n\nImageClassifier(\n  (model): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=43264, out_features=10, bias=True)\n  )\n)\n\n\n\nfor item in model.state_dict():\n    print(f'{item}        size: {model.state_dict()[item].shape}')\n\nmodel.0.weight        size: torch.Size([32, 3, 3, 3])\nmodel.0.bias        size: torch.Size([32])\nmodel.2.weight        size: torch.Size([64, 32, 3, 3])\nmodel.2.bias        size: torch.Size([64])\nmodel.4.weight        size: torch.Size([64, 64, 3, 3])\nmodel.4.bias        size: torch.Size([64])\nmodel.7.weight        size: torch.Size([10, 43264])\nmodel.7.bias        size: torch.Size([10])\n\n\n\nfor param in model.parameters():\n    print(param.shape)\n\ntorch.Size([32, 3, 3, 3])\ntorch.Size([32])\ntorch.Size([64, 32, 3, 3])\ntorch.Size([64])\ntorch.Size([64, 64, 3, 3])\ntorch.Size([64])\ntorch.Size([10, 43264])\ntorch.Size([10])\n\n\n\nfor child in model.children():\n    print(child)\n\nSequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (3): ReLU()\n  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (5): ReLU()\n  (6): Flatten(start_dim=1, end_dim=-1)\n  (7): Linear(in_features=43264, out_features=10, bias=True)\n)\n\n\n\na = 'sadf(asdf(123(123'\na.split('(', 1)\n\n['sadf', 'asdf(123(123']\n\n\n\n# Iterate through each layer in the model\nfor name, layer in model.named_modules():\n    layer_name = str(layer).split('(', 1)[0]\n    print(f'[name:{name}] layer:{layer_name}')\n\n[name:] layer:ImageClassifier\n[name:model] layer:Sequential\n[name:model.0] layer:Conv2d\n[name:model.1] layer:ReLU\n[name:model.2] layer:Conv2d\n[name:model.3] layer:ReLU\n[name:model.4] layer:Conv2d\n[name:model.5] layer:ReLU\n[name:model.6] layer:Flatten\n[name:model.7] layer:Linear\n\n\n\ndef create_model_diagram(model):\n    from nbdevAuto.functions import graph\n    from graphviz import Digraph\n    dot = graph()\n    dot.attr(rankdir='TB')\n\n    for name, layer in model.named_modules():\n        layer_name = str(layer).split('(', 1)[0]\n        node = f'{name}\\n{layer_name}'\n        dot.node(node, node)\n\n    for index, (name, layer) in enumerate(model.named_modules()):\n        layer_name = str(layer).split('(', 1)[0]\n        node = f'{name}\\n{layer_name}'\n        if index == 0:\n            previous = node\n            continue    \n        \n        dot.edge(previous, node)\n        previous = node\n\n    # Render the graph\n    return dot\n\n\ncreate_model_diagram(model)\n\n\n\n\n\n\n\n\n\nfrom torchviz import make_dot\n\n\n# Define a dummy input tensor\ndummy_input = torch.randn(1, 3, 32, 32)\n\n# Perform a forward pass\noutput = model(dummy_input)\n\n# Visualize the model's forward structure\n# This will create a graph representing the forward pass\ngraph = make_dot(output, params=dict(model.named_parameters()))\ngraph\n\n\n\n\n\n\n\n\n\ngraph.render(\"forward_structure\", format=\"png\", cleanup=True)\n\n'forward_structure.png'",
    "crumbs": [
      "Blog",
      "Model Diagrams"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html",
    "href": "datasets_and_dataloaders.html",
    "title": "Dataset and Dataloaders",
    "section": "",
    "text": "from torchvision import datasets\ndatasets.__all__\n\n('LSUN',\n 'LSUNClass',\n 'ImageFolder',\n 'DatasetFolder',\n 'FakeData',\n 'CocoCaptions',\n 'CocoDetection',\n 'CIFAR10',\n 'CIFAR100',\n 'EMNIST',\n 'FashionMNIST',\n 'QMNIST',\n 'MNIST',\n 'KMNIST',\n 'StanfordCars',\n 'STL10',\n 'SUN397',\n 'SVHN',\n 'PhotoTour',\n 'SEMEION',\n 'Omniglot',\n 'SBU',\n 'Flickr8k',\n 'Flickr30k',\n 'Flowers102',\n 'VOCSegmentation',\n 'VOCDetection',\n 'Cityscapes',\n 'ImageNet',\n 'Caltech101',\n 'Caltech256',\n 'CelebA',\n 'WIDERFace',\n 'SBDataset',\n 'VisionDataset',\n 'USPS',\n 'Kinetics',\n 'HMDB51',\n 'UCF101',\n 'Places365',\n 'Kitti',\n 'INaturalist',\n 'LFWPeople',\n 'LFWPairs',\n 'KittiFlow',\n 'Sintel',\n 'FlyingChairs',\n 'FlyingThings3D',\n 'HD1K',\n 'Food101',\n 'DTD',\n 'FER2013',\n 'GTSRB',\n 'CLEVRClassification',\n 'OxfordIIITPet',\n 'PCAM',\n 'Country211',\n 'FGVCAircraft',\n 'EuroSAT',\n 'RenderedSST2',\n 'Kitti2012Stereo',\n 'Kitti2015Stereo',\n 'CarlaStereo',\n 'Middlebury2014Stereo',\n 'CREStereo',\n 'FallingThingsStereo',\n 'SceneFlowStereo',\n 'SintelStereo',\n 'InStereo2k',\n 'ETH3DStereo',\n 'wrap_dataset_for_transforms_v2',\n 'Imagenette')",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#mnist",
    "href": "datasets_and_dataloaders.html#mnist",
    "title": "Dataset and Dataloaders",
    "section": "MNIST",
    "text": "MNIST\n\nfrom torchvision import datasets, transforms\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\npath = './Data'\n# Define transforms for preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),          # Convert image to tensor\n    transforms.Normalize((0.5,), (0.5,))  # Normalize image pixel values to range [-1, 1]\n])\n\n# Define batch size for data loader\nbatch_size = 64\n\n# Create train and test datasets\ntrain_dataset = datasets.MNIST(root=path, train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root=path, train=False, download=True, transform=transform)\n\n# Create train and test data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\nlen(train_dataset), len(test_dataset)\n\n(60000, 10000)\n\n\n\nimage, label = train_dataset[1]\nplt.imshow(transforms.ToPILImage()(image), cmap='gray')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nexamples = iter(train_loader)\n\n\nimages, labels = next(examples)\nimages.shape, labels.shape\n\n(torch.Size([64, 1, 28, 28]), torch.Size([64]))\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n        \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.1, hspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Convert image to numpy array and adjust pixel values\n        img_np = images[i].numpy().transpose((1, 2, 0))\n        img_np = (img_np + 1) / 2  # Adjust pixel values to range [0, 1]\n        \n        # Display image\n        ax.imshow(img_np, cmap='gray')\n        ax.axis('off')\n        ax.set_title(f'Label: {labels[i]}')\n    plt.show()\n\n\nshow_images(images, labels)",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#cifar10",
    "href": "datasets_and_dataloaders.html#cifar10",
    "title": "Dataset and Dataloaders",
    "section": "Cifar10",
    "text": "Cifar10\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\npath = 'Data'\n\n# Define transforms for preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),                       # Convert image to tensor\n    transforms.Normalize((0.5, 0.5, 0.5),       # Normalize image pixel values to range [-1, 1]\n                         (0.5, 0.5, 0.5))\n])\n\n# Define batch size for data loader\n\n\n# Create train and test datasets\ntrain_dataset = datasets.CIFAR10(root=path, train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root=path, train=False, download=True, transform=transform)\n\n# Create train and test data loaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\nlen(train_dataset), len(test_dataset)\n\n(50000, 10000)\n\n\n\nimages, labels = train_dataset[1]\n\n\ntype(labels)\n\nint\n\n\n\nclasses = train_dataset.class_to_idx\n\n\nclasses = list(train_dataset.class_to_idx)\n\n\nlist(classes)\n\n['airplane',\n 'automobile',\n 'bird',\n 'cat',\n 'deer',\n 'dog',\n 'frog',\n 'horse',\n 'ship',\n 'truck']\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n        \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Convert image to numpy array and adjust pixel values\n        img_np = images[i].numpy().transpose((1, 2, 0))\n        img_np = (img_np + 1) / 2  # Adjust pixel values to range [0, 1]\n        \n        # Display image\n        ax.imshow(img_np)\n        ax.axis('off')\n        ax.set_title(f' {classes[labels[i]]}')\n    plt.show()\n\n\n# Get a batch of images and labels from the data loader\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\ntype(labels)\n\ntorch.Tensor\n\n\n\nimages.shape, labels.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\n# Display the images\nshow_images(images, labels)",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#imagenette",
    "href": "datasets_and_dataloaders.html#imagenette",
    "title": "Dataset and Dataloaders",
    "section": "Imagenette",
    "text": "Imagenette\n\nfrom torchvision.datasets import ImageFolder\nfrom tqdm import tqdm\n\n\n# Define transformation to convert images to tensors\ntransform = transforms.Compose([\n    transforms.Resize(256),     # Resize images to 256x256\n    transforms.CenterCrop(224), # Crop the center 224x224 region\n    transforms.ToTensor()       # Convert images to PyTorch tensors\n])\n\n# Load Imagenette dataset\nimagenette_dataset = ImageFolder(root='Data/Imagenette_depth/imagenette2', transform=transform)\nimagenette_dataset\n\nDataset ImageFolder\n    Number of datapoints: 13394\n    Root location: Data/Imagenette_depth/imagenette2\n    StandardTransform\nTransform: Compose(\n               Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n               CenterCrop(size=(224, 224))\n               ToTensor()\n           )\n\n\n\n# Calculate mean and standard deviation\nloader = torch.utils.data.DataLoader(imagenette_dataset, batch_size=128, shuffle=False)\nmean_list = []\nstd_list = []\n\nfor image, label in tqdm(loader):\n    mean = image.mean(dim=[0, 2, 3])  # Calculate mean across batch, height, and width\n    std = image.std(dim=[0, 2, 3])    # Calculate standard deviation across batch, height, and width\n    mean_list.append(mean)\n    std_list.append(std)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [03:44&lt;00:00,  2.14s/it]\n\n\n\nmean_tensor = torch.stack(mean_list)\nstd_tensor = torch.stack(std_list)\n\n\nmean_tensor.mean(dim=[0]), std_tensor.mean(dim=[0])\n\n(tensor([0.4654, 0.4544, 0.4252]), tensor([0.2761, 0.2679, 0.2839]))\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Define the transformations to apply to the images\ntransform = transforms.Compose([\n    transforms.Resize((375, 500)),\n    transforms.ToTensor(),\n])\n\n# Download and load the Imagenette dataset\ntrain_dataset = datasets.Imagenette(root='Data',\n                                    split='train',\n                                    # download=True,\n                                    transform=transform,\n                                    )\n\n\n# Download and load the Imagenette dataset\ntest_dataset = datasets.Imagenette(root='Data',\n                                  split='val',\n                                  # download=True,\n                                  transform=transform,\n                                 )\n\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\ntrain_dataset.__dict__.keys()\n\ndict_keys(['root', 'transform', 'target_transform', 'transforms', '_split', '_size', '_url', '_md5', '_size_root', '_image_root', 'wnids', 'wnid_to_idx', 'classes', 'class_to_idx', '_samples'])\n\n\n\ntrain_dataset.classes\n\n[('tench', 'Tinca tinca'),\n ('English springer', 'English springer spaniel'),\n ('cassette player',),\n ('chain saw', 'chainsaw'),\n ('church', 'church building'),\n ('French horn', 'horn'),\n ('garbage truck', 'dustcart'),\n ('gas pump', 'gasoline pump', 'petrol pump', 'island dispenser'),\n ('golf ball',),\n ('parachute', 'chute')]\n\n\n\nlen(train_dataset), len(test_dataset)\n\n(9469, 3925)\n\n\n\nimages, labels = train_dataset[10]\nimages.shape\n\ntorch.Size([3, 375, 500])\n\n\n\n# Define a function to display images\ndef show_image(dataset):\n    # Access an image and its label from the dataset\n    image, label = dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image.numpy().transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_np)\n    plt.axis('off')\n    plt.title(f' {train_dataset.classes[label][0]}')\n    plt.show()\n    \n   \n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Convert image to numpy array and adjust pixel values\n        img_np = images[i].numpy().transpose((1, 2, 0))\n        \n        # Display image\n        ax.imshow(img_np)\n        ax.axis('off')\n        ax.set_title(f' {train_dataset.classes[labels[i]][0]}')\n    plt.show()\n\n\nshow_image(train_dataset[2])\n\n\n\n\n\n\n\n\n\n# Get a batch of images and labels from the data loader\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nshow_images(images, labels)",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#country211",
    "href": "datasets_and_dataloaders.html#country211",
    "title": "Dataset and Dataloaders",
    "section": "Country211",
    "text": "Country211\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms, datasets, utils\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\nimport torch\nimport os\nfrom PIL import Image\n\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport timm\nimport numpy as np\nfrom datetime import datetime\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda')\n\n\n\nfrom torchvision import transforms\n\n# Define transforms to apply to the images\ntransform_default = transforms.Compose([\n    transforms.Resize((200, 300)),  # Resize images to a fixed size\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    # Add more transformations as needed (e.g., normalization)\n])\n\n# Path to the root directory of the dataset\nroot_dir = 'Data'\n\n# Create datasets\ntrain_dataset = datasets.Country211(root_dir,\n                                   split = 'train',\n                                    transform=transform_default,\n                                    download = False)\nval_dataset = datasets.Country211(root_dir,\n                                   split = 'valid',\n                                    transform=transform_default,\n                                    download = False)\ntest_dataset = datasets.Country211(root_dir,\n                                   split = 'test',\n                                    transform=transform_default,\n                                    download = False)\n\n\nimages, label = train_dataset[1000]\n\n\nimages.shape\n\ntorch.Size([3, 200, 300])\n\n\n\nlen(train_dataset.classes)\n\n211\n\n\n\nlen(train_dataset), len(val_dataset), len(test_dataset)\n\n(31650, 10550, 21100)\n\n\n\ntrain_dataset\n\nDataset Country211\n    Number of datapoints: 31650\n    Root location: Data\n    StandardTransform\nTransform: Compose(\n               Resize(size=(200, 300), interpolation=bilinear, max_size=None, antialias=True)\n               ToTensor()\n           )\n\n\n\ntrain_dataset.__dict__.keys()\n\ndict_keys(['_split', 'root', '_base_folder', 'transform', 'target_transform', 'transforms', 'loader', 'extensions', 'classes', 'class_to_idx', 'samples', 'targets', 'imgs'])\n\n\n\n# Define a function to display images\ndef show_image(dataset):\n    # Access an image and its label from the dataset\n    image, label = dataset\n    # Convert the image tensor to a NumPy array\n    image_np = image.numpy().transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_np.clip(0,1))\n    plt.axis('off')\n    plt.title(f' {train_dataset.classes[label]}')\n    plt.show()\n    \n   \n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        img_np = image.numpy().transpose((1, 2, 0))\n        \n        # Display image\n        ax.imshow(img_np.clip(0,1))\n        ax.axis('off')\n        ax.set_title(f' {train_dataset.classes[label]}')\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n        \n    plt.show()\n\n\nshow_image(train_dataset[6])\n\n\n\n\n\n\n\n\n\ndef loaders(batch_size):\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              num_workers=8)\n    val_loader = DataLoader(val_dataset,\n                             batch_size=batch_size,\n                             shuffle=True, \n                             num_workers=8)\n    test_loader = DataLoader(test_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              num_workers=8)\n\n    # dataloaders = {'train': train_loader, 'val': test_loader}\n    # dataset_sizes = {'train': len(train_dataset), 'val': len(test_dataset) }\n    return train_loader, val_loader, test_loader\n\n\nbatch_size = 32\ntrain_loader, val_loader, test_loader = loaders(batch_size)\n\n\n# Get a batch of images and labels from the data loader\nexamples = iter(test_loader)\nimages, labels = next(examples)\n# ############## TENSORBOARD ########################\n# img_grid = utils.make_grid(images)\n\n# writer.add_image('Imagenette', img_grid)\n# writer.flush()\n# #sys.exit()\n# ###################################################\n\n\nshow_images(images, labels)\n\n\n\n\n\n\n\n\n\ndef find_mean_std(loader):\n    mean_list = []\n    std_list = []\n    for images, label in tqdm(loader):\n        mean, std = images.mean([0,2,3]), images.std([0,2,3])  \n        mean_list.append(mean)\n        std_list.append(std)\n    \n    mean_tensor = torch.stack(mean_list)\n    std_tensor = torch.stack(std_list)\n    return mean_tensor.mean(dim=[0]), std_tensor.mean(dim=[0])\n\n\ntrain_norm = find_mean_std(train_loader)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 990/990 [00:32&lt;00:00, 30.55it/s]\n\n\n(tensor([0.4571, 0.4504, 0.4209]), tensor([0.2706, 0.2646, 0.2857]))\n\n\n\nfind_mean_std(val_loader)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330/330 [00:11&lt;00:00, 29.80it/s]\n\n\n(tensor([0.4587, 0.4514, 0.4219]), tensor([0.2706, 0.2647, 0.2852]))\n\n\n\nfind_mean_std(test_loader)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 660/660 [00:25&lt;00:00, 26.12it/s]\n\n\n(tensor([0.4578, 0.4512, 0.4218]), tensor([0.2702, 0.2642, 0.2856]))",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#kitti",
    "href": "datasets_and_dataloaders.html#kitti",
    "title": "Dataset and Dataloaders",
    "section": "Kitti",
    "text": "Kitti\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torchvision.transforms import ToPILImage, v2\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\n\n# Create a histogram plot\noutput_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nclass Kitti_v2(datasets.Kitti):\n    def __init__(self, *args, **kwargs):\n        super(Kitti_v2, self).__init__(*args, **kwargs)\n\n\npath = './Data'\nbatch_size = 16\n\n# Define transforms\n# Define transforms for the dataset\ntransform2 = v2.Compose(\n    [\n        v2.ToImage(),\n        # v2.Resize(size = desired_size),  # Resize image\n        v2.RandomPhotometricDistort(p=0.2),\n        # v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n        # # v2.RandomIoUCrop(),\n        v2.RandomHorizontalFlip(p=0.4),\n        # # v2.SanitizeBoundingBoxes(),\n        v2.ToDtype(torch.float32, scale=True),\n    ]\n)\n\n\n# Load KITTI train dataset\ntrain_dataset = Kitti_v2(root=path, train='true', download=True, transform=transform2)\n\n# Load KITTI test dataset\ntest_dataset = Kitti_v2(root=path, train='false', download=True, transform=transform2)\n\n\nsample = train_dataset[1000]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\")\n\ntype(img) = &lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntype(target) = &lt;class 'list'&gt;\n\n\n\ntrain_dataset2 = datasets.wrap_dataset_for_transforms_v2(train_dataset, target_keys=(\"boxes\", \"labels\"))\n\ntest_dataset2 = datasets.wrap_dataset_for_transforms_v2(test_dataset, target_keys=(\"boxes\", \"labels\"))\n\n\nsample = train_dataset2[1000]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\nprint(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")\n\ntype(img) = &lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntype(target) = &lt;class 'dict'&gt;\ntarget.keys() = dict_keys(['boxes', 'labels'])\ntype(target['boxes']) = &lt;class 'torchvision.tv_tensors._bounding_boxes.BoundingBoxes'&gt;\ntype(target['labels']) = &lt;class 'torch.Tensor'&gt;\n\n\n\nbatch_size = 8\n\ntrain_loader = DataLoader(train_dataset2,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          collate_fn=lambda batch: tuple(zip(*batch)),\n                          num_workers = 8)\n\n# Create DataLoader for test dataset\ntest_loader = DataLoader(test_dataset2,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         collate_fn=lambda batch: tuple(zip(*batch)),\n                         num_workers = 8)\n\n\ntrain_dataset\n\nDataset Kitti_v2\n    Number of datapoints: 7481\n    Root location: ./Data\n\n\n\nlen(train_dataset)\n\n7481\n\n\n\nimage, targets = train_dataset[2]\ntype(targets)\n\nlist\n\n\n\nimage\n\n\n\n\n\n\n\n\n\ncar_types = ['Car', 'Van', 'DontCare',\n             'Cyclist', 'Pedestrian', 'Truck',\n             'Tram', 'Misc', 'Person_sitting']\n\n\ndef cv2_show(image_np, label):\n    image_cv2 = cv2.cvtColor((image_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n    if 'scores' in label:\n        for bbox, item, score in zip(label['boxes'], label['labels'], label['scores']):\n            if score &gt; 0.2:\n                cv2.rectangle(image_cv2,\n                              (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n                              (0, 255, 0), 2)\n            \n                # Display the label\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                cv2.putText(image_cv2, str(item),\n                            (int(bbox[0]), int(bbox[1]) - 10),\n                            font, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n    else:\n        for bbox, item in zip(label['boxes'], label['labels']):\n            # Draw the bounding box\n            cv2.rectangle(image_cv2,\n                          (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n                          (0, 255, 0), 2)\n        \n            # Display the label\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            cv2.putText(image_cv2, str(item),\n                        (int(bbox[0]), int(bbox[1]) - 10),\n                        font, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n\n    # Convert the image back to RGB format for display with Matplotlib\n    image_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n    \n    return image_rgb\n\ndef show_image(kitti_dataset):\n    # Access an image and its label from the dataset\n    image, label = kitti_dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image.numpy().transpose((1, 2, 0))\n    image_rgb = cv2_show(image_np, label)\n    # Display the image using Matplotlib\n    plt.imshow(image_rgb)\n    plt.axis('off')\n    plt.show()\n\n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.sqrt(len(images)))\n    ncols = int(np.floor(len(images)/nrows))\n        \n    fig, axes = plt.subplots(nrows, ncols, **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.01, hspace=0.01)\n\n    # Display the image using Matplotlib\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        image_np = image.numpy().transpose((1, 2, 0))\n        image_rgb = cv2_show(image_np, label)\n        \n        # Display image\n        ax.imshow(image_rgb)\n        ax.axis('off')\n\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n    plt.show()\n\n\nimage, label = train_dataset2[18]\nshow_image(train_dataset2[18])\n\n\n\n\n\n\n\n\n\nIter\n\n# Get a batch of images and labels from the data loader\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nshow_images(images, labels, figsize=(15, 5))",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#coco",
    "href": "datasets_and_dataloaders.html#coco",
    "title": "Dataset and Dataloaders",
    "section": "COCO",
    "text": "COCO\n\nData download\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n#\n# Only the required images will be downloaded (if necessary).\n# By default, only detections are loaded\n#\n\n\ndataset = foz.load_zoo_dataset(\n    name=\"coco-2017\",\n    dataset_dir= \"Data/coco\",\n    splits=[\"validation\",\"train\"],\n    classes=[\"person\", \"car\"],\n    max_samples=50,\n)\n\nDownloading split 'validation' to 'Data/coco/validation' if necessary\nFound annotations at 'Data/coco/raw/instances_val2017.json'\nSufficient images already downloaded\nExisting download of split 'validation' is sufficient\nDownloading split 'train' to 'Data/coco/train' if necessary\nFound annotations at 'Data/coco/raw/instances_train2017.json'\nSufficient images already downloaded\nExisting download of split 'train' is sufficient\nLoading 'coco-2017' split 'validation'\n 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [304.1ms elapsed, 0s remaining, 164.4 samples/s]     \nLoading 'coco-2017' split 'train'\n 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [291.6ms elapsed, 0s remaining, 171.5 samples/s]     \nDataset 'coco-2017-validation-train-50' created\n\n\n\nclasses = dataset.default_classes\n\n\n# Visualize the dataset in the FiftyOne App\nsession = fo.launch_app(dataset)\n\n\n        \n        \n\n\n\nWelcome to\n\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\nâ–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•\nâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\nâ–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•     â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•\nâ–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\nâ•šâ•â•     â•šâ•â•â•šâ•â•        â•šâ•â•      â•šâ•â•    â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• v0.23.8\n\nIf you're finding FiftyOne helpful, here's how you can get involved:\n\n|\n|  â­â­â­ Give the project a star on GitHub â­â­â­\n|  https://github.com/voxel51/fiftyone\n|\n|  ðŸš€ðŸš€ðŸš€ Join the FiftyOne Slack community ðŸš€ðŸš€ðŸš€\n|  https://slack.voxel51.com\n|\n\n\n\n\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\n\n# Create a histogram plot\noutput_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\nDataset and DataLoaders\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision import transforms, datasets\nfrom torchvision.transforms import ToPILImage, v2\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n\nclasses = ['0', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus','train', \n           'truck', 'boat', 'traffic light', 'fire hydrant', '12', 'stop sign',\n           'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n           'elephant', 'bear', 'zebra', 'giraffe', '26', 'backpack', 'umbrella', '29',\n           '30', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n           'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n           'surfboard', 'tennis racket', 'bottle', '45', 'wine glass', 'cup', 'fork',\n           'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n           'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n           'potted plant', 'bed', '66', 'dining table', '68', '69', 'toilet', '71',\n           'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n           'oven', 'toaster', 'sink', 'refrigerator', '83', 'book', 'clock', 'vase',\n           'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n\n\ntrain_path = './Data/coco/train'\nval_path = './Data/coco/validation'\n\n# Define transforms\n# Define transforms for the dataset\ntransform2 = v2.Compose(\n    [\n        v2.ToImage(),\n        # v2.Resize(size = desired_size),  # Resize image\n        # v2.RandomPhotometricDistort(p=0.2),\n        # v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n        # # v2.RandomIoUCrop(),\n        # v2.RandomHorizontalFlip(p=0.4),\n        # # v2.SanitizeBoundingBoxes(),\n        v2.ToDtype(torch.float32, scale=True),\n    ]\n)\n\n\n# Load KITTI train dataset\ntrain_dataset = datasets.CocoDetection(root=f'{train_path}/data',\n                              annFile=f'{train_path}/labels.json', \n                              transform = transform2)\n\n# Load KITTI test dataset\ntest_dataset = datasets.CocoDetection(root=f'{val_path}/data',\n                            annFile=f'{val_path}/labels.json',\n                            transform=transform2)\n\nloading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\n\n\n\nsample = train_dataset[49]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\")\n\ntype(img) = &lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntype(target) = &lt;class 'list'&gt;\n\n\n\ntrain_dataset2 = datasets.wrap_dataset_for_transforms_v2(train_dataset)\n\ntest_dataset2 = datasets.wrap_dataset_for_transforms_v2(test_dataset)\n\n\nlen(train_dataset2), len(test_dataset2)\n\n(50, 50)\n\n\n\nbatch_size = 2\n\ntrain_loader = DataLoader(train_dataset2,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          collate_fn=lambda batch: tuple(zip(*batch)),\n                          num_workers = 8)\n\n# Create DataLoader for test dataset\ntest_loader = DataLoader(test_dataset2,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         collate_fn=lambda batch: tuple(zip(*batch)),\n                         num_workers = 8)\n\n\nsample = train_dataset2[10]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\nprint(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")\n\ntype(img) = &lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntype(target) = &lt;class 'dict'&gt;\ntarget.keys() = dict_keys(['image_id', 'boxes', 'labels'])\ntype(target['boxes']) = &lt;class 'torchvision.tv_tensors._bounding_boxes.BoundingBoxes'&gt;\ntype(target['labels']) = &lt;class 'torch.Tensor'&gt;\n\n\n\ndef cv2_show(image_np, label):\n    image_cv2 = cv2.cvtColor((image_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n    if 'scores' in label:\n        for bbox, item, score in zip(label['boxes'], label['labels'], label['scores']):\n            if score &gt; 0.2:\n                cv2.rectangle(image_cv2,\n                              (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n                              (0, 255, 0), 2)\n            \n                # Display the label\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                cv2.putText(image_cv2, str(item),\n                            (int(bbox[0]), int(bbox[1]) - 10),\n                            font, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n    else:\n        for bbox, item in zip(label['boxes'], label['labels']):\n            # Draw the bounding box\n            cv2.rectangle(image_cv2,\n                          (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n                          (0, 255, 0), 2)\n        \n            # Display the label\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            cv2.putText(image_cv2, classes[item],\n                        (int(bbox[0]), int(bbox[1]) - 10),\n                        font, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n\n    # Convert the image back to RGB format for display with Matplotlib\n    image_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n    \n    return image_rgb\n\ndef show_image(kitti_dataset):\n    # Access an image and its label from the dataset\n    image, label = kitti_dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image.numpy().transpose((1, 2, 0))\n    image_rgb = cv2_show(image_np, label)\n    # Display the image using Matplotlib\n    plt.imshow(image_rgb)\n    plt.axis('off')\n    plt.show()\n\n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.sqrt(len(images)))\n    ncols = int(np.floor(len(images)/nrows))\n        \n    fig, axes = plt.subplots(nrows, ncols, **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.01, hspace=0.01)\n\n    # Display the image using Matplotlib\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        image_np = np.asarray(image).transpose((1, 2, 0))\n        image_rgb = cv2_show(image_np, label)\n        \n        # Display image\n        ax.imshow(image_rgb)\n        ax.axis('off')\n\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n    plt.show()\n\n\nshow_image(train_dataset2[18])\n\n\n\n\n\n\n\n\n\nIter\n\n# Get a batch of images and labels from the data loader\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nshow_images(images, labels, figsize=(15, 5))",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "LLM/rag.html",
    "href": "LLM/rag.html",
    "title": "RAG",
    "section": "",
    "text": "Retrieval-Augmented Generation (RAG) is a method that enhances Large Language Models (LLMs) by integrating external knowledge retrieval before generating responses. Unlike standard LLMs that rely solely on pre-trained knowledge, RAG retrieves relevant documents from a knowledge base and injects them into the prompt before inference.\n\n\n\nImproves factual accuracy by pulling real-time or domain-specific knowledge.\nReduces hallucinations by grounding the model in reliable sources.\nEnhances performance on domain-specific tasks such as finance, healthcare, and legal analysis.\nEliminates the need for full fine-tuning by dynamically incorporating external information.",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#what-is-retrieval-augmented-generation-rag",
    "href": "LLM/rag.html#what-is-retrieval-augmented-generation-rag",
    "title": "RAG",
    "section": "",
    "text": "Retrieval-Augmented Generation (RAG) is a method that enhances Large Language Models (LLMs) by integrating external knowledge retrieval before generating responses. Unlike standard LLMs that rely solely on pre-trained knowledge, RAG retrieves relevant documents from a knowledge base and injects them into the prompt before inference.\n\n\n\nImproves factual accuracy by pulling real-time or domain-specific knowledge.\nReduces hallucinations by grounding the model in reliable sources.\nEnhances performance on domain-specific tasks such as finance, healthcare, and legal analysis.\nEliminates the need for full fine-tuning by dynamically incorporating external information.",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#how-rag-works",
    "href": "LLM/rag.html#how-rag-works",
    "title": "RAG",
    "section": "2. How RAG Works",
    "text": "2. How RAG Works\n\nStep-by-Step Process\n\nUser Query: The user inputs a question or request.\nRetrieval: A search engine or vector database retrieves the most relevant documents from a knowledge source.\nAugmentation: The retrieved documents are inserted into the modelâ€™s prompt.\nGeneration: The LLM processes both the query and retrieved information to generate a response.\n\n\n\nArchitecture Overview\nBelow is a high-level architecture of RAG:\nUser Query â†’ Embedding Model â†’ Vector Database â†’ Top-K Document Retrieval â†’ Prompt Augmentation â†’ LLM â†’ Response\n\n\nDiagram: Basic RAG Workflow\n+-------------+        +----------------+        +--------------------+        +-------------+\n| User Query  | ----&gt;  |  Document Index| ----&gt;  |  LLM Augmentation  | ----&gt;  |  AI Response |\n+-------------+        +----------------+        +--------------------+        +-------------+",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#comparing-rag-with-traditional-llm-methods",
    "href": "LLM/rag.html#comparing-rag-with-traditional-llm-methods",
    "title": "RAG",
    "section": "3. Comparing RAG with Traditional LLM Methods",
    "text": "3. Comparing RAG with Traditional LLM Methods\n\n\n\nFeature\nStandard LLMs\nFine-Tuned LLMs\nRAG\n\n\n\n\nExternal Knowledge\nâŒ No\nâœ… Limited\nâœ… Yes\n\n\nMemory Efficiency\nâœ… Yes\nâŒ No (New Weights)\nâœ… Yes\n\n\nReal-Time Updates\nâŒ No\nâŒ No\nâœ… Yes\n\n\nAccuracy Improvement\nâŒ Limited\nâœ… Yes\nâœ… Yes\n\n\nScalability\nâœ… High\nâŒ Costly\nâœ… High",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#implementing-rag-in-python",
    "href": "LLM/rag.html#implementing-rag-in-python",
    "title": "RAG",
    "section": "4. Implementing RAG in Python",
    "text": "4. Implementing RAG in Python\nA basic RAG system consists of: - LLM (e.g., OpenAI GPT, Mistral, LLaMA) - Vector database (e.g., FAISS, Pinecone, ChromaDB) - Embedding model (e.g., SentenceTransformers, OpenAI embeddings)\n\nStep 1: Install Dependencies\npip install langchain faiss-cpu openai sentence-transformers\n\n\nStep 2: Load a Knowledge Base\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Load documents\nloader = TextLoader(\"data.txt\")\ndocuments = loader.load()\n\n# Split into smaller chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\ndocs = text_splitter.split_documents(documents)\n\n\nStep 3: Generate Embeddings and Store in FAISS\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\n\n# Load embedding model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Convert documents to embeddings\ndoc_texts = [doc.page_content for doc in docs]\ndoc_embeddings = embedding_model.encode(doc_texts)\n\n# Store embeddings in FAISS\ndimension = doc_embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(np.array(doc_embeddings))\n\n\nStep 4: Retrieve Relevant Documents\ndef retrieve_documents(query, k=3):\n    query_embedding = embedding_model.encode([query])\n    distances, indices = index.search(np.array(query_embedding), k)\n    retrieved_texts = [doc_texts[i] for i in indices[0]]\n    return \"\\n\".join(retrieved_texts)\n\nquery = \"Explain neural networks.\"\nretrieved_docs = retrieve_documents(query)\nprint(retrieved_docs)\n\n\nStep 5: Pass Retrieved Data to LLM\nimport openai\n\ndef generate_rag_response(query):\n    retrieved_docs = retrieve_documents(query)\n    \n    prompt = f\"Use the following retrieved documents to answer:\\n\\n{retrieved_docs}\\n\\nUser: {query}\\nAssistant:\"\n    \n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n                  {\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return response[\"choices\"][0][\"message\"][\"content\"]\n\n# Example query\nresponse = generate_rag_response(\"What is deep learning?\")\nprint(response)",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#evaluating-rag-performance",
    "href": "LLM/rag.html#evaluating-rag-performance",
    "title": "RAG",
    "section": "5. Evaluating RAG Performance",
    "text": "5. Evaluating RAG Performance\nTo measure the effectiveness of RAG, compare: - Retrieval Accuracy: How relevant are the retrieved documents? - Response Quality: Does the LLM provide accurate answers based on the retrieval? - Latency: Is retrieval slowing down response generation?\n\nPerformance Metrics\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nRecall@K\nPercentage of correct documents retrieved\n\n\nBLEU Score\nMeasures text similarity to ground truth\n\n\nResponse Latency\nMeasures time taken to retrieve + generate response\n\n\n\n\n\nGraph: Accuracy Improvement Using RAG\nThis graph shows accuracy improvement with RAG compared to a standalone LLM.\nAccuracy (%)\nâ”‚\nâ”‚   90 â”€â”€  RAG-based LLM\nâ”‚\nâ”‚   75 â”€â”€  Fine-tuned LLM\nâ”‚\nâ”‚   60 â”€â”€  Standard LLM\nâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        LLM Type",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#scaling-rag-for-large-applications",
    "href": "LLM/rag.html#scaling-rag-for-large-applications",
    "title": "RAG",
    "section": "6. Scaling RAG for Large Applications",
    "text": "6. Scaling RAG for Large Applications\nFor production-scale RAG systems: 1. Use Distributed Vector Databases (Pinecone, Weaviate) instead of FAISS. 2. Pre-filter Documents to improve retrieval speed. 3. Optimize Context Window to avoid overloading the LLM. 4. Use Hybrid Search (BM25 + Embeddings) for better recall.",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#challenges-and-considerations",
    "href": "LLM/rag.html#challenges-and-considerations",
    "title": "RAG",
    "section": "7. Challenges and Considerations",
    "text": "7. Challenges and Considerations\n\n\n\n\n\n\n\nChallenge\nSolution\n\n\n\n\nLatency in Retrieval\nOptimize vector search, use caching\n\n\nMemory Consumption\nUse compressed embeddings, distributed storage\n\n\nData Drift\nRegularly update knowledge base\n\n\nHallucination Despite RAG\nFilter retrieved documents to ensure factual consistency",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#advanced-rag-techniques",
    "href": "LLM/rag.html#advanced-rag-techniques",
    "title": "RAG",
    "section": "8. Advanced RAG Techniques",
    "text": "8. Advanced RAG Techniques\n\nMulti-Stage RAG\nInstead of a single retrieval step, multi-stage RAG refines retrieval by applying re-ranking algorithms.\n\n\nGraph-Based RAG\nInstead of keyword matching, knowledge graphs can retrieve structured facts more accurately.\n\n\nAgent-Based RAG\nCombines RAG with autonomous AI agents that perform multiple reasoning steps before generating a response.",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#real-world-use-cases",
    "href": "LLM/rag.html#real-world-use-cases",
    "title": "RAG",
    "section": "9. Real-World Use Cases",
    "text": "9. Real-World Use Cases\n\n\n\n\n\n\n\nIndustry\nApplication\n\n\n\n\nHealthcare\nMedical chatbots retrieving up-to-date research papers\n\n\nLegal\nAI-powered legal document search and Q&A\n\n\nFinance\nMarket analysis by retrieving real-time reports\n\n\nCustomer Support\nAI assistants providing support from internal documentation",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/rag.html#summary",
    "href": "LLM/rag.html#summary",
    "title": "RAG",
    "section": "10. Summary",
    "text": "10. Summary\n\nRAG enhances LLMs by integrating real-time information retrieval.\nIt prevents hallucinations and improves factual accuracy.\nUsing vector databases like FAISS or Pinecone enables fast retrieval.\nHybrid search and multi-stage retrieval can further optimize results.\nScaling RAG requires optimizing retrieval efficiency and memory usage.\n\nBy leveraging RAG, LLMs can become more accurate, reliable, and adaptable without expensive fine-tuning.",
    "crumbs": [
      "Blog",
      "LLM",
      "RAG"
    ]
  },
  {
    "objectID": "LLM/langchain.html",
    "href": "LLM/langchain.html",
    "title": "LangChain",
    "section": "",
    "text": "LangChain enables developers to build powerful applications that leverage LLMs like OpenAIâ€™s GPT, Cohere, Mistral, DeepSeek, and more.\n\n\n\nSimplifies LLM integration with APIs, documents, and databases.\nProvides modular components for easy prototyping and scaling.\nSupports memory and state management for multi-turn conversations.\nBuilt-in retrieval and knowledge augmentation for reducing hallucinations.\nEnables reasoning and action using LLM Agents.",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#what-is-langchain",
    "href": "LLM/langchain.html#what-is-langchain",
    "title": "LangChain",
    "section": "",
    "text": "LangChain enables developers to build powerful applications that leverage LLMs like OpenAIâ€™s GPT, Cohere, Mistral, DeepSeek, and more.\n\n\n\nSimplifies LLM integration with APIs, documents, and databases.\nProvides modular components for easy prototyping and scaling.\nSupports memory and state management for multi-turn conversations.\nBuilt-in retrieval and knowledge augmentation for reducing hallucinations.\nEnables reasoning and action using LLM Agents.",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#core-components-of-langchain",
    "href": "LLM/langchain.html#core-components-of-langchain",
    "title": "LangChain",
    "section": "2. Core Components of LangChain",
    "text": "2. Core Components of LangChain\nLangChain consists of six primary components:\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nLLMs\nInterface for calling language models (GPT-4, LLaMA, Claude, etc.)\n\n\nChains\nSequences of operations that process inputs and outputs\n\n\nAgents\nAI decision-making system that dynamically selects tools\n\n\nMemory\nStores conversation history and user state\n\n\nRetrieval\nEnables RAG with vector databases\n\n\nTools & Utilities\nFunctions like Google Search, APIs, or code execution",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#setting-up-langchain",
    "href": "LLM/langchain.html#setting-up-langchain",
    "title": "LangChain",
    "section": "3. Setting Up LangChain",
    "text": "3. Setting Up LangChain\n\nInstall LangChain and Dependencies\npip install langchain openai chromadb faiss-cpu tiktoken\n\n\nSet Up API Keys (Example: OpenAI)\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#using-llms-in-langchain",
    "href": "LLM/langchain.html#using-llms-in-langchain",
    "title": "LangChain",
    "section": "4. Using LLMs in LangChain",
    "text": "4. Using LLMs in LangChain\n\nBasic Example with OpenAIâ€™s GPT-4\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7)\nresponse = llm.predict(\"What is LangChain?\")\nprint(response)\n\n\nUsing LLaMA or Other Local Models\nfrom langchain.llms import Ollama\n\nllm = Ollama(model=\"llama2\")\nresponse = llm(\"Tell me about AI applications.\")\nprint(response)",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#chains-creating-pipelines",
    "href": "LLM/langchain.html#chains-creating-pipelines",
    "title": "LangChain",
    "section": "5. Chains: Creating Pipelines",
    "text": "5. Chains: Creating Pipelines\nA Chain is a sequence of operations (e.g., input â†’ LLM â†’ output).\n\nBasic LLM Chain\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"What are the top 3 applications of {technology}?\")\nchain = LLMChain(llm=llm, prompt=prompt)\n\nresponse = chain.run(technology=\"Machine Learning\")\nprint(response)\n\n\nSequential Chains (Multiple Steps)\nfrom langchain.chains import SimpleSequentialChain\n\nchain1 = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Explain {concept}\"))\nchain2 = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Summarize in one line: {text}\"))\n\npipeline = SimpleSequentialChain(chains=[chain1, chain2])\nresponse = pipeline.run(\"Quantum Computing\")\nprint(response)",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#agents-dynamic-ai-with-tool-use",
    "href": "LLM/langchain.html#agents-dynamic-ai-with-tool-use",
    "title": "LangChain",
    "section": "6. Agents: Dynamic AI with Tool Use",
    "text": "6. Agents: Dynamic AI with Tool Use\nAgents allow an LLM to choose tools dynamically rather than following a fixed flow.\n\nBasic Agent with OpenAI Functions\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.tools import Tool\nfrom langchain.chat_models import ChatOpenAI\n\ndef search_tool(query):\n    return f\"Searching for {query} on the web...\"\n\ntools = [Tool(name=\"WebSearch\", func=search_tool, description=\"Searches the web\")]\n\nagent = initialize_agent(\n    tools=tools,\n    llm=ChatOpenAI(model=\"gpt-4\"),\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\nresponse = agent.run(\"What is the latest AI research?\")\nprint(response)\n\n\nSelf-Reflecting AI Agent\nfrom langchain.agents import OpenAIFunctionsAgent\nfrom langchain.agents.agent import AgentExecutor\n\nagent = OpenAIFunctionsAgent.from_llm(llm=ChatOpenAI(model=\"gpt-4\"))\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresponse = executor.invoke(\"Find the latest AI papers.\")\nprint(response)",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#memory-and-state-management",
    "href": "LLM/langchain.html#memory-and-state-management",
    "title": "LangChain",
    "section": "7. Memory and State Management",
    "text": "7. Memory and State Management\nBy default, LLMs do not remember past interactions. Memory in LangChain enables persistence.\n\nAdding Memory to Conversations\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(llm=llm, memory=memory)\n\nprint(conversation.run(\"Hello, who are you?\"))\nprint(conversation.run(\"Can you summarize our conversation?\"))\n\n\nDifferent Memory Types\n\n\n\n\n\n\n\nMemory Type\nUse Case\n\n\n\n\nConversationBufferMemory\nStores complete conversation history\n\n\nConversationSummaryMemory\nSummarizes past interactions\n\n\nVectorStoreRetrieverMemory\nUses embeddings to store long-term memory",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#retrieval-augmented-generation-rag",
    "href": "LLM/langchain.html#retrieval-augmented-generation-rag",
    "title": "LangChain",
    "section": "8. Retrieval-Augmented Generation (RAG)",
    "text": "8. Retrieval-Augmented Generation (RAG)\nRAG combines vector search with LLMs to reduce hallucinations.\n\nLoading a Document and Storing Embeddings\nfrom langchain.document_loaders import TextLoader\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Load and split documents\nloader = TextLoader(\"docs.txt\")\ndocuments = loader.load()\n\n# Store as vector embeddings\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(documents, embeddings)\n\n\nRetrieving and Passing to LLM\nretriever = vectorstore.as_retriever()\nretrieved_docs = retriever.get_relevant_documents(\"What is AI?\")",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#langchain-tools-and-integrations",
    "href": "LLM/langchain.html#langchain-tools-and-integrations",
    "title": "LangChain",
    "section": "9. LangChain Tools and Integrations",
    "text": "9. LangChain Tools and Integrations\nLangChain connects with APIs, databases, and search engines.\n\nGoogle Search API\nfrom langchain.tools import Tool\nfrom langchain.utilities import GoogleSearchAPIWrapper\n\nsearch = GoogleSearchAPIWrapper()\ntool = Tool(name=\"Google Search\", func=search.run)\n\nresult = tool.run(\"Latest AI news\")\nprint(result)\n\n\nCode Execution\nfrom langchain.tools import PythonREPLTool\n\npython_tool = PythonREPLTool()\nprint(python_tool.run(\"import math; math.sqrt(16)\"))",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#scaling-and-optimization",
    "href": "LLM/langchain.html#scaling-and-optimization",
    "title": "LangChain",
    "section": "10. Scaling and Optimization",
    "text": "10. Scaling and Optimization\n\nOptimizing Token Usage\nimport tiktoken\n\nencoder = tiktoken.get_encoding(\"cl100k_base\")\nprint(len(encoder.encode(\"This is a test message.\")))  # Token count\n\n\nUsing Local LLMs Instead of API Calls\nFor privacy or cost efficiency, use models like LLaMA or Mistral:\nfrom langchain.llms import Ollama\nlocal_llm = Ollama(model=\"mistral\")\n\n\nBatch Processing for Efficiency\nresponses = llm.batch([\"Explain AI\", \"Define RAG\", \"What is LangChain?\"])\nprint(responses)",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#example-architectures-and-use-cases",
    "href": "LLM/langchain.html#example-architectures-and-use-cases",
    "title": "LangChain",
    "section": "11. Example Architectures and Use Cases",
    "text": "11. Example Architectures and Use Cases\n\n\n\nUse Case\nLangChain Components Used\n\n\n\n\nAI-Powered Chatbots\nMemory, Chains, LLMs\n\n\nRetrieval-Augmented QA Systems\nRetrieval, Vector Stores, LLMs\n\n\nAI Agents for Research\nAgents, Tools, API Integrations\n\n\nAutomated Code Generation\nLLMs, Python Execution Tools\n\n\nPersonalized Assistants\nMemory, State Tracking",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/langchain.html#final-thoughts",
    "href": "LLM/langchain.html#final-thoughts",
    "title": "LangChain",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nLangChain simplifies LLM development by abstracting complex workflows.\nIt integrates retrieval, memory, and agents for dynamic applications.\nIt supports local and cloud-based LLMs, ensuring flexibility.\nOptimizing context size, retrieval methods, and caching improves efficiency.\n\nLangChain is the foundation for building advanced, production-ready AI applications with modular, scalable components.",
    "crumbs": [
      "Blog",
      "LLM",
      "LangChain"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html",
    "href": "LLM/openai_sdk.html",
    "title": "OpenAI SDK",
    "section": "",
    "text": "from openai import OpenAI\nimport json\nfrom dotenv import load_dotenv\nimport os\nfrom IPython.display import Markdown, display\n\n\nload_dotenv()\n\nclient = OpenAI(\n  api_key=os.getenv('openai_api_key')\n)\n\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  store=True,\n  messages=[\n    {\"role\": \"user\", \"content\": \"what language model is this?\"}\n  ],\n  max_tokens=150,\n)\n\ncontent = completion.choices[0].message.content\ndisplay(Markdown(content))\n\nI am based on OpenAIâ€™s GPT-3 model, which is a language model designed for natural language understanding and generation. If you have any questions or need assistance, feel free to ask!",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#example",
    "href": "LLM/openai_sdk.html#example",
    "title": "OpenAI SDK",
    "section": "",
    "text": "from openai import OpenAI\nimport json\nfrom dotenv import load_dotenv\nimport os\nfrom IPython.display import Markdown, display\n\n\nload_dotenv()\n\nclient = OpenAI(\n  api_key=os.getenv('openai_api_key')\n)\n\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  store=True,\n  messages=[\n    {\"role\": \"user\", \"content\": \"what language model is this?\"}\n  ],\n  max_tokens=150,\n)\n\ncontent = completion.choices[0].message.content\ndisplay(Markdown(content))\n\nI am based on OpenAIâ€™s GPT-3 model, which is a language model designed for natural language understanding and generation. If you have any questions or need assistance, feel free to ask!",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#installation",
    "href": "LLM/openai_sdk.html#installation",
    "title": "OpenAI SDK",
    "section": "ðŸ“¦ 1. Installation",
    "text": "ðŸ“¦ 1. Installation\npip install openai\n\nâœ… This installs the official SDK",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#authentication",
    "href": "LLM/openai_sdk.html#authentication",
    "title": "OpenAI SDK",
    "section": "ðŸ” 2. Authentication",
    "text": "ðŸ” 2. Authentication\n\nOptions:\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nOther methods: - Environment variable: OPENAI_API_KEY - .openai config file - Azure or custom base URL: base_url=\"https://...\"",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#sdk-client-overview",
    "href": "LLM/openai_sdk.html#sdk-client-overview",
    "title": "OpenAI SDK",
    "section": "âš™ï¸ 3. SDK Client Overview",
    "text": "âš™ï¸ 3. SDK Client Overview\nfrom openai import OpenAI\nclient = OpenAI()\nClient supports: - client.chat.completions.create(...) - client.images.generate(...) - client.embeddings.create(...) - client.audio.transcriptions.create(...) - client.files.create(...) - client.fine_tuning.jobs.create(...) - and moreâ€¦",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#chat-completions",
    "href": "LLM/openai_sdk.html#chat-completions",
    "title": "OpenAI SDK",
    "section": "ðŸ’¬ 4. Chat Completions",
    "text": "ðŸ’¬ 4. Chat Completions\n\nBasic usage:\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n\n\nOptions:\n\nmax_tokens, temperature, top_p, presence_penalty\ntools, tool_choice\nfunction_call (deprecated in favor of tools)",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#streaming-responses",
    "href": "LLM/openai_sdk.html#streaming-responses",
    "title": "OpenAI SDK",
    "section": "ðŸ” 5. Streaming Responses",
    "text": "ðŸ” 5. Streaming Responses\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Stream this response\"}],\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n\nâœ… Useful for real-time apps, CLI tools, or dashboards.",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#tool-use-function-calling",
    "href": "LLM/openai_sdk.html#tool-use-function-calling",
    "title": "OpenAI SDK",
    "section": "ðŸ§  6. Tool Use / Function Calling",
    "text": "ðŸ§  6. Tool Use / Function Calling\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}],\n    tools=[\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather by city\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"city\": {\"type\": \"string\"},\n                    },\n                    \"required\": [\"city\"]\n                }\n            }\n        }\n    ],\n    tool_choice=\"auto\"\n)\nCheck for tool_calls in response and call your function accordingly.",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#image-generation-dalle",
    "href": "LLM/openai_sdk.html#image-generation-dalle",
    "title": "OpenAI SDK",
    "section": "ðŸ–¼ï¸ 7. Image Generation (DALLÂ·E)",
    "text": "ðŸ–¼ï¸ 7. Image Generation (DALLÂ·E)\nimage = client.images.generate(\n    prompt=\"A futuristic city skyline at dusk\",\n    model=\"dall-e-3\",\n    n=1,\n    size=\"1024x1024\"\n)\nprint(image.data[0].url)",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#embeddings",
    "href": "LLM/openai_sdk.html#embeddings",
    "title": "OpenAI SDK",
    "section": "ðŸ§¬ 8. Embeddings",
    "text": "ðŸ§¬ 8. Embeddings\nresponse = client.embeddings.create(\n    input=\"Deep learning is powerful.\",\n    model=\"text-embedding-3-small\"\n)\nvector = response.data[0].embedding\nUse for semantic search, vector databases, etc.",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#audio-transcription-whisper",
    "href": "LLM/openai_sdk.html#audio-transcription-whisper",
    "title": "OpenAI SDK",
    "section": "ðŸ”Š 9. Audio Transcription (Whisper)",
    "text": "ðŸ”Š 9. Audio Transcription (Whisper)\nwith open(\"audio.mp3\", \"rb\") as file:\n    transcript = client.audio.transcriptions.create(\n        model=\"whisper-1\", file=file\n    )\nprint(transcript.text)",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#file-upload-fine-tuning",
    "href": "LLM/openai_sdk.html#file-upload-fine-tuning",
    "title": "OpenAI SDK",
    "section": "ðŸ“ 10. File Upload + Fine-tuning",
    "text": "ðŸ“ 10. File Upload + Fine-tuning\n\nUpload a file:\nclient.files.create(file=open(\"data.jsonl\", \"rb\"), purpose=\"fine-tune\")\n\n\nFine-tuning:\nclient.fine_tuning.jobs.create(training_file=\"file-abc123\")",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#billing-usage-manual-requests",
    "href": "LLM/openai_sdk.html#billing-usage-manual-requests",
    "title": "OpenAI SDK",
    "section": "ðŸ“Š 11. Billing / Usage (manual requests)",
    "text": "ðŸ“Š 11. Billing / Usage (manual requests)\nThe SDK does not expose billing endpoints, but you can query them:\nimport requests\n\nheaders = {\"Authorization\": f\"Bearer {api_key}\"}\nr = requests.get(\"https://api.openai.com/v1/dashboard/billing/usage?start_date=2024-04-01&end_date=2024-04-30\", headers=headers)\nprint(r.json())",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#error-handling",
    "href": "LLM/openai_sdk.html#error-handling",
    "title": "OpenAI SDK",
    "section": "ðŸ“Ž 12. Error Handling",
    "text": "ðŸ“Ž 12. Error Handling\nfrom openai import OpenAI, APIError, RateLimitError\n\ntry:\n    ...\nexcept RateLimitError:\n    print(\"Rate limited.\")\nexcept APIError as e:\n    print(f\"OpenAI API error: {e}\")",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#models-available-as-of-2024-2025",
    "href": "LLM/openai_sdk.html#models-available-as-of-2024-2025",
    "title": "OpenAI SDK",
    "section": "âœ… 13. Models Available (as of 2024-2025)",
    "text": "âœ… 13. Models Available (as of 2024-2025)\n\n\n\nModel\nType\nUse Case\n\n\n\n\ngpt-4o\nChat\nFast, cheap, vision, voice\n\n\ngpt-4\nChat\nHighest reasoning\n\n\ngpt-3.5-turbo\nChat\nCost-effective\n\n\ndall-e-3\nImage\nPrompt-to-image\n\n\nwhisper-1\nAudio\nSpeech to text\n\n\ntext-embedding-3\nEmbeddings\nVector search",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#advanced-tips",
    "href": "LLM/openai_sdk.html#advanced-tips",
    "title": "OpenAI SDK",
    "section": "ðŸ§° 14. Advanced Tips",
    "text": "ðŸ§° 14. Advanced Tips\n\nresponse.usage.total_tokens to track billing\nUse .store=True to save conversations if supported\nUse logprobs for token-level info (GPT-3 only)",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#testing-in-notebooks",
    "href": "LLM/openai_sdk.html#testing-in-notebooks",
    "title": "OpenAI SDK",
    "section": "ðŸ§ª 15. Testing in Notebooks",
    "text": "ðŸ§ª 15. Testing in Notebooks\nfrom IPython.display import Markdown\nres = client.chat.completions.create(...)\ndisplay(Markdown(res.choices[0].message.content))",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  },
  {
    "objectID": "LLM/openai_sdk.html#summary",
    "href": "LLM/openai_sdk.html#summary",
    "title": "OpenAI SDK",
    "section": "ðŸ”š Summary",
    "text": "ðŸ”š Summary\n\n\n\nTask\nSDK Support\n\n\n\n\nChat Completion\nâœ… .chat.completions.create()\n\n\nImage Generation\nâœ… .images.generate()\n\n\nAudio Transcribe\nâœ… .audio.transcriptions.create()\n\n\nEmbeddings\nâœ… .embeddings.create()\n\n\nFile Upload\nâœ… .files.create()\n\n\nBilling Info\nâŒ Use requests manually",
    "crumbs": [
      "Blog",
      "LLM",
      "OpenAI SDK"
    ]
  }
]